#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bosch Production Line Performance - ì‹¤ì œ ë°ì´í„° ë¶„ì„
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
import gc

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')

def analyze_data_overview():
    """ë°ì´í„° ì „ì²´ ê°œìš” ë¶„ì„"""
    print("=" * 60)
    print("ğŸ­ BOSCH PRODUCTION LINE PERFORMANCE ì‹¤ì œ ë°ì´í„° ë¶„ì„")
    print("=" * 60)
    
    data_dir = "C:/Users/ASUS/bosch/data/"
    
    # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
    files = {
        'train_numeric': f'{data_dir}train_numeric.csv',
        'train_categorical': f'{data_dir}train_categorical.csv', 
        'train_date': f'{data_dir}train_date.csv',
        'test_numeric': f'{data_dir}test_numeric.csv',
        'test_categorical': f'{data_dir}test_categorical.csv',
        'test_date': f'{data_dir}test_date.csv'
    }
    
    print("\nğŸ“Š ë°ì´í„° íŒŒì¼ ì •ë³´:")
    print("-" * 50)
    
    for name, path in files.items():
        try:
            import os
            size_mb = os.path.getsize(path) / (1024*1024)
            
            # ì²« ì¤„ë§Œ ì½ì–´ì„œ ì»¬ëŸ¼ ìˆ˜ í™•ì¸
            header = pd.read_csv(path, nrows=0)
            num_cols = len(header.columns)
            
            # ì „ì²´ í–‰ ìˆ˜ í™•ì¸ (ë” ë¹ ë¥¸ ë°©ë²•)
            with open(path, 'r') as f:
                num_rows = sum(1 for line in f) - 1  # í—¤ë” ì œì™¸
            
            print(f"{name:20s}: {size_mb:7.1f} MB | {num_rows:8,} rows | {num_cols:4,} cols")
            
        except Exception as e:
            print(f"{name:20s}: Error - {str(e)}")
    
    return files

def analyze_train_numeric_sample(data_dir, sample_size=100000):
    """ìˆ«ì ë°ì´í„° ìƒ˜í”Œ ë¶„ì„"""
    print(f"\nğŸ”¢ TRAIN NUMERIC ë°ì´í„° ë¶„ì„ (ìƒ˜í”Œ: {sample_size:,}ê°œ)")
    print("-" * 50)
    
    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv(f'{data_dir}train_numeric.csv', nrows=sample_size)
    
    print(f"ìƒ˜í”Œ í¬ê¸°: {df.shape[0]:,} x {df.shape[1]:,}")
    
    # ëª©í‘œ ë³€ìˆ˜ ë¶„ì„
    print(f"\nğŸ¯ ëª©í‘œ ë³€ìˆ˜ (Response) ë¶„ì„:")
    response_counts = df['Response'].value_counts()
    failure_rate = df['Response'].mean()
    
    print(f"   ì •ìƒí’ˆ (0): {response_counts[0]:,}ê°œ ({(1-failure_rate)*100:.2f}%)")
    print(f"   ë¶ˆëŸ‰í’ˆ (1): {response_counts[1]:,}ê°œ ({failure_rate*100:.2f}%)")
    print(f"   ë¶ˆëŸ‰ë¥ : {failure_rate:.4%}")
    
    # ê²°ì¸¡ê°’ ë¶„ì„
    print(f"\nâ“ ê²°ì¸¡ê°’ ë¶„ì„:")
    missing_stats = df.isnull().sum()
    total_features = len(df.columns) - 2  # Id, Response ì œì™¸
    
    completely_empty = (missing_stats == len(df)).sum()
    mostly_empty = (missing_stats > len(df) * 0.9).sum() 
    half_empty = (missing_stats > len(df) * 0.5).sum()
    no_missing = (missing_stats == 0).sum()
    
    print(f"   ì „ì²´ íŠ¹ì§• ìˆ˜: {total_features:,}ê°œ")
    print(f"   ì™„ì „íˆ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {completely_empty:,}ê°œ ({completely_empty/total_features*100:.1f}%)")
    print(f"   90% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {mostly_empty:,}ê°œ ({mostly_empty/total_features*100:.1f}%)")
    print(f"   50% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {half_empty:,}ê°œ ({half_empty/total_features*100:.1f}%)")
    print(f"   ê²°ì¸¡ê°’ì´ ì—†ëŠ” ì»¬ëŸ¼: {no_missing:,}ê°œ ({no_missing/total_features*100:.1f}%)")
    
    # íŠ¹ì§• ê·¸ë£¹ ë¶„ì„
    print(f"\nğŸ­ íŠ¹ì§• ê·¸ë£¹ ë¶„ì„:")
    feature_groups = {}
    
    for col in df.columns:
        if col not in ['Id', 'Response']:
            parts = col.split('_')
            if len(parts) >= 2:
                group = parts[0] + '_' + parts[1]
                if group not in feature_groups:
                    feature_groups[group] = 0
                feature_groups[group] += 1
    
    # ìƒìœ„ 10ê°œ ê·¸ë£¹
    sorted_groups = sorted(feature_groups.items(), key=lambda x: x[1], reverse=True)
    print("   ìƒìœ„ 10ê°œ ì¸¡ì • ìŠ¤í…Œì´ì…˜:")
    for i, (group, count) in enumerate(sorted_groups[:10], 1):
        print(f"   {i:2d}. {group}: {count:3d}ê°œ íŠ¹ì§•")
    
    return df

def create_basic_features(df):
    """ê¸°ë³¸ í†µê³„ íŠ¹ì§• ìƒì„±"""
    print(f"\nğŸ› ï¸ íŠ¹ì§• ê³µí•™:")
    print("-" * 30)
    
    feature_df = pd.DataFrame()
    feature_df['Id'] = df['Id']
    
    # ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(['Id', 'Response'], errors='ignore')
    
    # ì§‘ê³„ íŠ¹ì§•ë“¤
    print("   ê¸°ë³¸ í†µê³„ íŠ¹ì§• ìƒì„± ì¤‘...")
    feature_df['count_non_null'] = df[numeric_cols].count(axis=1)
    feature_df['count_zeros'] = (df[numeric_cols] == 0).sum(axis=1)
    feature_df['missing_count'] = df[numeric_cols].isnull().sum(axis=1)
    feature_df['missing_ratio'] = feature_df['missing_count'] / len(numeric_cols)
    
    # í†µê³„ëŸ‰
    feature_df['mean'] = df[numeric_cols].mean(axis=1)
    feature_df['std'] = df[numeric_cols].std(axis=1)
    feature_df['min'] = df[numeric_cols].min(axis=1)
    feature_df['max'] = df[numeric_cols].max(axis=1)
    feature_df['range'] = feature_df['max'] - feature_df['min']
    feature_df['median'] = df[numeric_cols].median(axis=1)
    
    # ìŠ¤í…Œì´ì…˜ë³„ ì§‘ê³„ íŠ¹ì§•
    print("   ìŠ¤í…Œì´ì…˜ë³„ íŠ¹ì§• ìƒì„± ì¤‘...")
    station_groups = {}
    for col in numeric_cols:
        parts = col.split('_')
        if len(parts) >= 2:
            station = parts[0] + '_' + parts[1]
            if station not in station_groups:
                station_groups[station] = []
            station_groups[station].append(col)
    
    # ì£¼ìš” ìŠ¤í…Œì´ì…˜ë“¤ì˜ í†µê³„
    for station, cols in list(station_groups.items())[:5]:  # ìƒìœ„ 5ê°œë§Œ
        station_data = df[cols]
        feature_df[f'{station}_mean'] = station_data.mean(axis=1)
        feature_df[f'{station}_count'] = station_data.count(axis=1)
    
    # ëª©í‘œ ë³€ìˆ˜ ì¶”ê°€
    if 'Response' in df.columns:
        feature_df['Response'] = df['Response']
    
    print(f"   ìƒì„±ëœ íŠ¹ì§• ìˆ˜: {len(feature_df.columns)}ê°œ")
    
    return feature_df

def analyze_correlations(feature_df):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    print(f"\nğŸ“Š íŠ¹ì§•-ëª©í‘œë³€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„:")
    print("-" * 40)
    
    if 'Response' not in feature_df.columns:
        print("ëª©í‘œ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    feature_cols = [col for col in feature_df.columns if col not in ['Id', 'Response']]
    correlations = feature_df[feature_cols].corrwith(feature_df['Response'])
    correlations = correlations.sort_values(key=abs, ascending=False)
    
    print("ìƒìœ„ 10ê°œ ìƒê´€ê´€ê³„:")
    for i, (feature, corr) in enumerate(correlations.head(10).items(), 1):
        direction = "ì–‘ì˜" if corr > 0 else "ìŒì˜"
        print(f"   {i:2d}. {feature:20s}: {corr:+.4f} ({direction})")
    
    return correlations

def create_visualizations(df, feature_df, correlations):
    """ë°ì´í„° ì‹œê°í™”"""
    print(f"\nğŸ“ˆ ë°ì´í„° ì‹œê°í™” ìƒì„± ì¤‘...")
    
    plt.figure(figsize=(20, 15))
    
    # 1. ëª©í‘œ ë³€ìˆ˜ ë¶„í¬
    plt.subplot(2, 4, 1)
    response_counts = df['Response'].value_counts()
    colors = ['lightgreen', 'salmon']
    plt.bar(['ì •ìƒí’ˆ', 'ë¶ˆëŸ‰í’ˆ'], response_counts.values, color=colors)
    plt.title('ëª©í‘œ ë³€ìˆ˜ ë¶„í¬')
    plt.ylabel('ê°œìˆ˜')
    for i, v in enumerate(response_counts.values):
        plt.text(i, v + max(response_counts.values) * 0.01, f'{v:,}', ha='center', fontweight='bold')
    
    # 2. ê²°ì¸¡ê°’ ë¶„í¬
    plt.subplot(2, 4, 2)
    missing_pct = (df.isnull().sum() / len(df) * 100)
    plt.hist(missing_pct, bins=50, edgecolor='black', alpha=0.7)
    plt.xlabel('ê²°ì¸¡ê°’ ë¹„ìœ¨ (%)')
    plt.ylabel('íŠ¹ì§• ê°œìˆ˜')
    plt.title('ê²°ì¸¡ê°’ ë¶„í¬')
    
    # 3. íŠ¹ì§• ê°œìˆ˜ë³„ ë¶„í¬
    plt.subplot(2, 4, 3)
    plt.hist(feature_df['count_non_null'], bins=30, edgecolor='black', alpha=0.7)
    plt.xlabel('ì¸¡ì •ëœ íŠ¹ì§• ê°œìˆ˜')
    plt.ylabel('ì œí’ˆ ê°œìˆ˜')
    plt.title('ì œí’ˆë³„ ì¸¡ì • íŠ¹ì§• ê°œìˆ˜')
    
    # 4. ìƒê´€ê´€ê³„ Top 10
    plt.subplot(2, 4, 4)
    top_corr = correlations.head(10)
    colors = ['red' if x < 0 else 'blue' for x in top_corr.values]
    plt.barh(range(len(top_corr)), top_corr.values, color=colors)
    plt.yticks(range(len(top_corr)), top_corr.index, fontsize=8)
    plt.xlabel('ìƒê´€ê³„ìˆ˜')
    plt.title('ìƒìœ„ 10ê°œ ìƒê´€ê´€ê³„')
    plt.grid(axis='x', alpha=0.3)
    
    # 5. ë¶ˆëŸ‰í’ˆ vs ì •ìƒí’ˆ ë¹„êµ (ì¸¡ì • ê°œìˆ˜)
    plt.subplot(2, 4, 5)
    normal = feature_df[feature_df['Response'] == 0]['count_non_null']
    failure = feature_df[feature_df['Response'] == 1]['count_non_null']
    
    plt.hist(normal, bins=30, alpha=0.7, label='ì •ìƒí’ˆ', color='green', density=True)
    plt.hist(failure, bins=30, alpha=0.7, label='ë¶ˆëŸ‰í’ˆ', color='red', density=True)
    plt.xlabel('ì¸¡ì • íŠ¹ì§• ê°œìˆ˜')
    plt.ylabel('ë°€ë„')
    plt.title('ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ: ì¸¡ì • ê°œìˆ˜')
    plt.legend()
    
    # 6. ê²°ì¸¡ê°’ ë¹„ìœ¨ ë¹„êµ
    plt.subplot(2, 4, 6)
    normal_missing = feature_df[feature_df['Response'] == 0]['missing_ratio']
    failure_missing = feature_df[feature_df['Response'] == 1]['missing_ratio']
    
    plt.hist(normal_missing, bins=30, alpha=0.7, label='ì •ìƒí’ˆ', color='green', density=True)
    plt.hist(failure_missing, bins=30, alpha=0.7, label='ë¶ˆëŸ‰í’ˆ', color='red', density=True)
    plt.xlabel('ê²°ì¸¡ê°’ ë¹„ìœ¨')
    plt.ylabel('ë°€ë„')
    plt.title('ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ: ê²°ì¸¡ê°’ ë¹„ìœ¨')
    plt.legend()
    
    # 7. í‰ê· ê°’ ë¹„êµ
    plt.subplot(2, 4, 7)
    normal_mean = feature_df[feature_df['Response'] == 0]['mean'].dropna()
    failure_mean = feature_df[feature_df['Response'] == 1]['mean'].dropna()
    
    plt.hist(normal_mean, bins=30, alpha=0.7, label='ì •ìƒí’ˆ', color='green', density=True)
    plt.hist(failure_mean, bins=30, alpha=0.7, label='ë¶ˆëŸ‰í’ˆ', color='red', density=True)
    plt.xlabel('ì¸¡ì •ê°’ í‰ê· ')
    plt.ylabel('ë°€ë„')
    plt.title('ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ: í‰ê· ê°’')
    plt.legend()
    
    # 8. í‘œì¤€í¸ì°¨ ë¹„êµ
    plt.subplot(2, 4, 8)
    normal_std = feature_df[feature_df['Response'] == 0]['std'].dropna()
    failure_std = feature_df[feature_df['Response'] == 1]['std'].dropna()
    
    plt.hist(normal_std, bins=30, alpha=0.7, label='ì •ìƒí’ˆ', color='green', density=True)
    plt.hist(failure_std, bins=30, alpha=0.7, label='ë¶ˆëŸ‰í’ˆ', color='red', density=True)
    plt.xlabel('ì¸¡ì •ê°’ í‘œì¤€í¸ì°¨')
    plt.ylabel('ë°€ë„')
    plt.title('ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ: ë³€ë™ì„±')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('C:/Users/ASUS/bosch/bosch_analysis_results.png', dpi=300, bbox_inches='tight')
    print("   ì‹œê°í™” ì €ì¥ë¨: bosch_analysis_results.png")
    
    plt.show()

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    try:
        # 1. ì „ì²´ ê°œìš”
        files = analyze_data_overview()
        
        # 2. ìˆ«ì ë°ì´í„° ìƒ˜í”Œ ë¶„ì„
        data_dir = "C:/Users/ASUS/bosch/data/"
        df = analyze_train_numeric_sample(data_dir, sample_size=100000)
        
        # 3. íŠ¹ì§• ê³µí•™
        feature_df = create_basic_features(df)
        
        # 4. ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = analyze_correlations(feature_df)
        
        # 5. ì‹œê°í™”
        create_visualizations(df, feature_df, correlations)
        
        # 6. ìš”ì•½ ë¦¬í¬íŠ¸
        print(f"\n" + "="*60)
        print("ğŸ“‹ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        failure_rate = df['Response'].mean()
        total_features = len(df.columns) - 2
        useful_features = len([col for col in df.columns if df[col].isnull().sum() < len(df) * 0.9])
        
        print(f"â€¢ ë°ì´í„° í¬ê¸°: {df.shape[0]:,} x {df.shape[1]:,}")
        print(f"â€¢ ë¶ˆëŸ‰ë¥ : {failure_rate:.2%} (ë§¤ìš° ë¶ˆê· í˜•)")
        print(f"â€¢ ì „ì²´ íŠ¹ì§•: {total_features:,}ê°œ")
        print(f"â€¢ ìœ ìš©í•œ íŠ¹ì§•: {useful_features:,}ê°œ (90% ë¯¸ë§Œ ê²°ì¸¡)")
        print(f"â€¢ ìƒì„±ëœ ì§‘ê³„ íŠ¹ì§•: {len(feature_df.columns)}ê°œ")
        
        if len(correlations) > 0:
            max_corr = correlations.abs().max()
            print(f"â€¢ ìµœê³  ìƒê´€ê³„ìˆ˜: {max_corr:.4f}")
        
        print(f"\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
        print(f"  1. ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë°ì´í„° - íŠ¹ë³„í•œ ì²˜ë¦¬ í•„ìš”")
        print(f"  2. ëŒ€ë¶€ë¶„ íŠ¹ì§•ì´ í¬ì†Œ - ì§‘ê³„ íŠ¹ì§•ì´ ì¤‘ìš”")
        print(f"  3. ê²°ì¸¡ê°’ íŒ¨í„´ ìì²´ê°€ ì¤‘ìš”í•œ ì •ë³´")
        print(f"  4. ìŠ¤í…Œì´ì…˜ë³„ ë¶„ì„ì´ íš¨ê³¼ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del df, feature_df
        gc.collect()
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()