{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosch Production Line Performance Analysis\n",
    "\n",
    "## Competition Overview\n",
    "\n",
    "This notebook analyzes the Bosch Production Line Performance dataset from Kaggle competition.\n",
    "\n",
    "**Goal**: Predict internal failures using thousands of measurements and tests made for each component along the assembly line.\n",
    "\n",
    "**Evaluation Metric**: Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "**Dataset Characteristics**:\n",
    "- Highly imbalanced dataset (failure rate < 1%)\n",
    "- Large number of features (thousands)\n",
    "- Three types of features: numeric, categorical, and date\n",
    "- Anonymous feature names for confidentiality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, classification_report\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = './data/'\n",
    "\n",
    "# List all files in the data directory\n",
    "data_files = os.listdir(data_dir)\n",
    "print(\"Available files:\")\n",
    "for file in data_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract zip files if not already extracted\n",
    "def extract_zip_files(data_dir):\n",
    "    zip_files = [f for f in os.listdir(data_dir) if f.endswith('.zip')]\n",
    "    \n",
    "    for zip_file in zip_files:\n",
    "        file_path = os.path.join(data_dir, zip_file)\n",
    "        csv_file = zip_file.replace('.zip', '')\n",
    "        csv_path = os.path.join(data_dir, csv_file)\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Extracting {zip_file}...\")\n",
    "            with ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(data_dir)\n",
    "            print(f\"  Extracted to {csv_file}\")\n",
    "        else:\n",
    "            print(f\"{csv_file} already exists\")\n",
    "\n",
    "extract_zip_files(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration - Sample Loading\n",
    "\n",
    "Due to the large size of the dataset, we'll first load a sample to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of numeric data to understand structure\n",
    "print(\"Loading sample of train_numeric data...\")\n",
    "train_numeric_sample = pd.read_csv(data_dir + 'train_numeric.csv', nrows=10000)\n",
    "print(f\"Sample shape: {train_numeric_sample.shape}\")\n",
    "print(f\"\\nFirst few columns: {list(train_numeric_sample.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target variable (Response) distribution:\")\n",
    "print(train_numeric_sample['Response'].value_counts())\n",
    "print(f\"\\nFailure rate: {train_numeric_sample['Response'].mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the sample\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Number of features: {len(train_numeric_sample.columns) - 2}\")  # Excluding Id and Response\n",
    "print(f\"Number of samples: {len(train_numeric_sample)}\")\n",
    "print(f\"\\nMissing values percentage:\")\n",
    "missing_percent = (train_numeric_sample.isnull().sum() / len(train_numeric_sample) * 100).sort_values(ascending=False)\n",
    "print(missing_percent.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature groups based on naming patterns\n",
    "def analyze_feature_groups(df):\n",
    "    feature_groups = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in ['Id', 'Response']:\n",
    "            # Extract feature group from column name\n",
    "            parts = col.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                group = parts[0] + '_' + parts[1]\n",
    "                if group not in feature_groups:\n",
    "                    feature_groups[group] = []\n",
    "                feature_groups[group].append(col)\n",
    "    \n",
    "    # Sort groups by number of features\n",
    "    sorted_groups = sorted(feature_groups.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    print(\"Feature Groups (Top 10):\")\n",
    "    for group, features in sorted_groups[:10]:\n",
    "        print(f\"  {group}: {len(features)} features\")\n",
    "    \n",
    "    return feature_groups\n",
    "\n",
    "feature_groups = analyze_feature_groups(train_numeric_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate missing percentage for each column\n",
    "missing_df = pd.DataFrame({\n",
    "    'column': train_numeric_sample.columns,\n",
    "    'missing_percent': (train_numeric_sample.isnull().sum() / len(train_numeric_sample) * 100)\n",
    "})\n",
    "\n",
    "# Plot histogram of missing percentages\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(missing_df['missing_percent'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Missing Percentage (%)')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Distribution of Missing Data Across Features')\n",
    "\n",
    "# Plot cumulative distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_missing = np.sort(missing_df['missing_percent'].values)\n",
    "plt.plot(range(len(sorted_missing)), sorted_missing)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Missing Percentage (%)')\n",
    "plt.title('Cumulative Missing Data Pattern')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory-Efficient Data Processing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df):\n",
    "    \"\"\"Reduce memory usage by downcasting numeric types\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage of dataframe: {start_mem:.2f} MB')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply memory reduction to sample\n",
    "train_numeric_sample = reduce_memory_usage(train_numeric_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_features(df):\n",
    "    \"\"\"Create basic aggregate features\"\"\"\n",
    "    feature_df = pd.DataFrame()\n",
    "    feature_df['Id'] = df['Id']\n",
    "    \n",
    "    # Count of non-null values per row\n",
    "    feature_df['count_non_null'] = df.drop(['Id', 'Response'], axis=1, errors='ignore').count(axis=1)\n",
    "    \n",
    "    # Count of zero values per row\n",
    "    feature_df['count_zeros'] = (df.drop(['Id', 'Response'], axis=1, errors='ignore') == 0).sum(axis=1)\n",
    "    \n",
    "    # Basic statistics per row\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(['Id', 'Response'], errors='ignore')\n",
    "    feature_df['mean'] = df[numeric_cols].mean(axis=1)\n",
    "    feature_df['std'] = df[numeric_cols].std(axis=1)\n",
    "    feature_df['min'] = df[numeric_cols].min(axis=1)\n",
    "    feature_df['max'] = df[numeric_cols].max(axis=1)\n",
    "    feature_df['median'] = df[numeric_cols].median(axis=1)\n",
    "    \n",
    "    # Percentage of missing values\n",
    "    feature_df['missing_percent'] = df[numeric_cols].isnull().sum(axis=1) / len(numeric_cols)\n",
    "    \n",
    "    if 'Response' in df.columns:\n",
    "        feature_df['Response'] = df['Response']\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "# Create features for sample\n",
    "engineered_features = create_basic_features(train_numeric_sample)\n",
    "print(\"Engineered features shape:\", engineered_features.shape)\n",
    "print(\"\\nFeature columns:\", list(engineered_features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation with target\n",
    "if 'Response' in engineered_features.columns:\n",
    "    correlations = engineered_features.drop(['Id', 'Response'], axis=1).corrwith(engineered_features['Response'])\n",
    "    correlations = correlations.sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    correlations.plot(kind='bar')\n",
    "    plt.title('Correlation of Engineered Features with Target')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature Correlations with Target:\")\n",
    "    print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Processing Strategy for Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, feature_list=None):\n",
    "    \"\"\"Process a chunk of data\"\"\"\n",
    "    # Select only specified features if provided\n",
    "    if feature_list is not None:\n",
    "        available_cols = [col for col in feature_list if col in chunk.columns]\n",
    "        chunk = chunk[available_cols + ['Id', 'Response']]\n",
    "    \n",
    "    # Reduce memory\n",
    "    chunk = reduce_memory_usage(chunk)\n",
    "    \n",
    "    # Create engineered features\n",
    "    features = create_basic_features(chunk)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example of processing in chunks (commented out for sample notebook)\n",
    "# chunk_size = 50000\n",
    "# chunks = []\n",
    "# for chunk in pd.read_csv(data_dir + 'train_numeric.csv', chunksize=chunk_size):\n",
    "#     processed_chunk = process_chunk(chunk)\n",
    "#     chunks.append(processed_chunk)\n",
    "#     gc.collect()\n",
    "# \n",
    "# full_features = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(\"Chunk processing strategy defined for handling large dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(df, target_col='Response', threshold=0.95):\n",
    "    \"\"\"Select features based on missing value threshold and variance\"\"\"\n",
    "    \n",
    "    # Remove features with too many missing values\n",
    "    missing_percent = df.isnull().sum() / len(df)\n",
    "    keep_cols = missing_percent[missing_percent < threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Features after missing value filter: {len(keep_cols)} / {len(df.columns)}\")\n",
    "    \n",
    "    # Remove features with zero or very low variance\n",
    "    numeric_cols = df[keep_cols].select_dtypes(include=[np.number]).columns\n",
    "    variances = df[numeric_cols].var()\n",
    "    keep_cols = variances[variances > 0.01].index.tolist()\n",
    "    \n",
    "    # Always keep Id and Response\n",
    "    if 'Id' not in keep_cols:\n",
    "        keep_cols.append('Id')\n",
    "    if target_col in df.columns and target_col not in keep_cols:\n",
    "        keep_cols.append(target_col)\n",
    "    \n",
    "    print(f\"Features after variance filter: {len(keep_cols)}\")\n",
    "    \n",
    "    return keep_cols\n",
    "\n",
    "# Apply feature selection to sample\n",
    "important_features = select_important_features(train_numeric_sample)\n",
    "print(f\"\\nSelected {len(important_features)} features from {len(train_numeric_sample.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modeling libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "print(\"Model libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df, target_col='Response'):\n",
    "    \"\"\"Prepare data for modeling\"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(['Id', target_col], axis=1, errors='ignore')\n",
    "    y = df[target_col] if target_col in df.columns else None\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Prepare sample data\n",
    "X_sample, y_sample, scaler = prepare_data_for_modeling(engineered_features)\n",
    "print(f\"Prepared data shape: X={X_sample.shape}, y={y_sample.shape if y_sample is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X, y, strategy='undersample', ratio=0.1):\n",
    "    \"\"\"Handle class imbalance\"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = SMOTE(sampling_strategy=ratio, random_state=42)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(sampling_strategy=ratio, random_state=42)\n",
    "    else:\n",
    "        return X, y\n",
    "    \n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    print(f\"Resampled data: {len(y_resampled)} samples\")\n",
    "    print(f\"Class distribution: {pd.Series(y_resampled).value_counts()}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Example of handling imbalance (commented for sample)\n",
    "# X_balanced, y_balanced = handle_imbalance(X_sample, y_sample, strategy='undersample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "1. **Extreme Class Imbalance**: The failure rate is less than 1%, requiring special handling\n",
    "2. **High Dimensionality**: Thousands of features with high sparsity\n",
    "3. **Missing Data**: Many features have significant missing values\n",
    "\n",
    "### Recommended Approach:\n",
    "\n",
    "#### 1. Data Processing:\n",
    "- Process data in chunks due to size\n",
    "- Implement aggressive feature selection\n",
    "- Create aggregate features per production line/station\n",
    "\n",
    "#### 2. Feature Engineering:\n",
    "- Count-based features (non-nulls, zeros)\n",
    "- Statistical aggregations\n",
    "- Time-based features from date files\n",
    "- Interaction features between stations\n",
    "\n",
    "#### 3. Modeling Strategy:\n",
    "- Use Matthews Correlation Coefficient (MCC) for evaluation\n",
    "- Try ensemble methods (XGBoost, LightGBM)\n",
    "- Implement cross-validation with stratification\n",
    "- Consider anomaly detection approaches\n",
    "\n",
    "#### 4. Class Imbalance Handling:\n",
    "- Adjust class weights\n",
    "- Try SMOTE for oversampling\n",
    "- Use ensemble methods with balanced subsampling\n",
    "- Optimize threshold for prediction\n",
    "\n",
    "### Code Template for Full Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for full dataset processing\n",
    "def full_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline for Bosch dataset\n",
    "    \"\"\"\n",
    "    # 1. Extract all files\n",
    "    extract_zip_files(data_dir)\n",
    "    \n",
    "    # 2. Process numeric data in chunks\n",
    "    chunk_size = 50000\n",
    "    numeric_features = []\n",
    "    \n",
    "    for chunk in pd.read_csv(data_dir + 'train_numeric.csv', chunksize=chunk_size):\n",
    "        # Process chunk\n",
    "        chunk = reduce_memory_usage(chunk)\n",
    "        features = create_basic_features(chunk)\n",
    "        numeric_features.append(features)\n",
    "        gc.collect()\n",
    "    \n",
    "    # 3. Combine features\n",
    "    full_features = pd.concat(numeric_features, ignore_index=True)\n",
    "    \n",
    "    # 4. Add categorical features (simplified)\n",
    "    # Process categorical data similarly\n",
    "    \n",
    "    # 5. Add date features\n",
    "    # Process date data for time-based features\n",
    "    \n",
    "    # 6. Train model\n",
    "    X, y, scaler = prepare_data_for_modeling(full_features)\n",
    "    \n",
    "    # 7. Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 8. Train XGBoost\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=100,  # Handle imbalance\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 9. Evaluate and predict\n",
    "    # ...\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Full pipeline template defined\")\n",
    "print(\"\\nTo run the full pipeline, uncomment and execute:\")\n",
    "print(\"# model = full_pipeline()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission template\n",
    "submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "print(\"Submission template shape:\", submission.shape)\n",
    "print(\"\\nSubmission columns:\", submission.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, test_features, submission_template):\n",
    "    \"\"\"\n",
    "    Create submission file\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission = submission_template.copy()\n",
    "    submission['Response'] = predictions\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved to submission.csv\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "print(\"Submission function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive framework for analyzing the Bosch Production Line Performance dataset:\n",
    "\n",
    "1. **Data Understanding**: Explored the structure and characteristics of the dataset\n",
    "2. **Memory Management**: Implemented techniques to handle large files efficiently\n",
    "3. **Feature Engineering**: Created aggregate features to capture patterns\n",
    "4. **Imbalance Handling**: Prepared strategies for the extreme class imbalance\n",
    "5. **Pipeline Template**: Provided a complete processing pipeline\n",
    "\n",
    "To proceed with full analysis:\n",
    "1. Run the extraction to get all CSV files\n",
    "2. Implement the chunk processing for the full dataset\n",
    "3. Experiment with different feature engineering approaches\n",
    "4. Train and validate models using MCC metric\n",
    "5. Optimize prediction threshold for best performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}