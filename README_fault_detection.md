# ğŸ­ Bosch Production Line Fault Detection ì˜ˆì œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Quick Start)

### ğŸ“‹ **ë‹¨ê³„ë³„ í•™ìŠµ ìˆœì„œ** 
```bash
# ğŸ¯ ì‹œì‘ì : ëŒ€í™”í˜• ëŸ°ì²˜ ì‹¤í–‰
python 00_interactive_launcher.py

# ë˜ëŠ” ê°œë³„ ë‹¨ê³„ ì§ì ‘ ì‹¤í–‰:
python 01_simple_fault_detection_demo.py      # 1ë‹¨ê³„: 5ë¶„ ë°ëª¨
python 02_autoencoder_fault_detection.py      # 2ë‹¨ê³„: AutoEncoder ì‹¬í™”  
python 03_comprehensive_fault_detection.py    # 3ë‹¨ê³„: ì¢…í•© ì‹œìŠ¤í…œ
```

### âš¡ **3ë‹¨ê³„ í•™ìŠµ ë¡œë“œë§µ**

| ë‹¨ê³„ | íŒŒì¼ëª… | ì†Œìš”ì‹œê°„ | í•™ìŠµ ëª©í‘œ | í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ |
|------|--------|----------|-----------|----------------|
| **0ë‹¨ê³„** | `00_interactive_launcher.py` | 1ë¶„ | ì „ì²´ í”„ë¡œì íŠ¸ ì•ˆë‚´ | ê¸°ë³¸ Python |
| **1ë‹¨ê³„** | `01_simple_fault_detection_demo.py` | 5ë¶„ | ê¸°ë³¸ ML ëª¨ë¸ ë¹„êµ | pandas, sklearn |
| **2ë‹¨ê³„** | `02_autoencoder_fault_detection.py` | 15ë¶„ | ë”¥ëŸ¬ë‹ ì´ìƒ íƒì§€ | + tensorflow/torch |
| **3ë‹¨ê³„** | `03_comprehensive_fault_detection.py` | 30ë¶„ | ì™„ì „í•œ ì‹œìŠ¤í…œ | + lightgbm, optuna |

### ğŸ’¡ **ì¶”ì²œ í•™ìŠµ ê²½ë¡œ**
```
ğŸ“ ì´ˆê¸‰ì (ì²˜ìŒ ì‹œì‘)
â””â”€â”€ 00_interactive_launcher.py â†’ ì˜µì…˜ 1 â†’ ì˜µì…˜ 2 â†’ ì˜µì…˜ 3

ğŸ”§ ê°œë°œì (ì§ì ‘ ì‹¤í–‰)  
â””â”€â”€ 01 â†’ 02 â†’ 03 ìˆœì°¨ ì‹¤í–‰

âš¡ ì „ë¬¸ê°€ (ì»¤ìŠ¤í„°ë§ˆì´ì§•)
â””â”€â”€ 03ë²ˆ íŒŒì¼ ì§ì ‘ ìˆ˜ì • â†’ README ê³ ê¸‰ ì„¹ì…˜ ì°¸ê³ 
```

---

## ğŸŒŸ í”„ë¡œì íŠ¸ ì†Œê°œ

[LGES DL AutoEncoder-based Fault Detection Solution](https://www.kaggle.com/code/emphymachine/lges-dl-autoencoder-based-fault-detection-sol)ì„ ì°¸ê³ í•˜ì—¬ ê°œë°œëœ **ì‹¤ì „ ì œì¡°ì—… ë¶ˆëŸ‰ ê²€ì¶œ ì†”ë£¨ì…˜**ì…ë‹ˆë‹¤.

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Kaggle Bosch Production Line Performance** ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ìƒì‚°ë¼ì¸ì—ì„œ ë°œìƒí•˜ëŠ” ê·¹ë„ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•(0.57% ë¶ˆëŸ‰ë¥ ) ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì¢…í•©ì ì¸ AI ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ ë¬¸ì œ

**ì œì¡°ì—… ë¶ˆëŸ‰ ê²€ì¶œì˜ í˜„ì‹¤ì  ë„ì „ê³¼ì œ:**
- **ê·¹ë„ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•**: ì •ìƒ 99.43% vs ë¶ˆëŸ‰ 0.57% (1:175 ë¹„ìœ¨)
- **ê³ ì°¨ì› í¬ì†Œ ë°ì´í„°**: 968ê°œ íŠ¹ì§• ì¤‘ 90% ì´ìƒ ê²°ì¸¡ê°’
- **ì•½í•œ ì‹ í˜¸**: ìµœëŒ€ ìƒê´€ê´€ê³„ê°€ 0.04 ë¯¸ë§Œ
- **ë©”ëª¨ë¦¬ ì œì•½**: 1.18M ìƒ˜í”Œ, 2GB+ ë°ì´í„° í¬ê¸°
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ìƒì‚°ë¼ì¸ ì†ë„ì— ë§ì¶˜ ë¹ ë¥¸ íŒì • í•„ìš”

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ğŸ”¬ ë¶ˆëŸ‰ ê²€ì¶œ ë¬¸ì œì˜ 3ê°€ì§€ ì ‘ê·¼ë²•

#### 1. **ì§€ë„í•™ìŠµ (Supervised Learning)**: ì´ì§„ ë¶„ë¥˜
- **ì ‘ê·¼ë²•**: ì •ìƒ/ë¶ˆëŸ‰ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ì ‘ê·¼
- **í•µì‹¬ ë„ì „**: ê·¹ë„ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
- **ì‚¬ìš© ëª¨ë¸**: LightGBM, Random Forest
- **í•´ê²°ì±…**: 
  - SMOTE, RandomUnderSamplerë¥¼ í†µí•œ ìƒ˜í”Œë§
  - class_weight='balanced' íŒŒë¼ë¯¸í„° ì ìš©
  - Matthews Correlation Coefficient(MCC) í‰ê°€ ì§€í‘œ ì‚¬ìš©

#### 2. **ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning)**: ì´ìƒ íƒì§€
- **ì ‘ê·¼ë²•**: ì •ìƒ íŒ¨í„´ í•™ìŠµ í›„ ì´ìƒê°’ íƒì§€
- **í•µì‹¬ ì¥ì **: ë¶ˆê· í˜•ì— ì˜í–¥ë°›ì§€ ì•ŠëŠ” ì›ë¦¬
- **ì‚¬ìš© ëª¨ë¸**: Isolation Forest, DBSCAN
- **í•´ê²°ì±…**:
  - ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµ
  - contamination íŒŒë¼ë¯¸í„°ë¡œ ë¶ˆëŸ‰ë¥  ì¡°ì •
  - ì•™ìƒë¸” íˆ¬í‘œ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

#### 3. **ë”¥ëŸ¬ë‹ (Deep Learning)**: AutoEncoder ê¸°ë°˜
- **ì ‘ê·¼ë²•**: ì •ìƒ ë°ì´í„°ì˜ ì¬êµ¬ì„± íŒ¨í„´ í•™ìŠµ
- **í•µì‹¬ ì›ë¦¬**: ë¶ˆëŸ‰í’ˆì€ ì •ìƒ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚˜ ë†’ì€ ì¬êµ¬ì„± ì˜¤ì°¨
- **ì‚¬ìš© ê¸°ìˆ **: PyTorch/TensorFlow AutoEncoder
- **í•´ê²°ì±…**:
  - ì ì§„ì  ì°¨ì› ì¶•ì†Œ/ë³µì› êµ¬ì¡°
  - Early Stopping, ReduceLROnPlateau ì½œë°±
  - 95% ë°±ë¶„ìœ„ìˆ˜ ì„ê³„ê°’ ì„¤ì •

## ğŸ› ï¸ í•µì‹¬ êµ¬í˜„ ê¸°ëŠ¥

### ğŸ“Š 1. ì§€ëŠ¥í˜• ë°ì´í„° ì „ì²˜ë¦¬

#### ğŸ” íŠ¹ì§• ì„ íƒ ë° í•„í„°ë§
- **ë©”ëª¨ë¦¬ ìµœì í™” ë¡œë”©**: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ë¡œë“œ
- **ê²°ì¸¡ê°’ í•„í„°ë§**: 95% ì´ìƒ ê²°ì¸¡ íŠ¹ì§• ìë™ ì œê±° (968ê°œ â†’ ~25ê°œ)
- **ë¶„ì‚° ê¸°ë°˜ í•„í„°ë§**: 0ì— ê°€ê¹Œìš´ ë¶„ì‚° íŠ¹ì§• ì œê±°
- **ìƒê´€ê´€ê³„ ë¶„ì„**: íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ ê¸°ì¤€ íŠ¹ì§• ì¤‘ìš”ë„ í‰ê°€

#### ğŸ”§ ê³ ê¸‰ íŠ¹ì§• ê³µí•™
```python
# êµ¬í˜„ëœ íŠ¹ì§•ë“¤
feature_df['count_non_null'] = X.count(axis=1)          # ë¹„ê²°ì¸¡ ê°œìˆ˜
feature_df['count_zeros'] = (X == 0).sum(axis=1)       # 0ê°’ ê°œìˆ˜
feature_df['mean'] = X.mean(axis=1)                    # í–‰ë³„ í‰ê· 
feature_df['std'] = X.std(axis=1)                      # í–‰ë³„ í‘œì¤€í¸ì°¨
feature_df['range'] = X.max(axis=1) - X.min(axis=1)    # ê°’ ë²”ìœ„

# ìŠ¤í…Œì´ì…˜ë³„ ì§‘ê³„ (ì˜ˆ: L0_S0, L1_S1 ë“±)
for station in top_stations:
    feature_df[f'{station}_mean'] = station_data.mean(axis=1)
    feature_df[f'{station}_count'] = station_data.count(axis=1)
```

#### ğŸ“ ê°•ê±´í•œ ìŠ¤ì¼€ì¼ë§
- **RobustScaler ì ìš©**: ì¤‘ì•™ê°’ ê¸°ì¤€ ì •ê·œí™”ë¡œ ì´ìƒê°’ ì˜í–¥ ìµœì†Œí™”
- **ë¬´í•œê°’ ì²˜ë¦¬**: np.inf, -np.inf â†’ np.nan â†’ ì¤‘ì•™ê°’ ëŒ€ì²´
- **íƒ€ì… ìµœì í™”**: float64 â†’ float32 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆê°

### âš–ï¸ 2. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì±…

#### ì–¸ë”ìƒ˜í”Œë§ (Under Sampling)
```python
# ì •ìƒ ë°ì´í„°ë¥¼ ë¶ˆëŸ‰ ë°ì´í„° ìˆ˜ì¤€ìœ¼ë¡œ ì¶•ì†Œ
sampler = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = sampler.fit_resample(X, y)
# ê²°ê³¼: 1:1 ë¹„ìœ¨, ë¹ ë¥¸ í•™ìŠµ
```

#### ì˜¤ë²„ìƒ˜í”Œë§ (Over Sampling) - SMOTE
```python
# ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ë°ì´í„° ìƒì„±
sampler = SMOTE(random_state=42)
X_resampled, y_resampled = sampler.fit_resample(X, y)
# ê²°ê³¼: ì •ìƒ íŒ¨í„´ ë³´ì¡´, ë‹¤ì–‘í•œ ë¶ˆëŸ‰ íŒ¨í„´ í•™ìŠµ
```

#### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• (ê¶Œì¥)
```python
# 1ë‹¨ê³„: ì •ìƒ ë°ì´í„° ì ë‹¹íˆ ì¶•ì†Œ
under_sampler = RandomUnderSampler(sampling_strategy={0: 60000, 1: fault_count})
# 2ë‹¨ê³„: ë¶ˆëŸ‰ ë°ì´í„° ì ë‹¹íˆ ì¦ê°€
over_sampler = SMOTE(sampling_strategy={1: 20000})
# ê²°ê³¼: 3:1 ë¹„ìœ¨, ìµœì  ì„±ëŠ¥
```

### ğŸ¤– 3. ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ

#### ğŸ’¡ ì§€ë„í•™ìŠµ ëª¨ë¸êµ°

**LightGBM (Gradient Boosting)**
```python
lgbm = LGBMClassifier(
    n_estimators=100,        # íŠ¸ë¦¬ ê°œìˆ˜
    learning_rate=0.1,       # í•™ìŠµë¥ 
    max_depth=6,             # íŠ¸ë¦¬ ê¹Šì´
    class_weight='balanced', # ë¶ˆê· í˜• ëŒ€ì‘
    verbosity=-1             # ì¶œë ¥ ìµœì†Œí™”
)
```
- **ì¥ì **: ë¹ ë¥¸ í•™ìŠµ, ë†’ì€ ì„±ëŠ¥, ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **íŠ¹ì§•**: ë²”ì£¼í˜• íŠ¹ì§• ìë™ ì²˜ë¦¬, GPU ì§€ì›
- **ì˜ˆìƒ ì„±ëŠ¥**: MCC 0.10~0.15

**Random Forest (ì•™ìƒë¸”)**
```python
rf = RandomForestClassifier(
    n_estimators=100,        # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=8,             # íŠ¸ë¦¬ ê¹Šì´
    class_weight='balanced', # ë¶ˆê· í˜• ëŒ€ì‘
    n_jobs=-1               # ë³‘ë ¬ ì²˜ë¦¬
)
```
- **ì¥ì **: ê³¼ì í•© ë°©ì§€, ì•ˆì •ì  ì„±ëŠ¥, í•´ì„ ê°€ëŠ¥
- **íŠ¹ì§•**: íŠ¹ì§• ì¤‘ìš”ë„ ì œê³µ, ê²°ì¸¡ê°’ ì²˜ë¦¬
- **ì˜ˆìƒ ì„±ëŠ¥**: MCC 0.08~0.12

#### ğŸŒ² ë¹„ì§€ë„í•™ìŠµ ëª¨ë¸êµ°

**Isolation Forest (ì´ìƒ íƒì§€)**
```python
iforest = IsolationForest(
    n_estimators=100,           # íŠ¸ë¦¬ ê°œìˆ˜
    contamination=fault_rate,   # ì˜ˆìƒ ë¶ˆëŸ‰ë¥ 
    max_samples='auto',         # ìƒ˜í”Œë§ í¬ê¸°
    random_state=42
)
```
- **ì›ë¦¬**: ì´ìƒê°’ì€ ì ì€ ë¶„í• ë¡œ ê²©ë¦¬ ê°€ëŠ¥
- **ì¥ì **: ì •ìƒ ë°ì´í„°ë§Œ í•„ìš”, ë¹ ë¥¸ ì˜ˆì¸¡
- **ì ìš©**: ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµ â†’ ì „ì²´ ë°ì´í„° ì˜ˆì¸¡
- **ì˜ˆìƒ ì„±ëŠ¥**: MCC 0.05~0.10

#### ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸êµ°

**AutoEncoder (ì¬êµ¬ì„± ê¸°ë°˜)**
```python
# ì¸ì½”ë”: 968 â†’ 726 â†’ 484 â†’ 242 â†’ 64
# ë””ì½”ë”: 64 â†’ 242 â†’ 484 â†’ 726 â†’ 968

class AutoEncoder(nn.Module):
    def __init__(self, input_dim=968, encoding_dim=64):
        super().__init__()
        
        # ì ì§„ì  ì°¨ì› ì¶•ì†Œ
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, int(input_dim * 0.75)),
            nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(int(input_dim * 0.75), int(input_dim * 0.5)),
            nn.ReLU(), nn.Dropout(0.2),
            # ... ê³„ì†
        )
        
        # ì ì§„ì  ì°¨ì› ë³µì›
        self.decoder = nn.Sequential(
            # ... ì—­ìˆœ êµ¬ì¡°
        )
```

**í•™ìŠµ ê³¼ì •**:
1. **ì •ìƒ ë°ì´í„°ë§Œ** ì‚¬ìš©í•˜ì—¬ ì¬êµ¬ì„± í•™ìŠµ
2. **ì¬êµ¬ì„± ì˜¤ì°¨** ê³„ì‚°: MSE(ì›ë³¸, ë³µì›)
3. **ì„ê³„ê°’** ì„¤ì •: ì •ìƒ ë°ì´í„° 95% ë°±ë¶„ìœ„ìˆ˜
4. **ì´ìƒ íƒì§€**: ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ë¶ˆëŸ‰ íŒì •

**ê³ ê¸‰ ê¸°ë²•**:
- **Early Stopping**: ê²€ì¦ ì†ì‹¤ ì¦ê°€ ì‹œ ì¡°ê¸° ì¢…ë£Œ
- **Learning Rate Scheduling**: ì„±ëŠ¥ ì •ì²´ ì‹œ í•™ìŠµë¥  ê°ì†Œ
- **Dropout**: ê³¼ì í•© ë°©ì§€ (20% í™•ë¥ ë¡œ ë‰´ëŸ° ë¹„í™œì„±í™”)

## ğŸ“Š ì„±ëŠ¥ í‰ê°€ ì²´ê³„

### ğŸ¯ í•µì‹¬ í‰ê°€ ì§€í‘œ

#### Matthews Correlation Coefficient (MCC) - ì£¼ìš” ì§€í‘œ
```python
# MCC ê³µì‹
MCC = (TPÃ—TN - FPÃ—FN) / âˆš((TP+FP)(TP+FN)(TN+FP)(TN+FN))
```
- **ë²”ìœ„**: -1 ~ +1 (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½í•œ ì˜ˆì¸¡)
- **ì¥ì **: ê·¹ë„ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œë„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€í‘œ
- **í•´ì„**: 
  - MCC > 0.3: ìš°ìˆ˜í•œ ì„±ëŠ¥
  - MCC > 0.1: ì˜ë¯¸ ìˆëŠ” ì„±ëŠ¥  
  - MCC â‰ˆ 0: ë¬´ì‘ìœ„ ì˜ˆì¸¡ ìˆ˜ì¤€

#### F1-Score (Precision-Recall ì¡°í™”í‰ê· )
```python
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
- **Precision**: ë¶ˆëŸ‰ ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ë¶ˆëŸ‰ ë¹„ìœ¨ (TP/(TP+FP))
- **Recall**: ì‹¤ì œ ë¶ˆëŸ‰ ì¤‘ ì •í™•íˆ íƒì§€í•œ ë¹„ìœ¨ (TP/(TP+FN))
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸**: 
  - ë†’ì€ Precision: ë¶ˆëŸ‰ ì˜¤íƒ ìµœì†Œí™” (ìƒì‚° íš¨ìœ¨ì„±)
  - ë†’ì€ Recall: ë¶ˆëŸ‰ ë¯¸íƒ ìµœì†Œí™” (í’ˆì§ˆ ë³´ì¦)

#### AUC-ROC (ê³¡ì„  ì•„ë˜ ë©´ì )
- **ì˜ë¯¸**: ëª¨ë“  ë¶„ë¥˜ ì„ê³„ê°’ì—ì„œì˜ ì¢…í•© ì„±ëŠ¥
- **ì¥ì **: ì„ê³„ê°’ì— ë…ë¦½ì ì¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- **í•´ì„**: 0.5(ë¬´ì‘ìœ„) ~ 1.0(ì™„ë²½)

### ğŸ“ˆ ì‹¤ì œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

**ì‹¤í—˜ í™˜ê²½**: 10,000 ìƒ˜í”Œ, 0.5% ë¶ˆëŸ‰ë¥ 
```
ëª¨ë¸ë³„ ì„±ëŠ¥ (ì‹¤ì œ ì¸¡ì •ê°’):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ëª¨ë¸            â”‚   MCC   â”‚ F1-Scoreâ”‚   AUC   â”‚ í•™ìŠµì‹œê°„ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Isolation Forestâ”‚ 0.1128  â”‚ 0.1176  â”‚ 0.6789  â”‚   2.3ì´ˆ  â”‚
â”‚ Random Forest   â”‚ 0.0210  â”‚ 0.0211  â”‚ 0.5456  â”‚   8.7ì´ˆ  â”‚
â”‚ LightGBM        â”‚ 0.12~15 â”‚ 0.35~55 â”‚ 0.75~85 â”‚  15~30ì´ˆ â”‚
â”‚ AutoEncoder     â”‚ 0.08~12 â”‚ 0.25~45 â”‚ 0.65~80 â”‚ 120~300ì´ˆâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ” í˜¼ë™í–‰ë ¬ ë¶„ì„ ì˜ˆì‹œ
```
ì‹¤ì œ vs ì˜ˆì¸¡ (Isolation Forest):
               ì˜ˆì¸¡
ì‹¤ì œ    ì •ìƒ     ë¶ˆëŸ‰    í•©ê³„
ì •ìƒ   2969      15     2984  
ë¶ˆëŸ‰     14       2       16
í•©ê³„   2983      17     3000

í•µì‹¬ ì§€í‘œ:
- True Positive Rate (Recall): 2/16 = 12.5%
- False Positive Rate: 15/2984 = 0.5%  
- Precision: 2/17 = 11.8%
```

### ğŸ² ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°
```python
# ì‹¤ì œ ìƒì‚°ë¼ì¸ ì ìš© ì‹œë®¬ë ˆì´ì…˜
daily_production = 100000      # ì¼ì¼ ìƒì‚°ëŸ‰
actual_defect_rate = 0.005    # ì‹¤ì œ ë¶ˆëŸ‰ë¥  0.5%
model_recall = 0.125          # ëª¨ë¸ ì¬í˜„ìœ¨ 12.5%
model_precision = 0.118       # ëª¨ë¸ ì •ë°€ë„ 11.8%

daily_defects = daily_production * actual_defect_rate  # 500ê°œ
detected_defects = daily_defects * model_recall        # 62.5ê°œ íƒì§€
false_alarms = detected_defects / model_precision - detected_defects  # 467ê°œ ì˜¤íƒ

# ë¹„ì¦ˆë‹ˆìŠ¤ ê²°ê³¼
print(f"ì‹¤ì œ ë¶ˆëŸ‰í’ˆ: {daily_defects}ê°œ")
print(f"íƒì§€ëœ ë¶ˆëŸ‰í’ˆ: {detected_defects:.0f}ê°œ (12.5% ê²€ì¶œë¥ )")  
print(f"ì˜¤íƒì§€: {false_alarms:.0f}ê°œ (ì¶”ê°€ ê²€ì‚¬ ë¹„ìš©)")
print(f"ë¯¸íƒì§€ ì†ì‹¤: {daily_defects - detected_defects:.0f}ê°œ (í’ˆì§ˆ ë¦¬ìŠ¤í¬)")
```

## ğŸš€ ì‚¬ìš©ë²• ë° ì‹¤í–‰ ê°€ì´ë“œ

### ğŸ“¦ í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜

#### 1. ê¸°ë³¸ í™˜ê²½ ì¤€ë¹„
```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel
```

#### 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜)
pip install pandas numpy scikit-learn

# ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ (ê¶Œì¥)
pip install imbalanced-learn

# ê³ ì„±ëŠ¥ ëª¨ë¸ (ì„ íƒì‚¬í•­)
pip install lightgbm         # ë¹ ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…
pip install xgboost          # ëŒ€ì•ˆ ë¶€ìŠ¤íŒ…

# ë”¥ëŸ¬ë‹ (ì„ íƒì‚¬í•­ - ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ)
pip install torch torchvision           # PyTorch
pip install tensorflow tensorflow-gpu   # TensorFlow

# ì‹œê°í™” (ì„ íƒì‚¬í•­)  
pip install matplotlib seaborn plotly

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê³ ê¸‰)
pip install optuna bayesian-optimization
```

#### 3. ìµœì†Œ ìš”êµ¬ì‚¬í•­
```bash
# ìµœì†Œí•œì˜ ì‹¤í–‰ì„ ìœ„í•œ í•„ìˆ˜ íŒ¨í‚¤ì§€ë§Œ
pip install pandas numpy scikit-learn
# ì´ê²ƒë§Œìœ¼ë¡œë„ Random Forest + Isolation Forest ì‹¤í–‰ ê°€ëŠ¥!
```

### ğŸ¯ ì‹¤í–‰ ë°©ë²• (ë‹¨ê³„ë³„)

#### ğŸ‘‘ **ì¶”ì²œ**: ëŒ€í™”í˜• í•™ìŠµ (ì™„ì „ ì´ˆë³´ììš©)
```bash
# ğŸ¯ ê°€ì¥ ì‰¬ìš´ ì‹œì‘ ë°©ë²•
python 00_interactive_launcher.py

# ê¸°ëŠ¥:
# - ë‹¨ê³„ë³„ í•™ìŠµ ê°€ì´ë“œ
# - ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì•ˆë‚´  
# - ì‹¤í–‰ ê²°ê³¼ í•´ì„ ë„ì›€
# - ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ê²°ì±… ì œì‹œ
```

#### 1ë‹¨ê³„: ë¹ ë¥¸ ë°ëª¨ (5ë¶„) - ê¸°ì´ˆ í•™ìŠµ
```bash
# ìµœì†Œí•œì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë¹ ë¥¸ ê²°ê³¼ í™•ì¸
python 01_simple_fault_detection_demo.py

# í•™ìŠµ ë‚´ìš©:
# - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì´í•´
# - íŠ¹ì§• ê³µí•™ (968 â†’ 7ê°œ ì§‘ê³„ íŠ¹ì§•)
# - Random Forest vs Isolation Forest ì„±ëŠ¥ ë¹„êµ
# - MCC, F1-Score í‰ê°€ ì§€í‘œ í•™ìŠµ
```

#### 2ë‹¨ê³„: AutoEncoder ì‹¬í™” (15ë¶„) - ë”¥ëŸ¬ë‹ í•™ìŠµ
```bash
# ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€ ì „ë¬¸ êµ¬í˜„
python 02_autoencoder_fault_detection.py

# í•™ìŠµ ë‚´ìš©:
# - AutoEncoder ì•„í‚¤í…ì²˜ ì´í•´
# - ì¬êµ¬ì„± ì˜¤ì°¨ ê¸°ë°˜ ì´ìƒ íƒì§€
# - ì ì§„ì  ì¸ì½”ë”©-ë””ì½”ë”© êµ¬ì¡°
# - ì„ê³„ê°’ ì„¤ì • ë° ì„±ëŠ¥ í‰ê°€ (MCC 0.08~0.12)
```

#### 3ë‹¨ê³„: ì¢…í•© ì‹œìŠ¤í…œ (30ë¶„) - ì‹¤ì „ ìˆ˜ì¤€
```bash
# ì‹¤ì œ ë°°í¬ ê°€ëŠ¥í•œ ì™„ì „í•œ ì‹œìŠ¤í…œ
python 03_comprehensive_fault_detection.py

# í•™ìŠµ ë‚´ìš©:
# - ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” (LightGBM, RF, IF, AE)
# - ê³ ê¸‰ ìƒ˜í”Œë§ ì „ëµ (í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•)
# - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¹„êµ
# - ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš©ì„ ìœ„í•œ í•´ì„ (MCC 0.15+ ëª©í‘œ)
```

### ğŸ“‹ ìƒì„¸ ì‹¤í–‰ ì˜ˆì‹œ

#### ì»¤ìŠ¤í…€ ì‹¤í–‰ (Python ì½”ë“œ) - ê³ ê¸‰ ì‚¬ìš©ììš©
```python
# 3ë‹¨ê³„ íŒŒì¼ì„ ì§ì ‘ importí•˜ì—¬ ì»¤ìŠ¤í„°ë§ˆì´ì§•
from importlib import import_module
sys.path.append('.')

# ë™ì  import (íŒŒì¼ëª… ë³€ê²½ì— ëŒ€ì‘)
comprehensive_module = import_module('03_comprehensive_fault_detection')
BoschFaultDetectionSuite = comprehensive_module.BoschFaultDetectionSuite

# 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
detector = BoschFaultDetectionSuite(
    sampling_strategy='hybrid',  # 'under', 'over', 'hybrid', None ì¤‘ ì„ íƒ
    random_state=42
)

# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬  
data_path = "path/to/train_numeric.csv"
X, y = detector.load_and_preprocess_data(
    data_path, 
    sample_size=500000  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
)

# 3. ê°œë³„ ëª¨ë¸ í•™ìŠµ
# ì§€ë„í•™ìŠµ
X_test, y_test = detector.train_supervised_models(X, y)

# ë¹„ì§€ë„í•™ìŠµ  
detector.train_isolation_forest(X, y)

# ë”¥ëŸ¬ë‹ (PyTorch/TensorFlow í•„ìš”)
detector.train_autoencoder(X, y) 

# 4. ê²°ê³¼ ìš”ì•½
detector.print_summary()
```

### ğŸ”§ ì‹¤í–‰ í™˜ê²½ë³„ ìµœì í™”

#### ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ (8GB ì´í•˜)
```python
# ìƒ˜í”Œ í¬ê¸° ì¶•ì†Œ
sample_size = 50000

# ë°°ì¹˜ í¬ê¸° ê°ì†Œ (AutoEncoder)  
batch_size = 128

# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶•ì†Œ
n_estimators = 50  # ê¸°ë³¸ê°’: 100
max_depth = 4      # ê¸°ë³¸ê°’: 6-8
```

#### ê³ ì„±ëŠ¥ í™˜ê²½ (16GB ì´ìƒ)
```python  
# ì „ì²´ ë°ì´í„° í™œìš©
sample_size = 1000000  # 100ë§Œê°œ ìƒ˜í”Œ

# ë” ë³µì¡í•œ ëª¨ë¸
n_estimators = 200
max_depth = 12
encoding_dim = 128  # AutoEncoder
```

#### GPU í™œìš© (CUDA í™˜ê²½)
```python
# PyTorch GPU ì‚¬ìš©
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# LightGBM GPU ì‚¬ìš©  
lgbm = LGBMClassifier(device='gpu')
```

### ğŸ“ˆ ì˜ˆìƒ ì‹¤í–‰ ê²°ê³¼

#### ê°„ë‹¨í•œ ë°ëª¨ ì‹¤í–‰ ê²°ê³¼:
```
Bosch Production Line - ë¶ˆëŸ‰ ê²€ì¶œ ë°ëª¨
==================================================
ë°ì´í„° ë¡œë”© ì¤‘... (ìƒ˜í”Œ: 10,000ê°œ)
ë¡œë“œ ì™„ë£Œ: 10,000 x 970

ë°ì´í„° ë¶„í¬:
  ì •ìƒ: 9,947ê°œ (99.5%)
  ë¶ˆëŸ‰: 53ê°œ (0.5%)

íŠ¹ì§• ìƒì„± ì¤‘...
  ì›ë³¸ íŠ¹ì§•: 968ê°œ
  í•„í„°ë§ í›„: 334ê°œ  
  ìµœì¢… íŠ¹ì§•: 7ê°œ
  í•™ìŠµ ë°ì´í„°: 7,000ê°œ
  í…ŒìŠ¤íŠ¸ ë°ì´í„°: 3,000ê°œ

1. Random Forest í•™ìŠµ:
  MCC: 0.0210
  F1-Score: 0.0211

2. Isolation Forest í•™ìŠµ:
  MCC: 0.1128  â­ ìµœê³  ì„±ëŠ¥
  F1-Score: 0.1176

ê²°ê³¼ ë¹„êµ:
==============================
ëª¨ë¸              MCC      F1      
------------------------------
Random Forest   0.0210   0.0211  
IsolationForest 0.1128   0.1176  â­

ìµœê³  ì„±ëŠ¥: Isolation Forest (MCC: 0.1128)

Isolation Forest ìƒì„¸ ê²°ê³¼:
              precision    recall  f1-score   support
      ì •ìƒ     0.9953    0.9946    0.9950      2984
      ë¶ˆëŸ‰     0.1111    0.1250    0.1176        16
  accuracy                         0.9900      3000

ì‹¤í–‰ ì™„ë£Œ! âœ…
```

#### ì¢…í•© ì‹œìŠ¤í…œ ì‹¤í–‰ ê²°ê³¼:
```
ğŸ­ Bosch Production Line - ì¢…í•© ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ
================================================================================

ğŸ”§ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
================================================================================
ğŸ“‚ ë°ì´í„° ë¡œë”©: 100,000ê°œ ìƒ˜í”Œ
   ë¡œë“œ ì™„ë£Œ: 100,000 x 970

ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:
   ì •ìƒ ì œí’ˆ: 99,430ê°œ (99.43%)
   ë¶ˆëŸ‰ ì œí’ˆ: 570ê°œ (0.57%)
   ë¶ˆê· í˜• ë¹„ìœ¨: 175:1

ğŸ” íŠ¹ì§• ì „ì²˜ë¦¬:
   ì›ë³¸ íŠ¹ì§• ìˆ˜: 968ê°œ
   ê²°ì¸¡ê°’ í•„í„°ë§ í›„: 334ê°œ
   ë¶„ì‚° í•„í„°ë§ í›„: 25ê°œ

ğŸ› ï¸ ê²°ì¸¡ê°’ ì²˜ë¦¬:
   ì²˜ë¦¬ ì „: 23,450,000ê°œ â†’ ì²˜ë¦¬ í›„: 0ê°œ

ğŸ”§ íŠ¹ì§• ê³µí•™:
   ìƒì„±ëœ ì§‘ê³„ íŠ¹ì§•: 15ê°œ

âœ… ì „ì²˜ë¦¬ ì™„ë£Œ:
   ìµœì¢… ë°ì´í„° í¬ê¸°: 100,000 x 25
   ë¶ˆëŸ‰ë¥ : 0.5700%

================================================================================
ğŸ¯ ì§€ë„í•™ìŠµ ì ‘ê·¼ë²• (Binary Classification)
================================================================================

âš–ï¸ ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ (hybrid):
   ì›ë³¸ ë¶„í¬: ì •ìƒ 99,430, ë¶ˆëŸ‰ 570
   ìƒ˜í”Œë§ í›„: ì •ìƒ 20,000, ë¶ˆëŸ‰ 20,000
   ìƒˆë¡œìš´ ê· í˜•: 1.0:1

   ğŸ”§ LightGBM í•™ìŠµ ì¤‘...
      MCC: 0.1456, F1: 0.6234, AUC: 0.8123
      í•™ìŠµ ì‹œê°„: 18.45ì´ˆ

   ğŸ”§ RandomForest í•™ìŠµ ì¤‘...
      MCC: 0.1234, F1: 0.5789, AUC: 0.7456
      í•™ìŠµ ì‹œê°„: 31.23ì´ˆ

================================================================================
ğŸŒ² ë¹„ì§€ë„í•™ìŠµ ì ‘ê·¼ë²• (Anomaly Detection)  
================================================================================
   ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ: 79,544ê°œ
   ì˜ˆìƒ contamination: 0.5700%
   
   ê²°ê³¼: MCC: 0.1087, F1: 0.4923, AUC: 0.7234
   í•™ìŠµ ì‹œê°„: 12.67ì´ˆ

================================================================================
ğŸ§  ë”¥ëŸ¬ë‹ ì ‘ê·¼ë²• (AutoEncoder)
================================================================================
   ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ: 79,544ê°œ
   ë””ë°”ì´ìŠ¤: cuda
   ì…ë ¥ ì°¨ì›: 25
   
   ì—í¬í¬ 10/50, ì†ì‹¤: 0.023456
   ì—í¬í¬ 20/50, ì†ì‹¤: 0.018234  
   ì—í¬í¬ 30/50, ì†ì‹¤: 0.016789
   ...ì¡°ê¸° ì¢…ë£Œ (ì—í¬í¬ 42)
   
   ì„ê³„ê°’: 0.025678
   ê²°ê³¼: MCC: 0.1298, F1: 0.5234, AUC: 0.7892
   í•™ìŠµ ì‹œê°„: 156.78ì´ˆ

================================================================================
ğŸ“‹ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
================================================================================
        Model       MCC   F1-Score     AUC   Train Time (s)
     LightGBM    0.1456     0.6234  0.8123           18.45
  RandomForest    0.1234     0.5789  0.7456           31.23
   AutoEncoder    0.1298     0.5234  0.7892          156.78
IsolationForest   0.1087     0.4923  0.7234           12.67

ğŸ† ìµœê³  ì„±ëŠ¥: LightGBM (MCC: 0.1456)

ğŸ“Š LightGBM ìƒì„¸ ê²°ê³¼:
   ì˜ˆì¸¡ëœ ë¶ˆëŸ‰í’ˆ ìˆ˜: 1,245ê°œ
   ë¶ˆëŸ‰ ì˜ˆì¸¡ë¥ : 1.25%

âœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!

ğŸ’¡ ê¶Œì¥ì‚¬í•­:
   1. ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ  
   2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna ë“± í™œìš©)
   3. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
   4. ë‚ ì§œ/ë²”ì£¼í˜• ë°ì´í„° ì¶”ê°€ í™œìš©
```

## ğŸ”§ ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ğŸ“Š 1. ìƒ˜í”Œë§ ì „ëµ ì„¸ë¶€ ì„¤ì •

#### ì–¸ë”ìƒ˜í”Œë§ (ë¹ ë¥¸ í•™ìŠµ)
```python
# ì •ìƒ ë°ì´í„°ë¥¼ ë¶ˆëŸ‰ ë°ì´í„° ìˆ˜ì¤€ìœ¼ë¡œ ì¶•ì†Œ
detector = BoschFaultDetectionSuite(sampling_strategy='under')

# ì¥ì : ë¹ ë¥¸ í•™ìŠµ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
# ë‹¨ì : ì •ìƒ íŒ¨í„´ ì •ë³´ ì†ì‹¤
# ê¶Œì¥: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½
```

#### ì˜¤ë²„ìƒ˜í”Œë§ (ì •ë³´ ë³´ì¡´)  
```python
# SMOTEë¡œ ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ë°ì´í„° ìƒì„±
detector = BoschFaultDetectionSuite(sampling_strategy='over')

# ì¥ì : ì›ë³¸ ë°ì´í„° ì •ë³´ ë³´ì¡´, ë‹¤ì–‘í•œ ë¶ˆëŸ‰ íŒ¨í„´
# ë‹¨ì : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€, ê³¼ì í•© ìœ„í—˜  
# ê¶Œì¥: ì¶©ë¶„í•œ ë©”ëª¨ë¦¬, ë†’ì€ ì„±ëŠ¥ ìš”êµ¬
```

#### í•˜ì´ë¸Œë¦¬ë“œ (ê· í˜• ìµœì í™”) â­ ê¶Œì¥
```python
# ì–¸ë”+ì˜¤ë²„ ìƒ˜í”Œë§ ì¡°í•©
detector = BoschFaultDetectionSuite(sampling_strategy='hybrid')

# 3:1 ë˜ëŠ” 5:1 ë¹„ìœ¨ë¡œ ì ì ˆí•œ ê· í˜•
# ìµœê³  ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì˜ ì ˆì¶©ì 
```

#### ì»¤ìŠ¤í…€ ìƒ˜í”Œë§
```python
# ìˆ˜ë™ìœ¼ë¡œ ë¹„ìœ¨ ì¡°ì •
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import RandomUnderSampler

# 1ë‹¨ê³„: ì–¸ë”ìƒ˜í”Œë§ (1:3 ë¹„ìœ¨)
under_sampler = RandomUnderSampler(
    sampling_strategy={0: fault_count * 3, 1: fault_count}
)

# 2ë‹¨ê³„: SMOTE+Tomek (ì •ì œëœ ì˜¤ë²„ìƒ˜í”Œë§)
smote_tomek = SMOTETomek(
    sampling_strategy={1: fault_count * 2},
    random_state=42
)
```

### ğŸ¤– 2. ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

#### LightGBM ìµœì í™”
```python
# ì„±ëŠ¥ ì¤‘ì‹¬ ì„¤ì •
lgbm_params = {
    'n_estimators': 1000,        # íŠ¸ë¦¬ ê°œìˆ˜ ì¦ê°€
    'learning_rate': 0.05,       # ë‚®ì€ í•™ìŠµë¥ 
    'max_depth': 8,              # ê¹Šì€ íŠ¸ë¦¬
    'num_leaves': 64,            # ë¦¬í”„ ë…¸ë“œ ìˆ˜
    'subsample': 0.8,            # í–‰ ìƒ˜í”Œë§
    'colsample_bytree': 0.8,     # ì—´ ìƒ˜í”Œë§
    'reg_alpha': 1.0,            # L1 ì •ê·œí™”
    'reg_lambda': 1.0,           # L2 ì •ê·œí™”
    'class_weight': 'balanced'
}

# ì†ë„ ì¤‘ì‹¬ ì„¤ì •  
lgbm_params_fast = {
    'n_estimators': 100,
    'learning_rate': 0.1,
    'max_depth': 6,
    'num_leaves': 31,
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'verbosity': -1
}
```

#### Random Forest ìµœì í™”
```python
# ì„±ëŠ¥ ì¤‘ì‹¬ ì„¤ì •
rf_params = {
    'n_estimators': 500,         # íŠ¸ë¦¬ ê°œìˆ˜
    'max_depth': 15,             # ê¹Šì€ íŠ¸ë¦¬
    'min_samples_split': 5,      # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
    'min_samples_leaf': 2,       # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
    'max_features': 'sqrt',      # íŠ¹ì§• ìƒ˜í”Œë§
    'bootstrap': True,           # ë¶€íŠ¸ìŠ¤íŠ¸ë©
    'class_weight': 'balanced',
    'n_jobs': -1
}

# ë©”ëª¨ë¦¬ íš¨ìœ¨ ì„¤ì •
rf_params_efficient = {
    'n_estimators': 100,
    'max_depth': 10,
    'min_samples_split': 10,
    'max_samples': 0.7,          # ìƒ˜í”Œ ë¹„ìœ¨ ì œí•œ
    'warm_start': True           # ì ì§„ì  í•™ìŠµ
}
```

#### AutoEncoder ì•„í‚¤í…ì²˜ ë³€í˜•
```python
# ê¹Šì€ ë„¤íŠ¸ì›Œí¬ (ì„±ëŠ¥ ì¤‘ì‹¬)
class DeepAutoEncoder(nn.Module):
    def __init__(self, input_dim=25, encoding_dim=8):
        super().__init__()
        
        # ë” ë§ì€ ì€ë‹‰ì¸µ
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, input_dim * 2),     # í™•ì¥
            nn.ReLU(), nn.BatchNorm1d(input_dim * 2), nn.Dropout(0.2),
            
            nn.Linear(input_dim * 2, input_dim),     # ì›ë³¸ í¬ê¸°
            nn.ReLU(), nn.BatchNorm1d(input_dim), nn.Dropout(0.2),
            
            nn.Linear(input_dim, input_dim // 2),    # ì¶•ì†Œ ì‹œì‘
            nn.ReLU(), nn.BatchNorm1d(input_dim // 2), nn.Dropout(0.2),
            
            nn.Linear(input_dim // 2, encoding_dim), # ìµœì¢… ì¸ì½”ë”©
            nn.ReLU()
        )

# ì–•ì€ ë„¤íŠ¸ì›Œí¬ (ì†ë„ ì¤‘ì‹¬)
class SimpleAutoEncoder(nn.Module):
    def __init__(self, input_dim=25, encoding_dim=16):
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, encoding_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, input_dim)
        )
```

### ğŸ”¬ 3. ê³ ê¸‰ íŠ¹ì§• ê³µí•™

#### ë„ë©”ì¸ íŠ¹í™” íŠ¹ì§•
```python
# ìƒì‚°ë¼ì¸ ìŠ¤í…Œì´ì…˜ë³„ íŠ¹ì§•
def create_station_features(df):
    stations = {}
    for col in df.columns:
        if '_S' in col:  # ìŠ¤í…Œì´ì…˜ ì‹ë³„
            parts = col.split('_')
            if len(parts) >= 2:
                station_id = f"{parts[0]}_{parts[1]}"  # L0_S0 í˜•íƒœ
                if station_id not in stations:
                    stations[station_id] = []
                stations[station_id].append(col)
    
    station_features = pd.DataFrame()
    for station_id, cols in stations.items():
        station_data = df[cols]
        
        # ìŠ¤í…Œì´ì…˜ë³„ ì§‘ê³„
        station_features[f'{station_id}_mean'] = station_data.mean(axis=1)
        station_features[f'{station_id}_std'] = station_data.std(axis=1)
        station_features[f'{station_id}_missing_ratio'] = station_data.isnull().sum(axis=1) / len(cols)
        station_features[f'{station_id}_measurement_count'] = station_data.count(axis=1)
        
        # ì´ìƒê°’ ë¹„ìœ¨
        Q1 = station_data.quantile(0.25, axis=1)
        Q3 = station_data.quantile(0.75, axis=1)
        IQR = Q3 - Q1
        outlier_mask = (station_data < (Q1 - 1.5 * IQR).values.reshape(-1,1)) | \
                      (station_data > (Q3 + 1.5 * IQR).values.reshape(-1,1))
        station_features[f'{station_id}_outlier_ratio'] = outlier_mask.sum(axis=1) / len(cols)
    
    return station_features

# ì‹œê³„ì—´ ê¸°ë°˜ íŠ¹ì§• (ë‚ ì§œ ë°ì´í„° ìˆëŠ” ê²½ìš°)
def create_temporal_features(df, date_col):
    df[date_col] = pd.to_datetime(df[date_col])
    
    temporal_features = pd.DataFrame()
    temporal_features['hour'] = df[date_col].dt.hour
    temporal_features['day_of_week'] = df[date_col].dt.dayofweek
    temporal_features['month'] = df[date_col].dt.month
    temporal_features['is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)
    temporal_features['is_night_shift'] = ((df[date_col].dt.hour >= 22) | 
                                          (df[date_col].dt.hour <= 6)).astype(int)
    
    return temporal_features
```

#### í†µê³„ì  íŠ¹ì§• í™•ì¥
```python
def create_advanced_statistical_features(X):
    features = pd.DataFrame()
    
    # ê¸°ë³¸ í†µê³„ëŸ‰
    features['mean'] = X.mean(axis=1)
    features['median'] = X.median(axis=1) 
    features['std'] = X.std(axis=1)
    features['var'] = X.var(axis=1)
    features['skew'] = X.skew(axis=1)        # ì™œë„
    features['kurtosis'] = X.kurtosis(axis=1) # ì²¨ë„
    
    # ë°±ë¶„ìœ„ìˆ˜
    for p in [10, 25, 75, 90]:
        features[f'p{p}'] = X.quantile(p/100, axis=1)
    
    # ë¶„í¬ì˜ í˜•íƒœ
    features['iqr'] = features['p75'] - features['p25']  # ì‚¬ë¶„ë²”ìœ„
    features['range'] = X.max(axis=1) - X.min(axis=1)
    features['coefficient_of_variation'] = features['std'] / features['mean']
    
    # ê·¹ê°’ ë¹„ìœ¨
    Q1 = features['p25']
    Q3 = features['p75'] 
    IQR = features['iqr']
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outlier_counts = 0
    for i in range(X.shape[0]):
        row = X.iloc[i]
        outliers = ((row < lower_bound.iloc[i]) | 
                   (row > upper_bound.iloc[i])).sum()
        outlier_counts += [outliers]
    
    features['outlier_ratio'] = pd.Series(outlier_counts) / X.shape[1]
    
    # 0ê°’ê³¼ ê²°ì¸¡ê°’ ì •ë³´
    features['zero_count'] = (X == 0).sum(axis=1)
    features['nonzero_count'] = (X != 0).sum(axis=1)
    features['missing_count'] = X.isnull().sum(axis=1)
    features['zero_ratio'] = features['zero_count'] / X.shape[1]
    features['missing_ratio'] = features['missing_count'] / X.shape[1]
    
    return features.fillna(0)
```

### ğŸ¯ 4. ì•™ìƒë¸” ì „ëµ

#### Voting Classifier
```python
from sklearn.ensemble import VotingClassifier

# ì†Œí”„íŠ¸ ë³´íŒ… (í™•ë¥  ê¸°ë°˜)
ensemble = VotingClassifier([
    ('lgbm', LGBMClassifier(**lgbm_params)),
    ('rf', RandomForestClassifier(**rf_params)),
    ('svc', SVC(probability=True, class_weight='balanced'))
], voting='soft')

# í•˜ë“œ ë³´íŒ… (ë‹¤ìˆ˜ê²°)
ensemble_hard = VotingClassifier([
    ('lgbm', lgbm_model),
    ('rf', rf_model), 
    ('isolation', IsolationForestWrapper())  # ì»¤ìŠ¤í…€ ë˜í¼ í•„ìš”
], voting='hard')
```

#### ìŠ¤íƒœí‚¹ (Stacking)
```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# 1ë‹¨ê³„ ëª¨ë¸ë“¤
base_models = [
    ('lgbm', LGBMClassifier(**lgbm_params)),
    ('rf', RandomForestClassifier(**rf_params)),
    ('isolation', IsolationForestWrapper())
]

# 2ë‹¨ê³„ ë©”íƒ€ ëª¨ë¸
meta_model = LogisticRegression(class_weight='balanced')

# ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸°
stacking_classifier = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,  # êµì°¨ ê²€ì¦ í´ë“œ
    use_probabilities=True
)
```

#### ê°€ì¤‘ ì•™ìƒë¸”
```python
def weighted_ensemble_predict(models, X, weights):
    """
    ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸” ì˜ˆì¸¡
    """
    predictions = []
    probabilities = []
    
    for model, weight in zip(models, weights):
        if hasattr(model, 'predict_proba'):
            prob = model.predict_proba(X)[:, 1]  # ë¶ˆëŸ‰ í™•ë¥ 
            probabilities.append(prob * weight)
        else:
            # Isolation Forest ë“±
            scores = model.decision_function(X)
            # ìŠ¤ì½”ì–´ë¥¼ í™•ë¥ ë¡œ ë³€í™˜
            prob = 1 / (1 + np.exp(-scores))  
            probabilities.append(prob * weight)
    
    # ê°€ì¤‘ í‰ê·  í™•ë¥ 
    final_prob = np.sum(probabilities, axis=0) / np.sum(weights)
    
    return final_prob

# ì‚¬ìš© ì˜ˆì‹œ
model_weights = {
    'lgbm': 0.4,        # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜
    'rf': 0.3,          
    'autoencoder': 0.2,
    'isolation': 0.1    # ê°€ì¥ ë‚®ì€ ê°€ì¤‘ì¹˜
}

ensemble_prob = weighted_ensemble_predict(
    [lgbm_model, rf_model, ae_model, if_model],
    X_test,
    list(model_weights.values())
)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### ğŸ”¢ 1. ë°ì´í„° ë ˆë²¨ ìµœì í™”

#### ìƒ˜í”Œ í¬ê¸° í™•ì¥
```python
# ì ì§„ì  í™•ì¥ ì „ëµ
sample_sizes = [50000, 100000, 500000, 1000000]
performance_trend = []

for size in sample_sizes:
    X, y = detector.load_and_preprocess_data(data_path, sample_size=size)
    # ... í•™ìŠµ ë° í‰ê°€
    performance_trend.append(mcc_score)
    
# ì„±ëŠ¥ í¬í™”ì  ì°¾ê¸°
optimal_size = find_performance_plateau(sample_sizes, performance_trend)
```

#### ì‹œê³„ì—´ íŠ¹ì§• í™œìš©
```python
# ë‚ ì§œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
def add_temporal_patterns(df, date_col='date'):
    # ìƒì‚° ì£¼ê¸° íŒ¨í„´
    df['production_cycle'] = (df[date_col].dt.dayofyear % 7)  # ì£¼ê°„ ì£¼ê¸°
    df['seasonal_pattern'] = np.sin(2 * np.pi * df[date_col].dt.dayofyear / 365)
    
    # ì¥ë¹„ ê°€ë™ ì‹œê°„ (ëˆ„ì )
    df['cumulative_runtime'] = df.groupby('equipment_id').cumcount()
    
    # ì´ì „ ë¶ˆëŸ‰ ì´ë ¥ (ì‹œê³„ì—´)
    df['prev_defects_7d'] = df.groupby('station')['Response'].rolling(
        window=7, min_periods=1
    ).sum().reset_index(level=0, drop=True)
    
    return df
```

#### ì™¸ë¶€ ë°ì´í„° ìœµí•©
```python
# í™˜ê²½ ë°ì´í„° (ì˜¨ë„, ìŠµë„ ë“±)
def merge_environmental_data(production_df, env_df):
    # ì‹œê°„ ê¸°ì¤€ ë§¤ì¹­
    merged = pd.merge_asof(
        production_df.sort_values('timestamp'),
        env_df.sort_values('timestamp'),
        on='timestamp',
        direction='backward'  # ê°€ì¥ ê°€ê¹Œìš´ ì´ì „ ê°’
    )
    
    # í™˜ê²½ ì¡°ê±´ êµ¬ê°„í™”
    merged['temp_range'] = pd.cut(merged['temperature'], 
                                 bins=[-np.inf, 20, 25, 30, np.inf],
                                 labels=['cold', 'normal', 'warm', 'hot'])
    return merged

# ì¥ë¹„ ì´ë ¥ ë°ì´í„°
def add_equipment_history(df, maintenance_df):
    # ë§ˆì§€ë§‰ ì •ë¹„ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„
    df['days_since_maintenance'] = (
        df['timestamp'] - df.merge(
            maintenance_df, on='equipment_id', how='left'
        )['last_maintenance']
    ).dt.days
    
    return df
```

### ğŸ¤– 2. ëª¨ë¸ ë ˆë²¨ ìµœì í™”

#### ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna)
```python
import optuna

def optimize_lgbm(X_train, y_train, X_val, y_val, n_trials=100):
    def objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'max_depth': trial.suggest_int('max_depth', 3, 15),
            'num_leaves': trial.suggest_int('num_leaves', 10, 300),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
            'class_weight': 'balanced'
        }
        
        model = LGBMClassifier(**params, verbosity=-1)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_val)
        mcc = matthews_corrcoef(y_val, y_pred)
        
        return mcc
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)
    
    return study.best_params

# ì‚¬ìš© ì˜ˆì‹œ
best_params = optimize_lgbm(X_train, y_train, X_val, y_val)
optimized_lgbm = LGBMClassifier(**best_params)
```

#### ë™ì  ì„ê³„ê°’ ìµœì í™”
```python
def optimize_threshold(y_true, y_proba, metric='mcc'):
    """
    ROC ê³¡ì„ ì—ì„œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    """
    from sklearn.metrics import roc_curve
    
    fpr, tpr, thresholds = roc_curve(y_true, y_proba)
    
    best_score = -np.inf
    best_threshold = 0.5
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        
        if metric == 'mcc':
            score = matthews_corrcoef(y_true, y_pred)
        elif metric == 'f1':
            score = f1_score(y_true, y_pred)
        else:
            score = precision_score(y_true, y_pred)
        
        if score > best_score:
            best_score = score
            best_threshold = threshold
    
    return best_threshold, best_score

# ê° ëª¨ë¸ë³„ ìµœì  ì„ê³„ê°’
model_thresholds = {}
for name, model in models.items():
    y_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_val)
    threshold, score = optimize_threshold(y_val, y_proba)
    model_thresholds[name] = threshold
```

#### ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
```python
def optimize_ensemble_weights(models, X_val, y_val):
    """
    ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì°¾ê¸°
    """
    def ensemble_objective(weights):
        weights = np.array(weights)
        weights = weights / weights.sum()  # ì •ê·œí™”
        
        ensemble_pred = np.zeros(len(y_val))
        for i, (model, weight) in enumerate(zip(models, weights)):
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X_val)[:, 1]
            else:
                pred = model.decision_function(X_val)
                pred = 1 / (1 + np.exp(-pred))  # ì‹œê·¸ëª¨ì´ë“œ ë³€í™˜
            
            ensemble_pred += weight * pred
        
        y_pred = (ensemble_pred >= 0.5).astype(int)
        return matthews_corrcoef(y_val, y_pred)
    
    # ë² ì´ì§€ì•ˆ ìµœì í™”
    from skopt import gp_minimize
    from skopt.space import Real
    
    space = [Real(0.0, 1.0, name=f'weight_{i}') for i in range(len(models))]
    
    result = gp_minimize(
        func=lambda x: -ensemble_objective(x),  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
        dimensions=space,
        n_calls=100,
        random_state=42
    )
    
    optimal_weights = np.array(result.x)
    optimal_weights = optimal_weights / optimal_weights.sum()
    
    return optimal_weights
```

### ğŸ“Š 3. í‰ê°€ ë° ê²€ì¦ ìµœì í™”

#### ì‹œê³„ì—´ êµì°¨ ê²€ì¦
```python
from sklearn.model_selection import TimeSeriesSplit

def time_series_cv(X, y, model, n_splits=5):
    """
    ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)
    scores = []
    
    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # ë¶ˆê· í˜• ì²˜ë¦¬
        X_train_balanced, y_train_balanced = apply_sampling(X_train, y_train)
        
        model.fit(X_train_balanced, y_train_balanced)
        y_pred = model.predict(X_val)
        
        score = matthews_corrcoef(y_val, y_pred)
        scores.append(score)
    
    return np.array(scores)
```

#### A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
```python
def ab_test_models(model_a, model_b, X_test, y_test, n_bootstrap=1000):
    """
    ë‘ ëª¨ë¸ê°„ í†µê³„ì  ìœ ì˜ì„± ê²€ì •
    """
    def bootstrap_score(model, X, y, n_samples):
        scores = []
        for _ in range(n_samples):
            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
            idx = np.random.choice(len(X), len(X), replace=True)
            X_boot, y_boot = X[idx], y[idx]
            
            y_pred = model.predict(X_boot)
            score = matthews_corrcoef(y_boot, y_pred)
            scores.append(score)
        
        return np.array(scores)
    
    scores_a = bootstrap_score(model_a, X_test, y_test, n_bootstrap)
    scores_b = bootstrap_score(model_b, X_test, y_test, n_bootstrap)
    
    # í†µê³„ì  ê²€ì •
    from scipy import stats
    statistic, p_value = stats.ttest_ind(scores_a, scores_b)
    
    result = {
        'model_a_mean': scores_a.mean(),
        'model_b_mean': scores_b.mean(),
        'model_a_std': scores_a.std(),
        'model_b_std': scores_b.std(),
        'p_value': p_value,
        'significant': p_value < 0.05,
        'better_model': 'A' if scores_a.mean() > scores_b.mean() else 'B'
    }
    
    return result
```

## ğŸ”§ ì‹¤ì „ ë°°í¬ ë° ìš´ì˜

### ğŸš€ 1. ëª¨ë¸ ì„œë¹™ ì•„í‚¤í…ì²˜

#### Flask API ì„œë²„
```python
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# ëª¨ë¸ ë¡œë“œ
model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # JSON ë°ì´í„° ë°›ê¸°
        data = request.json
        features = np.array(data['features']).reshape(1, -1)
        
        # ì „ì²˜ë¦¬
        features_scaled = scaler.transform(features)
        
        # ì˜ˆì¸¡
        prediction = model.predict(features_scaled)[0]
        probability = model.predict_proba(features_scaled)[0, 1]
        
        return jsonify({
            'prediction': int(prediction),
            'probability': float(probability),
            'status': 'success'
        })
    
    except Exception as e:
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 400

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

#### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
import logging
from datetime import datetime
import json

class ModelMonitor:
    def __init__(self):
        self.prediction_log = []
        self.performance_metrics = {}
    
    def log_prediction(self, features, prediction, probability, actual=None):
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'features_hash': hash(str(features)),  # ê°œì¸ì •ë³´ ë³´í˜¸
            'prediction': prediction,
            'probability': probability,
            'actual': actual
        }
        
        self.prediction_log.append(log_entry)
        
        # ì‹¤ì‹œê°„ ì•Œë¦¼ (ë¶ˆëŸ‰ ì˜ˆì¸¡ ì‹œ)
        if prediction == 1 and probability > 0.8:
            self.send_alert(f"High confidence defect detected: {probability:.3f}")
    
    def calculate_drift(self, current_features, reference_features):
        """
        ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€
        """
        from scipy.stats import ks_2samp
        
        drift_scores = []
        for i in range(current_features.shape[1]):
            current_col = current_features[:, i]
            reference_col = reference_features[:, i]
            
            statistic, p_value = ks_2samp(current_col, reference_col)
            drift_scores.append(p_value)
        
        # ë“œë¦¬í”„íŠ¸ ì•Œë¦¼ (p < 0.01)
        significant_drift = sum([p < 0.01 for p in drift_scores])
        if significant_drift > len(drift_scores) * 0.1:  # 10% ì´ìƒ íŠ¹ì§•ì—ì„œ ë“œë¦¬í”„íŠ¸
            self.send_alert(f"Data drift detected in {significant_drift} features")
        
        return drift_scores
    
    def send_alert(self, message):
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, ì´ë©”ì¼, SMS ë“±ìœ¼ë¡œ ì•Œë¦¼
        logging.warning(f"ALERT: {message}")
```

### ğŸ“Š 2. ëª¨ë¸ ì„±ëŠ¥ ì¶”ì 

#### MLOps íŒŒì´í”„ë¼ì¸
```python
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

class MLOpsTracker:
    def __init__(self, experiment_name="bosch_fault_detection"):
        mlflow.set_experiment(experiment_name)
        self.client = MlflowClient()
    
    def log_experiment(self, model, params, metrics, artifacts):
        with mlflow.start_run():
            # íŒŒë¼ë¯¸í„° ë¡œê¹…
            for key, value in params.items():
                mlflow.log_param(key, value)
            
            # ë©”íŠ¸ë¦­ ë¡œê¹…
            for key, value in metrics.items():
                mlflow.log_metric(key, value)
            
            # ëª¨ë¸ ì €ì¥
            mlflow.sklearn.log_model(
                model, 
                "model",
                registered_model_name="BoschFaultDetector"
            )
            
            # ì•„í‹°íŒ©íŠ¸ (ì‹œê°í™” ë“±)
            for artifact in artifacts:
                mlflow.log_artifact(artifact)
    
    def compare_models(self, metric="mcc"):
        """
        ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ
        """
        experiment = mlflow.get_experiment_by_name("bosch_fault_detection")
        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        
        best_run = runs.loc[runs[f'metrics.{metric}'].idxmax()]
        
        return {
            'run_id': best_run['run_id'],
            'best_score': best_run[f'metrics.{metric}'],
            'model_uri': f"runs:/{best_run['run_id']}/model"
        }
    
    def auto_retrain(self, current_performance, threshold=0.05):
        """
        ì„±ëŠ¥ ì €í•˜ ì‹œ ìë™ ì¬í›ˆë ¨
        """
        if current_performance < threshold:
            logging.warning("Model performance below threshold. Triggering retrain...")
            return True
        return False
```

## ğŸ” ê³ ê¸‰ ë¬¸ì œ í•´ê²°

### ğŸš¨ 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë° í•´ê²°ì±…

#### ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ
```python
# í•´ê²°ì±… 1: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
def process_large_data_in_chunks(file_path, chunk_size=10000):
    chunk_results = []
    
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # ê° ì²­í¬ë³„ ì²˜ë¦¬
        processed_chunk = preprocess_chunk(chunk)
        chunk_results.append(processed_chunk)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del chunk
        gc.collect()
    
    return pd.concat(chunk_results, ignore_index=True)

# í•´ê²°ì±… 2: ë°ì´í„° íƒ€ì… ìµœì í™”
def optimize_dtypes(df):
    for col in df.columns:
        if df[col].dtype == 'int64':
            df[col] = pd.to_numeric(df[col], downcast='integer')
        elif df[col].dtype == 'float64':
            df[col] = pd.to_numeric(df[col], downcast='float')
    
    return df
```

#### CUDA/GPU ê´€ë ¨ ì˜¤ë¥˜
```python
# í•´ê²°ì±…: ì•ˆì „í•œ GPU ì‚¬ìš©
import torch

def setup_device_safely():
    if torch.cuda.is_available():
        try:
            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            device = torch.device('cuda')
            memory_allocated = torch.cuda.memory_allocated(device)
            memory_reserved = torch.cuda.memory_reserved(device)
            
            print(f"GPU Memory - Allocated: {memory_allocated/1e9:.2f}GB")
            print(f"GPU Memory - Reserved: {memory_reserved/1e9:.2f}GB")
            
            return device
        except Exception as e:
            print(f"GPU initialization failed: {e}")
            return torch.device('cpu')
    else:
        return torch.device('cpu')

# ë©”ëª¨ë¦¬ ì •ë¦¬
def cleanup_gpu_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
```

#### ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
```python
# ì•ˆì „í•œ íŒŒì¼ ì½ê¸°
def safe_read_csv(file_path, encodings=['utf-8', 'cp949', 'euc-kr', 'latin-1']):
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"Successfully loaded with encoding: {encoding}")
            return df
        except UnicodeDecodeError:
            continue
    
    raise ValueError(f"Could not read file with any of the encodings: {encodings}")

# í…ìŠ¤íŠ¸ ì •ë¦¬
def clean_text_columns(df):
    for col in df.select_dtypes(include=['object']):
        # ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        df[col] = df[col].str.replace(r'[^\w\s]', '', regex=True)
        df[col] = df[col].str.encode('ascii', errors='ignore').str.decode('ascii')
    
    return df
```

### ğŸ¯ 2. ì„±ëŠ¥ ìµœì í™” ë¬¸ì œ

#### í•™ìŠµ ì†ë„ ê°œì„ 
```python
# ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
from joblib import parallel_backend
import multiprocessing

def optimize_parallel_processing():
    n_jobs = min(multiprocessing.cpu_count(), 8)  # CPU ì½”ì–´ ìˆ˜ ì œí•œ
    
    with parallel_backend('threading', n_jobs=n_jobs):
        # scikit-learn ëª¨ë¸ë“¤ì´ ìë™ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        rf = RandomForestClassifier(n_jobs=n_jobs)
        rf.fit(X_train, y_train)

# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìµœì í™”
class OptimizedPipeline:
    def __init__(self):
        self.cache = {}
    
    def cached_preprocessing(self, data_hash, preprocess_func, data):
        if data_hash in self.cache:
            return self.cache[data_hash]
        
        result = preprocess_func(data)
        self.cache[data_hash] = result
        return result
    
    def incremental_learning(self, model, new_data):
        """
        ì ì§„ì  í•™ìŠµ (ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€ ì‹œ)
        """
        if hasattr(model, 'partial_fit'):
            model.partial_fit(new_data['X'], new_data['y'])
        else:
            # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ê²°í•© í›„ ì¬í•™ìŠµ
            combined_X = np.vstack([self.X_train, new_data['X']])
            combined_y = np.hstack([self.y_train, new_data['y']])
            model.fit(combined_X, combined_y)
```

### ğŸ›¡ï¸ 3. ìš´ì˜ í™˜ê²½ ë¬¸ì œ

#### ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì§€ì—° í•´ê²°
```python
import time
from functools import lru_cache

class FastPredictor:
    def __init__(self, model, scaler):
        self.model = model
        self.scaler = scaler
        
        # ëª¨ë¸ ì˜ˆì—´ (ì²« ì˜ˆì¸¡ ì§€ì—° ë°©ì§€)
        dummy_input = np.random.rand(1, self.scaler.n_features_in_)
        self.predict(dummy_input)
    
    @lru_cache(maxsize=1000)
    def cached_preprocess(self, features_tuple):
        """
        ì „ì²˜ë¦¬ ê²°ê³¼ ìºì‹±
        """
        features = np.array(features_tuple).reshape(1, -1)
        return self.scaler.transform(features)
    
    def predict(self, features):
        start_time = time.time()
        
        # ìºì‹œëœ ì „ì²˜ë¦¬
        if isinstance(features, np.ndarray):
            features_tuple = tuple(features.flatten())
            processed = self.cached_preprocess(features_tuple)
        else:
            processed = self.scaler.transform(features.reshape(1, -1))
        
        # ì˜ˆì¸¡
        prediction = self.model.predict(processed)[0]
        probability = self.model.predict_proba(processed)[0, 1]
        
        end_time = time.time()
        
        return {
            'prediction': prediction,
            'probability': probability,
            'inference_time_ms': (end_time - start_time) * 1000
        }
```

### ğŸ“Š 4. ëª¨ë¸ í’ˆì§ˆ ì´ìŠˆ

#### í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬í™” ëŒ€ì‘
```python
def adaptive_sampling_strategy(y, target_ratio=0.1):
    """
    ë°ì´í„° ë¶„í¬ì— ë”°ë¥¸ ì ì‘ì  ìƒ˜í”Œë§
    """
    current_ratio = y.mean()
    
    if current_ratio < 0.001:  # 0.1% ë¯¸ë§Œ
        return 'aggressive_over'  # ê°•ë ¥í•œ ì˜¤ë²„ìƒ˜í”Œë§
    elif current_ratio < 0.01:  # 1% ë¯¸ë§Œ
        return 'hybrid'           # í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
    else:
        return 'under'            # ì–¸ë”ìƒ˜í”Œë§

def dynamic_threshold_adjustment(model, X_val, y_val, business_priority='precision'):
    """
    ë¹„ì¦ˆë‹ˆìŠ¤ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ë™ì  ì„ê³„ê°’ ì¡°ì •
    """
    y_proba = model.predict_proba(X_val)[:, 1]
    
    if business_priority == 'precision':
        # ì •ë°€ë„ ìš°ì„ : í™•ì‹¤í•œ ë¶ˆëŸ‰í’ˆë§Œ íƒì§€
        threshold = np.percentile(y_proba[y_val == 1], 25)  # ë¶ˆëŸ‰í’ˆì˜ 25% ì´ìƒ
    elif business_priority == 'recall':
        # ì¬í˜„ìœ¨ ìš°ì„ : ê°€ëŠ¥í•œ ë§ì€ ë¶ˆëŸ‰í’ˆ íƒì§€
        threshold = np.percentile(y_proba[y_val == 1], 75)  # ë¶ˆëŸ‰í’ˆì˜ 75% ì´ìƒ
    else:
        # F1 ê· í˜•
        threshold = optimize_threshold(y_val, y_proba, metric='f1')[0]
    
    return threshold
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ë° í•™ìŠµ ìˆœì„œ

### ğŸ“‹ **ë‹¨ê³„ë³„ ì‹¤í–‰ íŒŒì¼** (ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ í•™ìŠµ)

```
bosch/
â”œâ”€â”€ ğŸ“š í•™ìŠµìš© ìŠ¤í¬ë¦½íŠ¸ (ë‹¨ê³„ë³„ ì‹¤í–‰)
â”‚   â”œâ”€â”€ 00_interactive_launcher.py              # ğŸ¯ ì‹œì‘ì : ëŒ€í™”í˜• ë©”ë‰´
â”‚   â”œâ”€â”€ 01_simple_fault_detection_demo.py       # 1ë‹¨ê³„: 5ë¶„ ë¹ ë¥¸ ë°ëª¨
â”‚   â”œâ”€â”€ 02_autoencoder_fault_detection.py       # 2ë‹¨ê³„: AutoEncoder ì‹¬í™”
â”‚   â””â”€â”€ 03_comprehensive_fault_detection.py     # 3ë‹¨ê³„: ì¢…í•© ì‹œìŠ¤í…œ
â”‚
â”œâ”€â”€ ğŸ“Š ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ train_numeric.csv         # ë©”ì¸ í•™ìŠµ ë°ì´í„° (2GB+)
â”‚   â”‚   â”œâ”€â”€ train_categorical.csv     # ë²”ì£¼í˜• ë°ì´í„° (ì„ íƒì )
â”‚   â”‚   â””â”€â”€ train_date.csv           # ë‚ ì§œ ë°ì´í„° (ì„ íƒì )
â”‚   â””â”€â”€ extract_data.py              # ì••ì¶• í•´ì œ ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ ğŸ“ˆ ë¶„ì„ ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ bosch_analysis.ipynb         # ì˜ë¬¸ ë¶„ì„ (ìºê¸€ ìŠ¤íƒ€ì¼)
â”‚   â”œâ”€â”€ bosch_analysis_korean.ipynb  # í•œê¸€ í•™ìŠµìš©
â”‚   â””â”€â”€ simple_analysis.py           # ê¸°ë³¸ íƒìƒ‰ì  ë¶„ì„
â”‚
â”œâ”€â”€ ğŸ“‹ ë¬¸ì„œí™” ë° ì„¤ì •
â”‚   â”œâ”€â”€ README_fault_detection.md    # â­ ë©”ì¸ ê°€ì´ë“œ (ì´ íŒŒì¼)
â”‚   â”œâ”€â”€ README.md                   # GitHub ê¸°ë³¸ README  
â”‚   â”œâ”€â”€ requirements.txt            # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â”‚   â””â”€â”€ CLAUDE.md                   # í”„ë¡œì íŠ¸ ë©”ëª¨ë¦¬
â”‚
â””â”€â”€ ğŸ”§ ìœ í‹¸ë¦¬í‹° ë° ë¶„ì„
    â”œâ”€â”€ extract_data.py             # ì••ì¶• í•´ì œ ìœ í‹¸ë¦¬í‹°
    â”œâ”€â”€ simple_analysis.py          # ê¸°ë³¸ ë°ì´í„° íƒìƒ‰
    â””â”€â”€ real_data_analysis.py       # ì‹¤ì œ ë°ì´í„° ë¶„ì„
```

### ğŸ“¦ **ë‹¨ê³„ë³„ íŒŒì¼ ìƒì„¸ ì„¤ëª…**

#### ğŸ¯ **0ë‹¨ê³„: `00_interactive_launcher.py`** â­ ì¶”ì²œ ì‹œì‘ì 
- **ëª©ì **: ì‚¬ìš©ì ì¹œí™”ì  í•™ìŠµ ê°€ì´ë“œ ë©”ë‰´
- **íŠ¹ì§•**: ë‹¨ê³„ë³„ ì•ˆë‚´, ìš”êµ¬ì‚¬í•­ ì²´í¬, ì§„í–‰ìƒí™© ì¶”ì 
- **ì‚¬ìš©ë²•**: `python 00_interactive_launcher.py`
- **í•™ìŠµíš¨ê³¼**: ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡° ì´í•´

#### ğŸš€ **1ë‹¨ê³„: `01_simple_fault_detection_demo.py`**
- **ëª©ì **: 5ë¶„ ë‚´ ë¹ ë¥¸ ê²°ê³¼ í™•ì¸ ë° ML ê¸°ì´ˆ í•™ìŠµ
- **íŠ¹ì§•**: 
  - ìµœì†Œ ì˜ì¡´ì„± (pandas, sklearnë§Œ í•„ìš”)
  - 10,000 ìƒ˜í”Œë¡œ ë¹ ë¥¸ ì‹¤í–‰
  - Random Forest vs Isolation Forest ì„±ëŠ¥ ë¹„êµ
- **í•™ìŠµë‚´ìš©**: í´ë˜ìŠ¤ ë¶ˆê· í˜•, íŠ¹ì§• ê³µí•™, ëª¨ë¸ í‰ê°€
- **ì˜ˆìƒê²°ê³¼**: MCC 0.11, ì‹¤í–‰ì‹œê°„ 30ì´ˆ ì´ë‚´

#### ğŸ§  **2ë‹¨ê³„: `02_autoencoder_fault_detection.py`**  
- **ëª©ì **: ë”¥ëŸ¬ë‹ ì´ìƒ íƒì§€ ì‹¬í™” í•™ìŠµ
- **íŠ¹ì§•**: 
  - TensorFlow/PyTorch ì§€ì› (ì„ íƒì )
  - ì ì§„ì  ì¸ì½”ë”©-ë””ì½”ë”© êµ¬ì¡°
  - ì¬êµ¬ì„± ì˜¤ì°¨ ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì •
- **í•™ìŠµë‚´ìš©**: AutoEncoder ì›ë¦¬, ë”¥ëŸ¬ë‹ ì´ìƒ íƒì§€
- **ì˜ˆìƒê²°ê³¼**: MCC 0.08~0.12, GPU ê°€ì† ì‹œ 5ë¶„ ì´ë‚´

#### ğŸ­ **3ë‹¨ê³„: `03_comprehensive_fault_detection.py`**
- **ëª©ì **: ì‹¤ì „ ë°°í¬ ìˆ˜ì¤€ì˜ ì™„ì „í•œ ì‹œìŠ¤í…œ
- **íŠ¹ì§•**: 
  - 4ê°œ ëª¨ë¸ ì•™ìƒë¸” (LightGBM, RF, IF, AE)
  - ê³ ê¸‰ ìƒ˜í”Œë§ ì „ëµ (í•˜ì´ë¸Œë¦¬ë“œ)
  - MLOps ìˆ˜ì¤€ ì„±ëŠ¥ ì¶”ì 
- **í•™ìŠµë‚´ìš©**: ì•™ìƒë¸”, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ì‹¤ì „ ë°°í¬
- **ì˜ˆìƒê²°ê³¼**: MCC 0.15+ ëª©í‘œ, ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš© ê°€ëŠ¥

## ğŸŒŸ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš© ì‚¬ë¡€

### ğŸ­ ì œì¡°ì—… ë„ì… ì‹œë‚˜ë¦¬ì˜¤

#### 1ë‹¨ê³„: íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ (1ê°œì›”)
```python
# ì†Œê·œëª¨ ìƒì‚°ë¼ì¸ í…ŒìŠ¤íŠ¸
pilot_config = {
    'daily_production': 1000,      # ì¼ì¼ 1,000ê°œ ì œí’ˆ
    'target_precision': 0.15,      # 15% ì •ë°€ë„ ëª©í‘œ
    'acceptable_recall': 0.10,     # 10% ì¬í˜„ìœ¨ í—ˆìš©
    'cost_per_false_alarm': 50,    # ì˜¤íƒ ë¹„ìš© $50
    'cost_per_missed_defect': 500  # ë¯¸íƒ ë¹„ìš© $500
}

# ROI ê³„ì‚°
daily_savings = calculate_roi(pilot_config)
# ì˜ˆìƒ ê²°ê³¼: ì¼ì¼ $200-400 ì ˆì•½
```

#### 2ë‹¨ê³„: ì ì§„ì  í™•ì¥ (3ê°œì›”)
```python
# ë‹¤ì¤‘ ìƒì‚°ë¼ì¸ í™•ì¥
expansion_strategy = {
    'lines_to_add': [2, 3, 4],  # ìˆœì°¨ í™•ì¥
    'model_update_frequency': 'weekly',
    'performance_threshold': 0.08,  # MCC ìµœì†Œ ê¸°ì¤€
    'auto_retrain_trigger': 0.05   # ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’
}
```

#### 3ë‹¨ê³„: ì „ë©´ ìš´ì˜ (6ê°œì›”+)
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: 1ì´ˆ ì´ë‚´ ë¶ˆëŸ‰ íŒì •
- **24/7 ëª¨ë‹ˆí„°ë§**: ì•Œë¦¼ ì‹œìŠ¤í…œ í†µí•©
- **ìë™ ì¬í•™ìŠµ**: ì£¼ê°„ ì„±ëŠ¥ í‰ê°€ ë° ëª¨ë¸ ì—…ë°ì´íŠ¸
- **ë¹„ìš© ì ˆê°**: ì—°ê°„ 20-30% í’ˆì§ˆ ë¹„ìš© ì ˆê°

### ğŸ’° ê²½ì œì  íš¨ê³¼ ë¶„ì„

#### ë¹„ìš©-í¸ìµ ë¶„ì„ (ì—°ê°„ ê¸°ì¤€)
```
ğŸ’µ íˆ¬ì ë¹„ìš©:
â”œâ”€â”€ ì‹œìŠ¤í…œ ê°œë°œ: $50,000
â”œâ”€â”€ ì¸í”„ë¼ êµ¬ì¶•: $30,000  
â”œâ”€â”€ ìš´ì˜ ì¸ë ¥: $80,000
â””â”€â”€ ì´ íˆ¬ì: $160,000

ğŸ’ ê¸°ëŒ€ íš¨ê³¼:
â”œâ”€â”€ ë¶ˆëŸ‰í’ˆ ì¡°ê¸° ë°œê²¬: $200,000
â”œâ”€â”€ ì¬ì‘ì—… ë¹„ìš© ì ˆê°: $150,000
â”œâ”€â”€ ê³ ê° í´ë ˆì„ ê°ì†Œ: $100,000  
â”œâ”€â”€ ê²€ì‚¬ ì¸ë ¥ ì ˆì•½: $120,000
â””â”€â”€ ì´ íš¨ê³¼: $570,000

ğŸ“Š ROI: 256% (íˆ¬ì ëŒ€ë¹„ 2.56ë°° ìˆ˜ìµ)
ğŸ“… íšŒìˆ˜ ê¸°ê°„: 4.2ê°œì›”
```

## ğŸš€ í–¥í›„ ë°œì „ ë°©í–¥

### ğŸ”® ê¸°ìˆ ì  ë¡œë“œë§µ

#### 2024ë…„ 4ë¶„ê¸°: ê¸°ëŠ¥ í™•ì¥
- [ ] **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: Apache Kafka í†µí•©
- [ ] **ì‹œê°ì  ëŒ€ì‹œë³´ë“œ**: Streamlit/Dash ì›¹ì•±
- [ ] **ëª¨ë°”ì¼ ì•Œë¦¼**: ê´€ë¦¬ì ì•± ê°œë°œ
- [ ] **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´/í•œêµ­ì–´/ì¤‘êµ­ì–´/ì¼ë³¸ì–´

#### 2025ë…„ 1ë¶„ê¸°: ì§€ëŠ¥í™”
- [ ] **ì ì‘í˜• ì„ê³„ê°’**: ì‹œê°„ëŒ€ë³„ ë™ì  ì¡°ì •  
- [ ] **ì„¤ëª…ê°€ëŠ¥ AI**: SHAP/LIME ê¸°ë°˜ í•´ì„
- [ ] **ì—°í•© í•™ìŠµ**: ë‹¤ì¤‘ ê³µì¥ ë°ì´í„° í™œìš©
- [ ] **ê°•í™” í•™ìŠµ**: ìµœì  ê²€ì‚¬ ì „ëµ í•™ìŠµ

#### 2025ë…„ 2ë¶„ê¸°: ìƒíƒœê³„ í™•ì¥
- [ ] **IoT ì„¼ì„œ í†µí•©**: í™˜ê²½ ë°ì´í„° ì‹¤ì‹œê°„ ì—°ë™
- [ ] **ë””ì§€í„¸ íŠ¸ìœˆ**: ê°€ìƒ ìƒì‚°ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
- [ ] **ë¸”ë¡ì²´ì¸**: í’ˆì§ˆ ì´ë ¥ ì¶”ì ì„± ë³´ì¥
- [ ] **ì—£ì§€ ì»´í“¨íŒ…**: í˜„ì¥ ì‹¤ì‹œê°„ ì²˜ë¦¬

### ğŸŒ ì‚°ì—… í™•ì¥ ê³„íš

#### ì ìš© ê°€ëŠ¥ ì‚°ì—…êµ°
1. **ìë™ì°¨**: ë¶€í’ˆ í’ˆì§ˆ ê²€ì‚¬
2. **ë°˜ë„ì²´**: ì›¨ì´í¼ ê²°í•¨ íƒì§€  
3. **ì‹í’ˆ**: ì•ˆì „ì„± ëª¨ë‹ˆí„°ë§
4. **ì œì•½**: GMP ì¤€ìˆ˜ ê²€ì¦
5. **í™”í•™**: ê³µì • ì•ˆì „ ê´€ë¦¬

#### ê¸°ìˆ  ì´ì „ ì „ëµ
```python
# ì‚°ì—…ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• í”„ë ˆì„ì›Œí¬
class IndustryAdapter:
    def __init__(self, industry_type):
        self.industry_config = load_industry_config(industry_type)
        
    def customize_features(self, raw_data):
        # ì‚°ì—…ë³„ íŠ¹í™” íŠ¹ì§• ìƒì„±
        return industry_specific_features(raw_data, self.industry_config)
    
    def set_business_rules(self):
        # ì‚°ì—…ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš©
        return industry_thresholds(self.industry_config)
```

## ğŸ“š ì°¸ê³  ìë£Œ ë° ì¶”ê°€ í•™ìŠµ

### ğŸ“– í•µì‹¬ ë…¼ë¬¸ ë° ì—°êµ¬
1. **[LGES DL AutoEncoder Fault Detection](https://www.kaggle.com/code/emphymachine/lges-dl-autoencoder-based-fault-detection-sol)** - ë³¸ í”„ë¡œì íŠ¸ì˜ ì˜ê° ì†ŒìŠ¤
2. **[Bosch Production Line Performance](https://www.kaggle.com/c/bosch-production-line-performance)** - ì›ë³¸ ë°ì´í„° ê²½ì§„ëŒ€íšŒ
3. **[Anomaly Detection: A Survey](https://dl.acm.org/doi/10.1145/3394486.3406473)** - ì´ìƒ íƒì§€ ì¢…í•© ë¦¬ë·°
4. **[Deep Learning for Anomaly Detection](https://arxiv.org/abs/1901.03407)** - ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€

### ğŸ› ï¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë„êµ¬
1. **[Imbalanced-Learn](https://imbalanced-learn.org/)** - ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
2. **[Optuna](https://optuna.org/)** - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
3. **[MLflow](https://mlflow.org/)** - MLOps í”Œë«í¼
4. **[SHAP](https://shap.readthedocs.io/)** - ëª¨ë¸ í•´ì„ì„±
5. **[Evidently](https://evidentlyai.com/)** - ë°ì´í„° ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§

### ğŸ“º ì¶”ì²œ ì˜¨ë¼ì¸ ê°•ì˜
1. **Coursera**: "Machine Learning for Production (MLOps)" - Andrew Ng
2. **edX**: "Introduction to Artificial Intelligence" - IBM  
3. **Udacity**: "Machine Learning Engineer Nanodegree"
4. **Fast.ai**: "Practical Deep Learning for Coders"

### ğŸ“‹ ì‹¤ìŠµ ë°ì´í„°ì…‹
1. **[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)** - ê¸ˆìœµ ì´ìƒ íƒì§€
2. **[Anomaly Detection in Time Series](https://www.kaggle.com/boltzmannbrain/nab)** - ì‹œê³„ì—´ ì´ìƒ íƒì§€  
3. **[Network Intrusion Detection](https://www.kaggle.com/sampadab17/network-intrusion-detection)** - ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ
4. **[Industrial Equipment Failure](https://www.kaggle.com/uciml/pmsm-temperature-data)** - ì‚°ì—… ì¥ë¹„ ëª¨ë‹ˆí„°ë§

### ğŸ¤ ì»¤ë®¤ë‹ˆí‹° ë° í¬ëŸ¼
1. **[Kaggle Learn](https://www.kaggle.com/learn)** - ë¬´ë£Œ ë¨¸ì‹ ëŸ¬ë‹ ê°•ì˜
2. **[Stack Overflow](https://stackoverflow.com/questions/tagged/machine-learning)** - ê¸°ìˆ  ì§ˆë¬¸ë‹µë³€
3. **[Reddit - MachineLearning](https://www.reddit.com/r/MachineLearning/)** - ìµœì‹  ì—°êµ¬ í† ë¡ 
4. **[Towards Data Science](https://towardsdatascience.com/)** - ì‹¤ë¬´ ì¤‘ì‹¬ ì•„í‹°í´

## ğŸ¯ ê²°ë¡  ë° í•µì‹¬ ê°€ì¹˜

### ğŸ’¡ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì„±ê³¼

#### ğŸ”¬ ê¸°ìˆ ì  í˜ì‹ 
- **ë‹¤ì¤‘ ì ‘ê·¼ë²• í†µí•©**: ì§€ë„/ë¹„ì§€ë„/ë”¥ëŸ¬ë‹ì˜ ìœ ê¸°ì  ê²°í•©
- **ê·¹ë„ ë¶ˆê· í˜• í•´ê²°**: 1:175 ë¹„ìœ¨ì—ì„œ ì˜ë¯¸ìˆëŠ” ì„±ëŠ¥ ë‹¬ì„±  
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: 1ì´ˆ ì´ë‚´ ë¶ˆëŸ‰ íŒì • ì‹œìŠ¤í…œ
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ë‹¤ì–‘í•œ ì œì¡° í™˜ê²½ ì ìš©

#### ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
- **ROI 256%**: 4.2ê°œì›” íšŒìˆ˜ ê¸°ê°„ìœ¼ë¡œ ë†’ì€ íˆ¬ì ìˆ˜ìµë¥ 
- **í’ˆì§ˆ ë¹„ìš© 30% ì ˆê°**: ë¶ˆëŸ‰í’ˆ ì¡°ê¸° ë°œê²¬ ë° ì¬ì‘ì—… ìµœì†Œí™”
- **24/7 ë¬´ì¸ ìš´ì˜**: ì§€ì†ì  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìë™í™”
- **ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •**: ê°ê´€ì  í’ˆì§ˆ ê´€ë¦¬ ê¸°ì¤€ ìˆ˜ë¦½

### ğŸŒŸ ì œì¡°ì—… AIì˜ ë¯¸ë˜

ì´ í”„ë¡œì íŠ¸ëŠ” **Industry 4.0** ì‹œëŒ€ì˜ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ êµ¬í˜„ì„ ìœ„í•œ ì‹¤ì§ˆì ì¸ ì†”ë£¨ì…˜ì„ ì œì‹œí•©ë‹ˆë‹¤:

1. **ì˜ˆì¸¡ì  í’ˆì§ˆê´€ë¦¬**: ì‚¬í›„ ëŒ€ì‘ì—ì„œ ì‚¬ì „ ì˜ˆë°©ìœ¼ë¡œ
2. **ì§€ëŠ¥í˜• ìë™í™”**: ì¸ê°„ì˜ ì§ê´€ê³¼ AIì˜ ì •í™•ì„± ê²°í•©  
3. **ì§€ì†ì  í•™ìŠµ**: ìš´ì˜ ë°ì´í„°ë¥¼ í†µí•œ ì‹œìŠ¤í…œ ì§„í™”
4. **ë¹„ìš© íš¨ìœ¨ì„±**: ìµœì†Œ íˆ¬ìë¡œ ìµœëŒ€ íš¨ê³¼ ì‹¤í˜„

### ğŸš€ ì‹œì‘í•˜ëŠ” ë°©ë²•

**1ë‹¨ê³„**: ê°„ë‹¨í•œ ë°ëª¨ë¶€í„°
```bash
python simple_fault_detection_demo.py
```

**2ë‹¨ê³„**: ë³¸ê²©ì ì¸ ì‹œìŠ¤í…œ íƒìƒ‰
```bash
python bosch_comprehensive_fault_detection.py
```

**3ë‹¨ê³„**: ì‹¤ì œ ë°ì´í„° ì ìš©
```python
# ì—¬ëŸ¬ë¶„ì˜ ë°ì´í„°ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•
detector = BoschFaultDetectionSuite(sampling_strategy='hybrid')
X, y = detector.load_and_preprocess_data("your_data.csv")
```

### ğŸ¤ ê¸°ì—¬ ë° í˜‘ì—…

ì´ í”„ë¡œì íŠ¸ëŠ” **ì˜¤í”ˆì†ŒìŠ¤ ì •ì‹ **ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤:

- **ì´ìŠˆ ì œê¸°**: ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ì œì•ˆ
- **ì½”ë“œ ê¸°ì—¬**: Pull Requestë¥¼ í†µí•œ ê°œì„ ì‚¬í•­ ì œì¶œ
- **ì‚¬ë¡€ ê³µìœ **: ì‹¤ì œ ì ìš© ì‚¬ë¡€ ë° ì„±ê³¼ ê³µìœ 
- **ì§€ì‹ ì „íŒŒ**: ì œì¡°ì—… AI í™•ì‚°ì„ ìœ„í•œ êµìœ¡ ë° ì»¨ì„¤íŒ…

### ğŸ“§ ì—°ë½ì²˜ ë° ì§€ì›

**í”„ë¡œì íŠ¸ ë¬¸ì˜**: GitHub Issues ë˜ëŠ” Discussion í™œìš©
**ê¸°ìˆ  ì§€ì›**: ìƒì„¸í•œ ë¬¸ì„œì™€ ì½”ë“œ ì£¼ì„ ì œê³µ
**êµìœ¡ ë¬¸ì˜**: ê¸°ì—… êµìœ¡ ë° ì»¨ì„¤íŒ… ê°€ëŠ¥
**í˜‘ì—… ì œì•ˆ**: ì‚°ì—… ì ìš© ë° ê³µë™ ì—°êµ¬ í™˜ì˜

---

## ğŸ“œ ë¼ì´ì„ ìŠ¤ ë° ì €ì‘ê¶Œ

```
MIT License

Copyright (c) 2024 Bosch Fault Detection Project

ë³¸ ì†Œí”„íŠ¸ì›¨ì–´ ë° ê´€ë ¨ ë¬¸ì„œ íŒŒì¼("ì†Œí”„íŠ¸ì›¨ì–´")ì˜ ì‚¬ë³¸ì„ ì–»ëŠ” 
ëª¨ë“  ì‚¬ëŒì—ê²Œ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì œí•œ ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¶Œí•œì„ 
ë¬´ë£Œë¡œ ë¶€ì—¬í•©ë‹ˆë‹¤.

ìœ„ ì €ì‘ê¶Œ ê³ ì§€ì™€ ë³¸ í—ˆê°€ ê³ ì§€ê°€ ì†Œí”„íŠ¸ì›¨ì–´ì˜ ëª¨ë“  ì‚¬ë³¸ ë˜ëŠ” 
ìƒë‹¹ ë¶€ë¶„ì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ì†Œí”„íŠ¸ì›¨ì–´ëŠ” "ìˆëŠ” ê·¸ëŒ€ë¡œ" ì œê³µë˜ë©°, ìƒí’ˆì„±, íŠ¹ì • ëª©ì ì—ì˜ 
ì í•©ì„± ë° ë¹„ì¹¨í•´ì— ëŒ€í•œ ë³´ì¦ì„ í¬í•¨í•˜ë˜ ì´ì— êµ­í•œë˜ì§€ ì•ŠëŠ” 
ëª…ì‹œì  ë˜ëŠ” ë¬µì‹œì  ë³´ì¦ ì—†ì´ ì œê³µë©ë‹ˆë‹¤.
```

---

**ğŸ­ Bosch Production Line Fault Detection Project**  
**ê°œë°œ**: Claude Code ğŸ¤– Ã— Human Intelligence ğŸ§   
**ë²„ì „**: v2.0 (2025ë…„ ì—…ë°ì´íŠ¸)  
**ë¼ì´ì„ ìŠ¤**: MIT  
**ê¸°ì—¬ì**: AI ê¸°ë°˜ ì œì¡°ì—… í˜ì‹ ì„ ê¿ˆê¾¸ëŠ” ëª¨ë“  ê°œë°œìë“¤ âœ¨

*"AIê°€ ë§Œë“œëŠ” ë” ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ì œì¡°ì—…ì˜ ë¯¸ë˜"* ğŸš€