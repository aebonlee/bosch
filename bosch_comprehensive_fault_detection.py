#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bosch Production Line - ì¢…í•©ì ì¸ ë¶ˆëŸ‰ ê²€ì¶œ ì†”ë£¨ì…˜
LGES DL AutoEncoder-based Fault Detection Solution ì°¸ê³ 

ë‹¤ì–‘í•œ ì ‘ê·¼ë²• êµ¬í˜„:
1. ì§€ë„í•™ìŠµ (Supervised Learning) - Binary Classification
2. ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning) - Clustering, Anomaly Detection  
3. AutoEncoder ê¸°ë°˜ ì´ìƒ íƒì§€
4. Isolation Forest
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import gc
import time
from datetime import datetime

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import (classification_report, confusion_matrix, 
                           matthews_corrcoef, roc_auc_score, f1_score)
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN

# ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.combine import SMOTETomek
    IMBALANCED_LEARN_AVAILABLE = True
except ImportError:
    IMBALANCED_LEARN_AVAILABLE = False
    print("Warning: imbalanced-learnì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ ìƒ˜í”Œë§ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# LightGBM
try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("Warning: LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# PyTorch (AutoEncoderìš©)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader, TensorDataset
    TORCH_AVAILABLE = True
    print("PyTorch ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. í†µê³„ì  ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

warnings.filterwarnings('ignore')

class BoschFaultDetectionSuite:
    """
    Bosch ìƒì‚°ë¼ì¸ ë¶ˆëŸ‰ ê²€ì¶œì„ ìœ„í•œ ì¢…í•©ì ì¸ ì†”ë£¨ì…˜
    """
    
    def __init__(self, sampling_strategy='hybrid', random_state=42):
        self.sampling_strategy = sampling_strategy  # 'under', 'over', 'hybrid'
        self.random_state = random_state
        self.scaler = None
        self.models = {}
        self.results = {}
        
    def load_and_preprocess_data(self, data_path, sample_size=100000):
        """
        ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        """
        print("=" * 80)
        print("ğŸ”§ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        print("=" * 80)
        
        # 1. ê¸°ë³¸ ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {sample_size:,}ê°œ ìƒ˜í”Œ")
        df = pd.read_csv(data_path, nrows=sample_size)
        print(f"   ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} x {df.shape[1]:,}")
        
        # 2. ê¸°ë³¸ ì •ë³´ í™•ì¸
        normal_count = (df['Response'] == 0).sum()
        fault_count = (df['Response'] == 1).sum()
        fault_rate = fault_count / len(df)
        
        print(f"\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
        print(f"   ì •ìƒ ì œí’ˆ: {normal_count:,}ê°œ ({(1-fault_rate)*100:.2f}%)")
        print(f"   ë¶ˆëŸ‰ ì œí’ˆ: {fault_count:,}ê°œ ({fault_rate*100:.2f}%)")
        print(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {int((1-fault_rate)/fault_rate)}:1")
        
        # 3. íŠ¹ì§• ì„ íƒ ë° ì •ë¦¬
        print(f"\nğŸ” íŠ¹ì§• ì „ì²˜ë¦¬:")
        
        # ê¸°ë³¸ íŠ¹ì§• ë¶„ë¦¬
        feature_cols = [col for col in df.columns if col not in ['Id', 'Response']]
        X_raw = df[feature_cols].copy()
        y = df['Response'].copy()
        
        print(f"   ì›ë³¸ íŠ¹ì§• ìˆ˜: {len(feature_cols):,}ê°œ")
        
        # 4. ê²°ì¸¡ê°’ì´ ë§ì€ íŠ¹ì§• ì œê±° (95% ì´ìƒ ê²°ì¸¡)
        missing_threshold = 0.95
        missing_ratio = X_raw.isnull().sum() / len(X_raw)
        valid_features = missing_ratio[missing_ratio < missing_threshold].index.tolist()
        
        X_filtered = X_raw[valid_features].copy()
        print(f"   ê²°ì¸¡ê°’ í•„í„°ë§ í›„: {len(valid_features):,}ê°œ")
        
        # 5. ë¶„ì‚°ì´ 0ì¸ íŠ¹ì§• ì œê±°
        X_temp = X_filtered.fillna(0)
        variances = X_temp.var()
        non_zero_var_features = variances[variances > 1e-8].index.tolist()
        
        X_clean = X_filtered[non_zero_var_features].copy()
        print(f"   ë¶„ì‚° í•„í„°ë§ í›„: {len(non_zero_var_features):,}ê°œ")
        
        # 6. ê²°ì¸¡ê°’ ì²˜ë¦¬
        print(f"\nğŸ› ï¸ ê²°ì¸¡ê°’ ì²˜ë¦¬:")
        missing_before = X_clean.isnull().sum().sum()
        X_clean = X_clean.fillna(X_clean.median())
        X_clean = X_clean.replace([np.inf, -np.inf], np.nan)
        X_clean = X_clean.fillna(X_clean.median())
        
        missing_after = X_clean.isnull().sum().sum()
        print(f"   ì²˜ë¦¬ ì „: {missing_before:,}ê°œ â†’ ì²˜ë¦¬ í›„: {missing_after:,}ê°œ")
        
        # 7. ê¸°ë³¸ ì§‘ê³„ íŠ¹ì§• ìƒì„±
        print(f"\nğŸ”§ íŠ¹ì§• ê³µí•™:")
        feature_df = pd.DataFrame()
        
        # ê¸°ë³¸ í†µê³„ íŠ¹ì§•
        feature_df['count_non_null'] = X_clean.count(axis=1)
        feature_df['count_zeros'] = (X_clean == 0).sum(axis=1)
        feature_df['missing_ratio'] = X_clean.isnull().sum(axis=1) / len(X_clean.columns)
        feature_df['mean'] = X_clean.mean(axis=1, skipna=True)
        feature_df['std'] = X_clean.std(axis=1, skipna=True)
        feature_df['min'] = X_clean.min(axis=1, skipna=True)
        feature_df['max'] = X_clean.max(axis=1, skipna=True)
        feature_df['range'] = feature_df['max'] - feature_df['min']
        feature_df['median'] = X_clean.median(axis=1, skipna=True)
        
        # ìŠ¤í…Œì´ì…˜ë³„ ì§‘ê³„ (ìƒìœ„ 5ê°œ ìŠ¤í…Œì´ì…˜)
        station_groups = {}
        for col in X_clean.columns:
            parts = col.split('_')
            if len(parts) >= 2:
                station = parts[0] + '_' + parts[1]
                if station not in station_groups:
                    station_groups[station] = []
                station_groups[station].append(col)
        
        top_stations = sorted(station_groups.items(), key=lambda x: len(x[1]), reverse=True)[:5]
        
        for station, cols in top_stations:
            if len(cols) > 1:
                station_data = X_clean[cols]
                feature_df[f'{station}_mean'] = station_data.mean(axis=1, skipna=True)
                feature_df[f'{station}_count'] = station_data.count(axis=1)
        
        # NaN ê°’ ì²˜ë¦¬
        feature_df = feature_df.fillna(0)
        
        print(f"   ìƒì„±ëœ ì§‘ê³„ íŠ¹ì§•: {len(feature_df.columns)}ê°œ")
        
        # 8. ìŠ¤ì¼€ì¼ë§
        print(f"\níŠ¹ì§• ìŠ¤ì¼€ì¼ë§:")\n        self.scaler = RobustScaler()\n        X_scaled = self.scaler.fit_transform(feature_df)\n        \n        print(f"   ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: RobustScaler ì‚¬ìš©")\n        \n        # ìµœì¢… ë°ì´í„° ì •ë³´ ì €ì¥\n        self.X = X_scaled\n        self.y = y.values\n        self.feature_names = feature_df.columns.tolist()\n        \n        print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")\n        print(f"   ìµœì¢… ë°ì´í„° í¬ê¸°: {self.X.shape[0]:,} x {self.X.shape[1]:,}")\n        print(f"   ë¶ˆëŸ‰ë¥ : {self.y.mean():.4%}")\n        \n        return self.X, self.y
    \n    def apply_sampling(self, X, y):\n        \"\"\"\n        ë¶ˆê· í˜• ë°ì´í„° ìƒ˜í”Œë§ ì ìš©\n        \"\"\"\n        print(f\"\\nâš–ï¸ ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ({self.sampling_strategy}):\")\n        \n        original_counts = np.bincount(y)\n        print(f\"   ì›ë³¸ ë¶„í¬: ì •ìƒ {original_counts[0]:,}, ë¶ˆëŸ‰ {original_counts[1]:,}\")\n        \n        if self.sampling_strategy == 'under':\n            # ì–¸ë”ìƒ˜í”Œë§: ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì†Œìˆ˜ í´ë˜ìŠ¤ ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì„\n            if IMBALANCED_LEARN_AVAILABLE:\n                sampler = RandomUnderSampler(random_state=self.random_state)\n                X_resampled, y_resampled = sampler.fit_resample(X, y)\n            else:\n                # ìˆ˜ë™ ì–¸ë”ìƒ˜í”Œë§\n                minority_count = original_counts[1]\n                normal_indices = np.where(y == 0)[0]\n                fault_indices = np.where(y == 1)[0]\n                \n                selected_normal = np.random.choice(normal_indices, minority_count, replace=False)\n                selected_indices = np.concatenate([selected_normal, fault_indices])\n                \n                X_resampled = X[selected_indices]\n                y_resampled = y[selected_indices]\n        \n        elif self.sampling_strategy == 'over':\n            # ì˜¤ë²„ìƒ˜í”Œë§: SMOTE ì‚¬ìš©\n            if IMBALANCED_LEARN_AVAILABLE:\n                sampler = SMOTE(random_state=self.random_state)\n                X_resampled, y_resampled = sampler.fit_resample(X, y)\n            else:\n                # ê°„ë‹¨í•œ ë³µì œ ê¸°ë°˜ ì˜¤ë²„ìƒ˜í”Œë§\n                majority_count = original_counts[0]\n                minority_count = original_counts[1]\n                \n                fault_indices = np.where(y == 1)[0]\n                n_copies = majority_count // minority_count\n                \n                fault_X = X[fault_indices]\n                fault_y = y[fault_indices]\n                \n                # ë³µì œ\n                replicated_X = np.tile(fault_X, (n_copies, 1))\n                replicated_y = np.tile(fault_y, n_copies)\n                \n                X_resampled = np.vstack([X, replicated_X])\n                y_resampled = np.concatenate([y, replicated_y])\n        \n        elif self.sampling_strategy == 'hybrid':\n            # í•˜ì´ë¸Œë¦¬ë“œ: ì ì ˆí•œ ê· í˜• ë§ì¶”ê¸°\n            target_size = 20000  # ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ í¬ê¸°\n            \n            if IMBALANCED_LEARN_AVAILABLE:\n                # ë¨¼ì € ì–¸ë”ìƒ˜í”Œë§\n                under_sampler = RandomUnderSampler(\n                    sampling_strategy={0: target_size * 3, 1: original_counts[1]},\n                    random_state=self.random_state\n                )\n                X_temp, y_temp = under_sampler.fit_resample(X, y)\n                \n                # ê·¸ë‹¤ìŒ ì˜¤ë²„ìƒ˜í”Œë§\n                over_sampler = SMOTE(\n                    sampling_strategy={1: target_size},\n                    random_state=self.random_state\n                )\n                X_resampled, y_resampled = over_sampler.fit_resample(X_temp, y_temp)\n            else:\n                # ìˆ˜ë™ í•˜ì´ë¸Œë¦¬ë“œ\n                normal_indices = np.where(y == 0)[0]\n                fault_indices = np.where(y == 1)[0]\n                \n                # ì •ìƒ ë°ì´í„° ì–¸ë”ìƒ˜í”Œë§\n                selected_normal = np.random.choice(normal_indices, target_size, replace=False)\n                \n                # ë¶ˆëŸ‰ ë°ì´í„° ì˜¤ë²„ìƒ˜í”Œë§ (ë³µì œ)\n                n_copies = target_size // len(fault_indices)\n                selected_fault = np.tile(fault_indices, n_copies)\n                remaining = target_size - len(selected_fault)\n                if remaining > 0:\n                    additional_fault = np.random.choice(fault_indices, remaining, replace=True)\n                    selected_fault = np.concatenate([selected_fault, additional_fault])\n                \n                selected_indices = np.concatenate([selected_normal, selected_fault])\n                X_resampled = X[selected_indices]\n                y_resampled = y[selected_indices]\n        \n        else:\n            # ìƒ˜í”Œë§ ì•ˆí•¨\n            X_resampled, y_resampled = X, y\n        \n        new_counts = np.bincount(y_resampled)\n        print(f\"   ìƒ˜í”Œë§ í›„: ì •ìƒ {new_counts[0]:,}, ë¶ˆëŸ‰ {new_counts[1]:,}\")\n        print(f\"   ìƒˆë¡œìš´ ê· í˜•: {new_counts[0]/new_counts[1]:.1f}:1\")\n        \n        return X_resampled, y_resampled\n    \n    def train_supervised_models(self, X, y):\n        \"\"\"\n        ì§€ë„í•™ìŠµ ëª¨ë¸ í•™ìŠµ (Binary Classification)\n        \"\"\"\n        print(f\"\\nğŸ¤– ì§€ë„í•™ìŠµ ëª¨ë¸ í•™ìŠµ:\")\n        print(\"-\" * 50)\n        \n        # ë°ì´í„° ë¶„í• \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=self.random_state, stratify=y\n        )\n        \n        print(f\"   í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ\")\n        print(f\"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ\")\n        \n        # ë¶ˆê· í˜• ì²˜ë¦¬\n        X_train_balanced, y_train_balanced = self.apply_sampling(X_train, y_train)\n        \n        models_to_train = []\n        \n        # LightGBM\n        if LIGHTGBM_AVAILABLE:\n            lgbm = LGBMClassifier(\n                n_estimators=100,\n                learning_rate=0.1,\n                max_depth=6,\n                random_state=self.random_state,\n                class_weight='balanced',\n                verbosity=-1\n            )\n            models_to_train.append(('LightGBM', lgbm))\n        \n        # Random Forest (sklearn ê¸°ë³¸)\n        from sklearn.ensemble import RandomForestClassifier\n        rf = RandomForestClassifier(\n            n_estimators=100,\n            max_depth=8,\n            random_state=self.random_state,\n            class_weight='balanced',\n            n_jobs=-1\n        )\n        models_to_train.append(('RandomForest', rf))\n        \n        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n        for name, model in models_to_train:\n            print(f\"\\n   ğŸ”§ {name} í•™ìŠµ ì¤‘...\")\n            \n            start_time = time.time()\n            model.fit(X_train_balanced, y_train_balanced)\n            train_time = time.time() - start_time\n            \n            # ì˜ˆì¸¡\n            y_pred = model.predict(X_test)\n            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n            \n            # í‰ê°€\n            mcc = matthews_corrcoef(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n            auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0\n            \n            self.models[name] = model\n            self.results[name] = {\n                'MCC': mcc,\n                'F1': f1,\n                'AUC': auc,\n                'train_time': train_time,\n                'predictions': y_pred,\n                'probabilities': y_pred_proba\n            }\n            \n            print(f\"      MCC: {mcc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n            print(f\"      í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ\")\n        \n        return X_test, y_test\n    \n    def train_isolation_forest(self, X, y):\n        \"\"\"\n        Isolation Forest ì´ìƒ íƒì§€\n        \"\"\"\n        print(f\"\\nğŸŒ² Isolation Forest ì´ìƒ íƒì§€:\")\n        print(\"-\" * 50)\n        \n        # ë°ì´í„° ë¶„í• \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=self.random_state, stratify=y\n        )\n        \n        # ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ\n        X_train_normal = X_train[y_train == 0]\n        print(f\"   ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ: {len(X_train_normal):,}ê°œ\")\n        \n        # ë¶ˆëŸ‰ë¥ ì— ê¸°ë°˜í•œ contamination ì„¤ì •\n        contamination = y.mean()\n        print(f\"   ì˜ˆìƒ contamination: {contamination:.4%}\")\n        \n        # Isolation Forest ëª¨ë¸\n        iforest = IsolationForest(\n            n_estimators=100,\n            max_samples='auto',\n            contamination=contamination,\n            random_state=self.random_state,\n            n_jobs=-1\n        )\n        \n        start_time = time.time()\n        iforest.fit(X_train_normal)  # ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµ\n        train_time = time.time() - start_time\n        \n        # ì˜ˆì¸¡ (1: normal, -1: abnormal)\n        y_pred_raw = iforest.predict(X_test)\n        y_pred = (y_pred_raw == -1).astype(int)  # -1ì„ 1ë¡œ, 1ì„ 0ìœ¼ë¡œ ë³€í™˜\n        \n        # ì´ìƒ ì ìˆ˜\n        anomaly_scores = iforest.decision_function(X_test)\n        \n        # í‰ê°€\n        mcc = matthews_corrcoef(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred)\n        auc = roc_auc_score(y_test, -anomaly_scores)  # ìŒìˆ˜ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)\n        \n        self.models['IsolationForest'] = iforest\n        self.results['IsolationForest'] = {\n            'MCC': mcc,\n            'F1': f1,\n            'AUC': auc,\n            'train_time': train_time,\n            'predictions': y_pred,\n            'anomaly_scores': anomaly_scores\n        }\n        \n        print(f\"   ê²°ê³¼: MCC: {mcc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n        print(f\"   í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ\")\n        \n        return X_test, y_test\n    \n    def train_autoencoder(self, X, y):\n        \"\"\"\n        AutoEncoder ê¸°ë°˜ ì´ìƒ íƒì§€\n        \"\"\"\n        if not TORCH_AVAILABLE:\n            print(f\"\\nâŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ AutoEncoderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n            return None, None\n        \n        print(f\"\\nğŸ§  AutoEncoder ì´ìƒ íƒì§€:\")\n        print(\"-\" * 50)\n        \n        # ë°ì´í„° ë¶„í• \n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=self.random_state, stratify=y\n        )\n        \n        # ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©\n        X_train_normal = X_train[y_train == 0]\n        print(f\"   ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ: {len(X_train_normal):,}ê°œ\")\n        \n        # AutoEncoder ëª¨ë¸ ì •ì˜\n        class AutoEncoder(nn.Module):\n            def __init__(self, input_dim, encoding_dim=64):\n                super(AutoEncoder, self).__init__()\n                \n                # ì¸ì½”ë”\n                self.encoder = nn.Sequential(\n                    nn.Linear(input_dim, input_dim // 2),\n                    nn.ReLU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(input_dim // 2, input_dim // 4),\n                    nn.ReLU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(input_dim // 4, encoding_dim),\n                    nn.ReLU()\n                )\n                \n                # ë””ì½”ë”\n                self.decoder = nn.Sequential(\n                    nn.Linear(encoding_dim, input_dim // 4),\n                    nn.ReLU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(input_dim // 4, input_dim // 2),\n                    nn.ReLU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(input_dim // 2, input_dim)\n                )\n            \n            def forward(self, x):\n                encoded = self.encoder(x)\n                decoded = self.decoder(encoded)\n                return decoded\n        \n        # ëª¨ë¸ ìƒì„±\n        input_dim = X_train.shape[1]\n        model = AutoEncoder(input_dim, encoding_dim=32)\n        \n        # ë””ë°”ì´ìŠ¤ ì„¤ì •\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = model.to(device)\n        \n        print(f\"   ë””ë°”ì´ìŠ¤: {device}\")\n        print(f\"   ì…ë ¥ ì°¨ì›: {input_dim}\")\n        \n        # í•™ìŠµ ì„¤ì •\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        \n        # ë°ì´í„° ë¡œë”\n        train_dataset = TensorDataset(\n            torch.FloatTensor(X_train_normal),\n            torch.FloatTensor(X_train_normal)  # AutoEncoderëŠ” ì…ë ¥=ì¶œë ¥\n        )\n        train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n        \n        # í•™ìŠµ\n        model.train()\n        epochs = 50\n        print(f\"   ì—í¬í¬: {epochs}\")\n        \n        start_time = time.time()\n        \n        for epoch in range(epochs):\n            epoch_loss = 0\n            for batch_x, batch_y in train_loader:\n                batch_x = batch_x.to(device)\n                batch_y = batch_y.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(batch_x)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n                \n                epoch_loss += loss.item()\n            \n            if (epoch + 1) % 10 == 0:\n                print(f\"   ì—í¬í¬ {epoch+1}/{epochs}, ì†ì‹¤: {epoch_loss/len(train_loader):.6f}\")\n        \n        train_time = time.time() - start_time\n        \n        # ì„ê³„ê°’ ê³„ì‚° (ì •ìƒ ë°ì´í„°ì˜ ì¬êµ¬ì„± ì˜¤ì°¨)\n        model.eval()\n        with torch.no_grad():\n            X_train_tensor = torch.FloatTensor(X_train_normal).to(device)\n            train_reconstructed = model(X_train_tensor).cpu().numpy()\n            train_errors = np.mean(np.square(X_train_normal - train_reconstructed), axis=1)\n            threshold = np.percentile(train_errors, 95)  # 95í¼ì„¼íƒ€ì¼ ì„ê³„ê°’\n        \n        print(f\"   ì„ê³„ê°’: {threshold:.6f}\")\n        \n        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡\n        with torch.no_grad():\n            X_test_tensor = torch.FloatTensor(X_test).to(device)\n            test_reconstructed = model(X_test_tensor).cpu().numpy()\n            test_errors = np.mean(np.square(X_test - test_reconstructed), axis=1)\n        \n        # ì´ìƒ íƒì§€\n        y_pred = (test_errors > threshold).astype(int)\n        \n        # í‰ê°€\n        mcc = matthews_corrcoef(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred)\n        auc = roc_auc_score(y_test, test_errors)\n        \n        self.models['AutoEncoder'] = model\n        self.results['AutoEncoder'] = {\n            'MCC': mcc,\n            'F1': f1,\n            'AUC': auc,\n            'train_time': train_time,\n            'predictions': y_pred,\n            'reconstruction_errors': test_errors,\n            'threshold': threshold\n        }\n        \n        print(f\"   ê²°ê³¼: MCC: {mcc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n        print(f\"   í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ\")\n        \n        return X_test, y_test\n    \n    def print_summary(self):\n        \"\"\"\n        ì „ì²´ ê²°ê³¼ ìš”ì•½\n        \"\"\"\n        print(f\"\\n\" + \"=\" * 80)\n        print(\"ğŸ“‹ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½\")\n        print(\"=\" * 80)\n        \n        if not self.results:\n            print(\"í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n            return\n        \n        # ê²°ê³¼ ì •ë¦¬\n        summary_df = pd.DataFrame()\n        for name, result in self.results.items():\n            summary_df = pd.concat([summary_df, pd.DataFrame({\n                'Model': [name],\n                'MCC': [result['MCC']],\n                'F1-Score': [result['F1']],\n                'AUC': [result['AUC']],\n                'Train Time (s)': [result['train_time']]\n            })], ignore_index=True)\n        \n        # ì •ë ¬ (MCC ê¸°ì¤€)\n        summary_df = summary_df.sort_values('MCC', ascending=False)\n        \n        print(summary_df.to_string(index=False, float_format='%.4f'))\n        \n        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n        best_model = summary_df.iloc[0]['Model']\n        best_mcc = summary_df.iloc[0]['MCC']\n        \n        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model} (MCC: {best_mcc:.4f})\")\n        \n        # ìƒì„¸ ë¶„ì„ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)\n        if best_model in self.results:\n            result = self.results[best_model]\n            if 'predictions' in result:\n                predictions = result['predictions']\n                print(f\"\\nğŸ“Š {best_model} ìƒì„¸ ê²°ê³¼:\")\n                print(f\"   ì˜ˆì¸¡ëœ ë¶ˆëŸ‰í’ˆ ìˆ˜: {predictions.sum():,}ê°œ\")\n                print(f\"   ë¶ˆëŸ‰ ì˜ˆì¸¡ë¥ : {predictions.mean():.2%}\")\n\ndef main():\n    \"\"\"\n    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n    \"\"\"\n    print(\"ğŸ­ Bosch Production Line - ì¢…í•© ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ\")\n    print(\"=\" * 80)\n    \n    try:\n        # ë°ì´í„° ê²½ë¡œ\n        data_path = \"C:/Users/ASUS/bosch/data/train_numeric.csv\"\n        \n        # ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n        detector = BoschFaultDetectionSuite(sampling_strategy='hybrid')\n        \n        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n        X, y = detector.load_and_preprocess_data(data_path, sample_size=100000)\n        \n        # 2. ì§€ë„í•™ìŠµ ëª¨ë¸ í•™ìŠµ\n        print(f\"\\n\" + \"=\" * 80)\n        print(\"ğŸ¯ ì§€ë„í•™ìŠµ ì ‘ê·¼ë²• (Binary Classification)\")\n        print(\"=\" * 80)\n        X_test, y_test = detector.train_supervised_models(X, y)\n        \n        # 3. Isolation Forest\n        print(f\"\\n\" + \"=\" * 80)\n        print(\"ğŸŒ² ë¹„ì§€ë„í•™ìŠµ ì ‘ê·¼ë²• (Anomaly Detection)\")\n        print(\"=\" * 80)\n        detector.train_isolation_forest(X, y)\n        \n        # 4. AutoEncoder\n        print(f\"\\n\" + \"=\" * 80)\n        print(\"ğŸ§  ë”¥ëŸ¬ë‹ ì ‘ê·¼ë²• (AutoEncoder)\")\n        print(\"=\" * 80)\n        detector.train_autoencoder(X, y)\n        \n        # 5. ê²°ê³¼ ìš”ì•½\n        detector.print_summary()\n        \n        print(f\"\\nâœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n        print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n        print(f\"   1. ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ\")\n        print(f\"   2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna ë“± í™œìš©)\")\n        print(f\"   3. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±\")\n        print(f\"   4. ë‚ ì§œ/ë²”ì£¼í˜• ë°ì´í„° ì¶”ê°€ í™œìš©\")\n        \n    except Exception as e:\n        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()