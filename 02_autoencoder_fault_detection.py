#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bosch Production Line - AutoEncoder ê¸°ë°˜ ì´ìƒ íƒì§€ ì˜ˆì œ
LGES DL AutoEncoder Fault Detection Solution ì°¸ê³ 

ì´ ì˜ˆì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
1. ì •ìƒ ì œí’ˆ ë°ì´í„°ë¡œ AutoEncoder í•™ìŠµ
2. ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ í†µí•œ ì´ìƒ íƒì§€
3. ì„ê³„ê°’ ê¸°ë°˜ ë¶ˆëŸ‰í’ˆ ë¶„ë¥˜
"""

import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# TensorFlow/Keras ì„í¬íŠ¸ (ì¡°ê±´ë¶€)
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TF_AVAILABLE = True
    print("TensorFlow ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    TF_AVAILABLE = False
    print("TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ëŒ€ì•ˆ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

warnings.filterwarnings('ignore')

class BoschAutoEncoderFaultDetector:
    """
    Bosch ìƒì‚°ë¼ì¸ ë°ì´í„°ë¥¼ ìœ„í•œ AutoEncoder ê¸°ë°˜ ì´ìƒ íƒì§€ê¸°
    """
    
    def __init__(self, encoding_dim=32, learning_rate=0.001):
        self.encoding_dim = encoding_dim
        self.learning_rate = learning_rate
        self.autoencoder = None
        self.encoder = None
        self.decoder = None
        self.scaler = None
        self.threshold = None
        self.history = None
        
    def prepare_data(self, data_path, sample_size=50000):
        """
        ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            data_path (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            sample_size (int): ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìƒ˜í”Œ í¬ê¸°
        """
        print("=" * 60)
        print("ğŸ”§ ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬")
        print("=" * 60)
        
        # ë°ì´í„° ë¡œë“œ
        print(f"ë°ì´í„° ë¡œë”© ì¤‘... (ìƒ˜í”Œ í¬ê¸°: {sample_size:,})")
        df = pd.read_csv(data_path, nrows=sample_size)
        print(f"ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} x {df.shape[1]:,}")
        
        # ê¸°ë³¸ ì •ë³´
        normal_count = (df['Response'] == 0).sum()
        fault_count = (df['Response'] == 1).sum()
        fault_rate = fault_count / len(df)
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬:")
        print(f"   ì •ìƒ ì œí’ˆ: {normal_count:,}ê°œ ({(1-fault_rate)*100:.2f}%)")
        print(f"   ë¶ˆëŸ‰ ì œí’ˆ: {fault_count:,}ê°œ ({fault_rate*100:.2f}%)")
        
        # íŠ¹ì§• ì„ íƒ ë° ì •ë¦¬
        print(f"\nğŸ” íŠ¹ì§• ì„ íƒ ë° ì •ë¦¬:")
        
        # IDì™€ Response ì œì™¸
        feature_cols = [col for col in df.columns if col not in ['Id', 'Response']]
        X = df[feature_cols].copy()
        y = df['Response'].copy()
        
        print(f"   ì›ë³¸ íŠ¹ì§• ìˆ˜: {len(feature_cols):,}ê°œ")
        
        # ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ì€ íŠ¹ì§• ì œê±° (95% ì´ìƒ)
        missing_threshold = 0.95
        missing_ratio = X.isnull().sum() / len(X)
        valid_features = missing_ratio[missing_ratio < missing_threshold].index.tolist()
        
        X_filtered = X[valid_features].copy()
        print(f"   ê²°ì¸¡ê°’ í•„í„°ë§ í›„: {len(valid_features):,}ê°œ")
        
        # ë¶„ì‚°ì´ 0ì¸ íŠ¹ì§• ì œê±°
        X_filled = X_filtered.fillna(0)  # ì„ì‹œë¡œ 0ìœ¼ë¡œ ì±„ì›€
        variances = X_filled.var()
        non_zero_var_features = variances[variances > 1e-8].index.tolist()
        
        X_final = X_filtered[non_zero_var_features].copy()
        print(f"   ë¶„ì‚° í•„í„°ë§ í›„: {len(non_zero_var_features):,}ê°œ")
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´)
        print(f"\nğŸ› ï¸ ê²°ì¸¡ê°’ ì²˜ë¦¬:")
        missing_before = X_final.isnull().sum().sum()
        
        # ì¤‘ì•™ê°’ìœ¼ë¡œ ê²°ì¸¡ê°’ ì±„ìš°ê¸°
        X_final = X_final.fillna(X_final.median())
        
        missing_after = X_final.isnull().sum().sum()
        print(f"   ì²˜ë¦¬ ì „ ê²°ì¸¡ê°’: {missing_before:,}ê°œ")
        print(f"   ì²˜ë¦¬ í›„ ê²°ì¸¡ê°’: {missing_after:,}ê°œ")
        
        # ë¬´í•œê°’ ì²˜ë¦¬
        X_final = X_final.replace([np.inf, -np.inf], np.nan)
        X_final = X_final.fillna(X_final.median())
        
        # ìµœì¢… ë°ì´í„° ì •ë³´
        print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ìµœì¢… íŠ¹ì§• ìˆ˜: {X_final.shape[1]:,}ê°œ")
        print(f"   ìƒ˜í”Œ ìˆ˜: {X_final.shape[0]:,}ê°œ")
        
        # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ (RobustScaler ì‚¬ìš© - ì´ìƒê°’ì— ê°•í•¨)
        print(f"\nğŸ“ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ (RobustScaler):")
        self.scaler = RobustScaler()
        X_scaled = self.scaler.fit_transform(X_final)
        
        print(f"   ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: í‰ê· â‰ˆ0, ì¤‘ì•™ê°’ ê¸°ì¤€ ì •ê·œí™”")
        
        # ì •ìƒ ë°ì´í„°ì™€ ë¶ˆëŸ‰ ë°ì´í„° ë¶„ë¦¬
        normal_indices = y == 0
        fault_indices = y == 1
        
        X_normal = X_scaled[normal_indices]
        X_fault = X_scaled[fault_indices]
        
        print(f"\nğŸ¯ ë°ì´í„° ë¶„ë¦¬:")
        print(f"   ì •ìƒ ë°ì´í„°: {X_normal.shape[0]:,} x {X_normal.shape[1]:,}")
        print(f"   ë¶ˆëŸ‰ ë°ì´í„°: {X_fault.shape[0]:,} x {X_fault.shape[1]:,}")
        
        return X_normal, X_fault, X_scaled, y
    
    def build_autoencoder(self, input_dim):
        """
        AutoEncoder ëª¨ë¸ êµ¬ì¶•
        
        Args:
            input_dim (int): ì…ë ¥ ì°¨ì›
        """
        if not TF_AVAILABLE:
            print("âŒ TensorFlowê°€ ì—†ì–´ì„œ AutoEncoderë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\nğŸ—ï¸ AutoEncoder ëª¨ë¸ êµ¬ì¶•:")
        print(f"   ì…ë ¥ ì°¨ì›: {input_dim}")
        print(f"   ì¸ì½”ë”© ì°¨ì›: {self.encoding_dim}")
        
        # ì…ë ¥ì¸µ
        input_layer = layers.Input(shape=(input_dim,))
        
        # ì¸ì½”ë”
        # ì ì§„ì ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ
        encoder_layers = input_layer
        layer_dims = [int(input_dim * 0.75), int(input_dim * 0.5), 
                     int(input_dim * 0.25), self.encoding_dim]
        
        print(f"   ì¸ì½”ë” êµ¬ì¡°:")
        for i, dim in enumerate(layer_dims):
            encoder_layers = layers.Dense(dim, activation='relu', 
                                        name=f'encoder_{i+1}')(encoder_layers)
            encoder_layers = layers.Dropout(0.2)(encoder_layers)
            print(f"     ë ˆì´ì–´ {i+1}: {dim}ê°œ ë‰´ëŸ°")
        
        # ì¸ì½”ë” ëª¨ë¸
        self.encoder = Model(input_layer, encoder_layers, name='encoder')
        
        # ë””ì½”ë”
        # ì ì§„ì ìœ¼ë¡œ ì°¨ì› ë³µì›
        decoder_input = layers.Input(shape=(self.encoding_dim,))
        decoder_layers = decoder_input
        
        decode_dims = layer_dims[:-1][::-1] + [input_dim]  # ì—­ìˆœ + ì›ë³¸ ì°¨ì›
        
        print(f"   ë””ì½”ë” êµ¬ì¡°:")
        for i, dim in enumerate(decode_dims):
            activation = 'relu' if i < len(decode_dims) - 1 else 'linear'
            decoder_layers = layers.Dense(dim, activation=activation,
                                        name=f'decoder_{i+1}')(decoder_layers)
            if i < len(decode_dims) - 1:  # ë§ˆì§€ë§‰ ì¸µì—ëŠ” Dropout ì ìš© ì•ˆí•¨
                decoder_layers = layers.Dropout(0.2)(decoder_layers)
            print(f"     ë ˆì´ì–´ {i+1}: {dim}ê°œ ë‰´ëŸ° ({activation})")
        
        # ë””ì½”ë” ëª¨ë¸
        self.decoder = Model(decoder_input, decoder_layers, name='decoder')
        
        # ì „ì²´ AutoEncoder
        encoded = self.encoder(input_layer)
        decoded = self.decoder(encoded)
        self.autoencoder = Model(input_layer, decoded, name='autoencoder')
        
        # ì»´íŒŒì¼
        optimizer = Adam(learning_rate=self.learning_rate)
        self.autoencoder.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae']
        )
        
        print(f"\nğŸ“‹ ëª¨ë¸ ìš”ì•½:")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {self.autoencoder.count_params():,}ê°œ")
        
        return self.autoencoder
    
    def train_autoencoder(self, X_normal, validation_split=0.2, epochs=100, batch_size=256):
        """
        ì •ìƒ ë°ì´í„°ë¡œ AutoEncoder í•™ìŠµ
        
        Args:
            X_normal (np.array): ì •ìƒ ë°ì´í„°
            validation_split (float): ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            epochs (int): í•™ìŠµ ì—í¬í¬
            batch_size (int): ë°°ì¹˜ í¬ê¸°
        """
        if not TF_AVAILABLE or self.autoencoder is None:
            print("âŒ AutoEncoder ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\nğŸš€ AutoEncoder í•™ìŠµ ì‹œì‘:")
        print(f"   í•™ìŠµ ë°ì´í„°: {X_normal.shape[0]:,}ê°œ")
        print(f"   ê²€ì¦ ë¶„í• : {validation_split*100:.0f}%")
        print(f"   ì—í¬í¬: {epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # í•™ìŠµ (ì •ìƒ ë°ì´í„°ë¡œë§Œ)
        # AutoEncoderëŠ” ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ë„ë¡ í•™ìŠµ
        self.history = self.autoencoder.fit(
            X_normal, X_normal,  # ì…ë ¥ê³¼ ì¶œë ¥ì´ ê°™ìŒ
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )
        
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        
        # ìµœì¢… ì†ì‹¤ê°’
        final_loss = self.history.history['loss'][-1]
        final_val_loss = self.history.history['val_loss'][-1]
        
        print(f"   ìµœì¢… í•™ìŠµ ì†ì‹¤: {final_loss:.6f}")
        print(f"   ìµœì¢… ê²€ì¦ ì†ì‹¤: {final_val_loss:.6f}")
        
        return self.history
    
    def calculate_threshold(self, X_normal, percentile=95):
        """
        ì´ìƒ íƒì§€ ì„ê³„ê°’ ê³„ì‚°
        
        Args:
            X_normal (np.array): ì •ìƒ ë°ì´í„°
            percentile (float): ì„ê³„ê°’ ë°±ë¶„ìœ„ìˆ˜
        """
        if not TF_AVAILABLE or self.autoencoder is None:
            print("âŒ í•™ìŠµëœ AutoEncoder ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\nğŸ¯ ì´ìƒ íƒì§€ ì„ê³„ê°’ ê³„ì‚°:")
        
        # ì •ìƒ ë°ì´í„°ì˜ ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°
        normal_predictions = self.autoencoder.predict(X_normal, verbose=0)
        normal_errors = np.mean(np.square(X_normal - normal_predictions), axis=1)
        
        # ì„ê³„ê°’ ì„¤ì • (ë°±ë¶„ìœ„ìˆ˜ ê¸°ì¤€)
        self.threshold = np.percentile(normal_errors, percentile)
        
        print(f"   ì •ìƒ ë°ì´í„° ì¬êµ¬ì„± ì˜¤ì°¨:")
        print(f"     í‰ê· : {normal_errors.mean():.6f}")
        print(f"     í‘œì¤€í¸ì°¨: {normal_errors.std():.6f}")
        print(f"     ìµœì†Ÿê°’: {normal_errors.min():.6f}")
        print(f"     ìµœëŒ“ê°’: {normal_errors.max():.6f}")
        print(f"   ì„ê³„ê°’ ({percentile}%): {self.threshold:.6f}")
        
        return self.threshold, normal_errors
    
    def detect_anomalies(self, X_test, y_test=None):
        """
        ì´ìƒ íƒì§€ ìˆ˜í–‰
        
        Args:
            X_test (np.array): í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test (np.array): ì‹¤ì œ ë ˆì´ë¸” (í‰ê°€ìš©)
        """
        if not TF_AVAILABLE or self.autoencoder is None or self.threshold is None:
            print("âŒ ëª¨ë¸ì´ë‚˜ ì„ê³„ê°’ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"\nğŸ” ì´ìƒ íƒì§€ ìˆ˜í–‰:")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ")
        
        # ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°
        test_predictions = self.autoencoder.predict(X_test, verbose=0)
        test_errors = np.mean(np.square(X_test - test_predictions), axis=1)
        
        # ì„ê³„ê°’ ê¸°ì¤€ ì´ìƒ íƒì§€
        anomaly_predictions = (test_errors > self.threshold).astype(int)
        
        print(f"   ì¬êµ¬ì„± ì˜¤ì°¨ í†µê³„:")
        print(f"     í‰ê· : {test_errors.mean():.6f}")
        print(f"     í‘œì¤€í¸ì°¨: {test_errors.std():.6f}")
        print(f"     ì„ê³„ê°’ ì´ˆê³¼: {anomaly_predictions.sum():,}ê°œ ({anomaly_predictions.mean()*100:.2f}%)")
        
        # ì‹¤ì œ ë ˆì´ë¸”ì´ ìˆìœ¼ë©´ í‰ê°€ ìˆ˜í–‰
        if y_test is not None:
            print(f"\nğŸ“Š ì„±ëŠ¥ í‰ê°€:")
            
            # ë¶„ë¥˜ ë³´ê³ ì„œ
            print("   ë¶„ë¥˜ ë³´ê³ ì„œ:")
            report = classification_report(y_test, anomaly_predictions, 
                                         target_names=['ì •ìƒ', 'ë¶ˆëŸ‰'], 
                                         digits=4)
            print(report)
            
            # í˜¼ë™ í–‰ë ¬
            cm = confusion_matrix(y_test, anomaly_predictions)
            print(f"   í˜¼ë™ í–‰ë ¬:")
            print(f"             ì˜ˆì¸¡")
            print(f"        ì •ìƒ    ë¶ˆëŸ‰")
            print(f"ì‹¤ì œ ì •ìƒ {cm[0,0]:5d}  {cm[0,1]:5d}")
            print(f"    ë¶ˆëŸ‰ {cm[1,0]:5d}  {cm[1,1]:5d}")
            
            # AUC ê³„ì‚°
            if len(np.unique(y_test)) > 1:
                auc = roc_auc_score(y_test, test_errors)
                print(f"   AUC (ì¬êµ¬ì„± ì˜¤ì°¨ ê¸°ì¤€): {auc:.4f}")
            
            # ë§¤íŠœìŠ¤ ìƒê´€ê³„ìˆ˜ (MCC)
            from sklearn.metrics import matthews_corrcoef
            mcc = matthews_corrcoef(y_test, anomaly_predictions)
            print(f"   Matthews ìƒê´€ê³„ìˆ˜: {mcc:.4f}")
            
        return anomaly_predictions, test_errors

def create_simple_example():
    """
    TensorFlowê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê°„ë‹¨í•œ ì˜ˆì œ
    """
    print("=" * 60)
    print("ğŸ“‹ ê°„ë‹¨í•œ ì´ìƒ íƒì§€ ì˜ˆì œ (í†µê³„ì  ë°©ë²•)")
    print("=" * 60)
    
    data_path = "C:/Users/ASUS/bosch/data/train_numeric.csv"
    
    # ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv(data_path, nrows=10000)
    
    # ê°„ë‹¨í•œ íŠ¹ì§• ìƒì„±
    feature_cols = [col for col in df.columns if col not in ['Id', 'Response']]
    X = df[feature_cols].fillna(0)
    y = df['Response']
    
    # ê¸°ë³¸ í†µê³„ íŠ¹ì§•
    X_simple = pd.DataFrame({
        'mean': X.mean(axis=1),
        'std': X.std(axis=1),
        'min': X.min(axis=1),
        'max': X.max(axis=1),
        'non_zero_count': (X != 0).sum(axis=1),
        'zero_ratio': (X == 0).sum(axis=1) / len(feature_cols)
    })
    
    # ì •ìƒ/ë¶ˆëŸ‰ ë¶„ë¦¬
    X_normal = X_simple[y == 0]
    X_fault = X_simple[y == 1]
    
    print(f"ì •ìƒ ë°ì´í„°: {len(X_normal)}ê°œ")
    print(f"ë¶ˆëŸ‰ ë°ì´í„°: {len(X_fault)}ê°œ")
    
    # ê°„ë‹¨í•œ ì´ìƒ íƒì§€ (Isolation Forest ëŒ€ì‹  í†µê³„ì  ë°©ë²•)
    # ê° íŠ¹ì§•ì˜ ì •ìƒ ë²”ìœ„ ê³„ì‚° (í‰ê·  Â± 3*í‘œì¤€í¸ì°¨)
    print("\ní†µê³„ì  ì´ìƒ íƒì§€:")
    
    anomaly_scores = []
    for _, row in X_simple.iterrows():
        score = 0
        for col in X_simple.columns:
            normal_mean = X_normal[col].mean()
            normal_std = X_normal[col].std()
            
            # z-score ê³„ì‚°
            if normal_std > 0:
                z_score = abs((row[col] - normal_mean) / normal_std)
                if z_score > 3:  # 3-sigma ê·œì¹™
                    score += z_score
        
        anomaly_scores.append(score)
    
    # ì„ê³„ê°’ ê¸°ì¤€ ë¶„ë¥˜
    threshold = np.percentile(anomaly_scores, 95)
    predictions = np.array(anomaly_scores) > threshold
    
    print(f"ì„ê³„ê°’: {threshold:.4f}")
    print(f"ì´ìƒ ì˜ˆì¸¡: {predictions.sum()}ê°œ")
    
    # ê°„ë‹¨í•œ í‰ê°€
    from sklearn.metrics import classification_report
    print("\nì„±ëŠ¥ í‰ê°€:")
    print(classification_report(y, predictions.astype(int), 
                              target_names=['ì •ìƒ', 'ë¶ˆëŸ‰'], 
                              digits=4))

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ­ Bosch AutoEncoder ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹œì‘")
    
    try:
        # ë°ì´í„° ê²½ë¡œ
        data_path = "C:/Users/ASUS/bosch/data/train_numeric.csv"
        
        if TF_AVAILABLE:
            # TensorFlow ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            detector = BoschAutoEncoderFaultDetector(
                encoding_dim=64,
                learning_rate=0.001
            )
            
            # ë°ì´í„° ì¤€ë¹„
            X_normal, X_fault, X_all, y_all = detector.prepare_data(
                data_path, sample_size=100000
            )
            
            # AutoEncoder êµ¬ì¶•
            detector.build_autoencoder(X_normal.shape[1])
            
            # í•™ìŠµ (ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©)
            detector.train_autoencoder(
                X_normal, 
                validation_split=0.2,
                epochs=50,
                batch_size=512
            )
            
            # ì„ê³„ê°’ ê³„ì‚°
            threshold, normal_errors = detector.calculate_threshold(
                X_normal, percentile=95
            )
            
            # ì „ì²´ ë°ì´í„°ë¡œ ì´ìƒ íƒì§€
            predictions, test_errors = detector.detect_anomalies(X_all, y_all)
            
            print(f"\nğŸ‰ AutoEncoder ê¸°ë°˜ ì´ìƒ íƒì§€ ì™„ë£Œ!")
            
        else:
            # TensorFlowê°€ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ì˜ˆì œ ì‹¤í–‰
            create_simple_example()
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("\nê°„ë‹¨í•œ ì˜ˆì œë¡œ ëŒ€ì²´ ì‹¤í–‰:")
        create_simple_example()

if __name__ == "__main__":
    main()