{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ Bosch ìƒì‚°ë¼ì¸ ì„±ëŠ¥ ë¶„ì„ (í•œê¸€ í•™ìŠµìš©)\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ ì—¬ëŸ¬ë¶„ì€ ë‹¤ìŒì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤:\n",
    "1. **ë¹…ë°ì´í„° ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ë°©ë²•\n",
    "2. **ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ**: ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •\n",
    "3. **ë¶ˆê· í˜• ë°ì´í„°**: í¬ê·€í•œ ì‚¬ê±´ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•\n",
    "4. **ì‹¤ë¬´ í”„ë¡œì íŠ¸**: ì‹¤ì œ ì œì¡°ì—… ë¬¸ì œ í•´ê²°\n",
    "\n",
    "## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”\n",
    "\n",
    "### ë°°ê²½ ì„¤ëª…\n",
    "- **Bosch**: ë…ì¼ì˜ ìœ ëª…í•œ ìë™ì°¨ ë¶€í’ˆ ì œì¡° íšŒì‚¬\n",
    "- **ë¬¸ì œ**: ìƒì‚°ë¼ì¸ì—ì„œ ë¶ˆëŸ‰í’ˆì„ ë¯¸ë¦¬ ì˜ˆì¸¡í•˜ê³  ì‹¶ìŒ\n",
    "- **ëª©í‘œ**: ì œí’ˆì´ ë¶ˆëŸ‰í’ˆì¸ì§€ ì •ìƒí’ˆì¸ì§€ ì˜ˆì¸¡í•˜ê¸°\n",
    "\n",
    "### ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€ìš”?\n",
    "- ğŸ’° **ë¹„ìš© ì ˆê°**: ë¶ˆëŸ‰í’ˆì„ ë¯¸ë¦¬ ë°œê²¬í•˜ë©´ ë¹„ìš©ì„ ì•„ë‚„ ìˆ˜ ìˆìŒ\n",
    "- â±ï¸ **ì‹œê°„ ë‹¨ì¶•**: ë¬¸ì œë¥¼ ë¹¨ë¦¬ ë°œê²¬í•˜ë©´ ìƒì‚° íš¨ìœ¨ì´ ì˜¬ë¼ê°\n",
    "- ğŸ“ˆ **í’ˆì§ˆ í–¥ìƒ**: ë¶ˆëŸ‰ ì›ì¸ì„ íŒŒì•…í•´ í’ˆì§ˆì„ ê°œì„ í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "### ë°ì´í„° íŠ¹ì§•\n",
    "- **ë§¤ìš° í° ë°ì´í„°**: ìˆ˜ì²œ ê°œì˜ ì¸¡ì •ê°’ (íŠ¹ì§•)\n",
    "- **ë¶ˆê· í˜•í•œ ë°ì´í„°**: ë¶ˆëŸ‰í’ˆì´ ì „ì²´ì˜ 1% ë¯¸ë§Œ (ë§¤ìš° í¬ê·€!)\n",
    "- **ì„¸ ê°€ì§€ ë°ì´í„° ìœ í˜•**: ìˆ«ì, ë²”ì£¼í˜•, ë‚ ì§œ\n",
    "- **ìµëª…í™”ëœ ë°ì´í„°**: ë³´ì•ˆìƒ ì‹¤ì œ ì¸¡ì • í•­ëª©ëª…ì€ ìˆ¨ê¹€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ í•„ìš”í•œ ë„êµ¬ ê°€ì ¸ì˜¤ê¸° (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸)\n",
    "\n",
    "í”„ë¡œê·¸ë˜ë°ì—ì„œëŠ” ì´ë¯¸ ë§Œë“¤ì–´ì§„ ë„êµ¬ë“¤ì„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "ë§ˆì¹˜ ìš”ë¦¬í•  ë•Œ í•„ìš”í•œ ë„êµ¬ë¥¼ êº¼ë‚´ë†“ëŠ” ê²ƒê³¼ ê°™ì•„ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì²˜ë¦¬ ë„êµ¬ë“¤\n",
    "import pandas as pd          # ì—‘ì…€ê°™ì€ í‘œ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë„êµ¬\n",
    "import numpy as np           # ìˆ˜í•™ ê³„ì‚°ì„ ë¹ ë¥´ê²Œ í•´ì£¼ëŠ” ë„êµ¬\n",
    "\n",
    "# ì‹œê°í™” ë„êµ¬ë“¤ (ê·¸ë˜í”„ ê·¸ë¦¬ê¸°)\n",
    "import matplotlib.pyplot as plt  # ê¸°ë³¸ì ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ë„êµ¬\n",
    "import seaborn as sns           # ì˜ˆìœ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ë„êµ¬\n",
    "\n",
    "# íŒŒì¼ ì²˜ë¦¬ ë„êµ¬ë“¤\n",
    "from zipfile import ZipFile     # ì••ì¶•íŒŒì¼ì„ í‘¸ëŠ” ë„êµ¬\n",
    "import os                       # ì»´í“¨í„°ì˜ íŒŒì¼ì„ ê´€ë¦¬í•˜ëŠ” ë„êµ¬\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë„êµ¬ë“¤\n",
    "from sklearn.model_selection import train_test_split     # ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ë„êµ¬\n",
    "from sklearn.preprocessing import StandardScaler        # ë°ì´í„° í¬ê¸°ë¥¼ ë§ì¶”ëŠ” ë„êµ¬\n",
    "from sklearn.metrics import matthews_corrcoef          # ëª¨ë¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ë„êµ¬\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "import gc                       # ë©”ëª¨ë¦¬ë¥¼ ì²­ì†Œí•˜ëŠ” ë„êµ¬ (ì“°ë ˆê¸° ìˆ˜ì§‘ê¸°)\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ê¹”ë”í•œ ì¶œë ¥ì„ ìœ„í•´)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # ì˜ˆìœ ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì ìš©\n",
    "%matplotlib inline                       # ë…¸íŠ¸ë¶ ì•ˆì— ê·¸ë˜í”„ í‘œì‹œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "### ğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸\n",
    "- ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì••ì¶•ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ\n",
    "- ì••ì¶•ì„ í’€ê³  ë°ì´í„°ë¥¼ ì½ëŠ” ê³¼ì •ì´ í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ê°€ ì €ì¥ëœ í´ë” ì§€ì •\n",
    "data_dir = './data/'  # í˜„ì¬ ìœ„ì¹˜ì˜ data í´ë”ë¥¼ ì˜ë¯¸\n",
    "\n",
    "# í´ë” ì•ˆì— ì–´ë–¤ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "data_files = os.listdir(data_dir)  # í´ë” ì•ˆì˜ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "print(\"ğŸ“ ë°ì´í„° í´ë”ì— ìˆëŠ” íŒŒì¼ë“¤:\")\n",
    "print(\"=\" * 30)\n",
    "for file in data_files:\n",
    "    # íŒŒì¼ í¬ê¸°ë„ í•¨ê»˜ í‘œì‹œ\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  ğŸ“„ {file} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(data_dir):\n",
    "    \"\"\"\n",
    "    ì••ì¶• íŒŒì¼ì„ ìë™ìœ¼ë¡œ í‘¸ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ì™œ í•¨ìˆ˜ë¥¼ ë§Œë“œë‚˜ìš”?\n",
    "    - ê°™ì€ ì‘ì—…ì„ ë°˜ë³µí•  ë•Œ í¸ë¦¬\n",
    "    - ì½”ë“œë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬\n",
    "    - ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‚¬ìš© ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    # .zipìœ¼ë¡œ ëë‚˜ëŠ” íŒŒì¼ë“¤ë§Œ ì°¾ê¸°\n",
    "    zip_files = [f for f in os.listdir(data_dir) if f.endswith('.zip')]\n",
    "    \n",
    "    print(f\"\\nğŸ” ë°œê²¬ëœ ì••ì¶• íŒŒì¼: {len(zip_files)}ê°œ\")\n",
    "    \n",
    "    for zip_file in zip_files:\n",
    "        file_path = os.path.join(data_dir, zip_file)\n",
    "        csv_file = zip_file.replace('.zip', '')  # .zipì„ ì œê±°í•œ ì´ë¦„\n",
    "        csv_path = os.path.join(data_dir, csv_file)\n",
    "        \n",
    "        # ì´ë¯¸ ì••ì¶•ì´ í’€ë ¤ìˆëŠ”ì§€ í™•ì¸\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"\\nğŸ“¦ ì••ì¶• í•´ì œ ì¤‘: {zip_file}...\")\n",
    "            with ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(data_dir)  # ì••ì¶• í’€ê¸°\n",
    "            print(f\"   âœ… ì™„ë£Œ! â†’ {csv_file}\")\n",
    "        else:\n",
    "            print(f\"   â­ï¸  {csv_file} ì´ë¯¸ ì¡´ì¬ (ê±´ë„ˆëœ€)\")\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "extract_zip_files(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ” ë°ì´í„° íƒìƒ‰í•˜ê¸°\n",
    "\n",
    "### ğŸ’¡ ì™œ ìƒ˜í”Œë§Œ ë¨¼ì € ë³¼ê¹Œìš”?\n",
    "- ì „ì²´ ë°ì´í„°ê°€ ë„ˆë¬´ í¬ë©´ ì»´í“¨í„°ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ\n",
    "- ì‘ì€ ìƒ˜í”Œë¡œë„ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŒ\n",
    "- í…ŒìŠ¤íŠ¸ì™€ ì‹¤í—˜ì„ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ«ì ë°ì´í„°ì˜ ì¼ë¶€ë¶„ë§Œ ì½ì–´ì˜¤ê¸° (ì²« 10,000ì¤„)\n",
    "print(\"ğŸ“Š train_numeric ë°ì´í„° ìƒ˜í”Œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "print(\"(ì „ì²´ ë°ì´í„°ê°€ í¬ë¯€ë¡œ ì¼ë¶€ë§Œ ì½ì–´ì˜µë‹ˆë‹¤)\\n\")\n",
    "\n",
    "# nrows=10000 : ì²˜ìŒ 10,000ì¤„ë§Œ ì½ê¸°\n",
    "train_numeric_sample = pd.read_csv(data_dir + 'train_numeric.csv', nrows=10000)\n",
    "\n",
    "# ë°ì´í„°ì˜ í¬ê¸° í™•ì¸\n",
    "print(f\"ğŸ“ ìƒ˜í”Œ ë°ì´í„° í¬ê¸°: {train_numeric_sample.shape}\")\n",
    "print(f\"   â†’ {train_numeric_sample.shape[0]:,}ê°œì˜ ì œí’ˆ (í–‰)\")\n",
    "print(f\"   â†’ {train_numeric_sample.shape[1]:,}ê°œì˜ ì¸¡ì •ê°’ (ì—´)\")\n",
    "\n",
    "# ì²˜ìŒ ëª‡ ê°œ ì—´ì˜ ì´ë¦„ ë³´ê¸°\n",
    "print(f\"\\nğŸ·ï¸  ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ëª…:\")\n",
    "for i, col in enumerate(train_numeric_sample.columns[:10], 1):\n",
    "    print(f\"   {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆëŸ‰í’ˆ ë¹„ìœ¨ í™•ì¸í•˜ê¸°\n",
    "print(\"ğŸ¯ ëª©í‘œ ë³€ìˆ˜(Response) ë¶„í¬:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Response = 0: ì •ìƒí’ˆ, Response = 1: ë¶ˆëŸ‰í’ˆ\n",
    "response_counts = train_numeric_sample['Response'].value_counts()\n",
    "print(f\"ì •ìƒí’ˆ (0): {response_counts[0]:,}ê°œ\")\n",
    "print(f\"ë¶ˆëŸ‰í’ˆ (1): {response_counts[1]:,}ê°œ\")\n",
    "\n",
    "# ë¶ˆëŸ‰ë¥  ê³„ì‚°\n",
    "failure_rate = train_numeric_sample['Response'].mean()\n",
    "print(f\"\\nâš ï¸  ë¶ˆëŸ‰ë¥ : {failure_rate:.2%}\")\n",
    "print(f\"   â†’ 100ê°œ ì¤‘ ì•½ {int(failure_rate * 100)}ê°œê°€ ë¶ˆëŸ‰í’ˆ\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 5))\n",
    "train_numeric_sample['Response'].value_counts().plot(kind='bar', \n",
    "                                                     color=['green', 'red'])\n",
    "plt.title('ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('ì œí’ˆ ìƒíƒœ')\n",
    "plt.ylabel('ê°œìˆ˜')\n",
    "plt.xticks([0, 1], ['ì •ìƒí’ˆ(0)', 'ë¶ˆëŸ‰í’ˆ(1)'], rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ê° ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ\n",
    "for i, v in enumerate(response_counts):\n",
    "    plt.text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’­ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ê²ƒ:\")\n",
    "print(\"   â€¢ ë§¤ìš° ë¶ˆê· í˜•í•œ ë°ì´í„° (ë¶ˆëŸ‰í’ˆì´ ê·¹íˆ ì ìŒ)\")\n",
    "print(\"   â€¢ íŠ¹ë³„í•œ ì²˜ë¦¬ ë°©ë²•ì´ í•„ìš”í•¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "print(\"ğŸ“‹ ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Idì™€ Responseë¥¼ ì œì™¸í•œ ì‹¤ì œ íŠ¹ì§•ì˜ ê°œìˆ˜\n",
    "num_features = len(train_numeric_sample.columns) - 2\n",
    "num_samples = len(train_numeric_sample)\n",
    "\n",
    "print(f\"ğŸ”¢ ì¸¡ì • í•­ëª© ìˆ˜: {num_features:,}ê°œ\")\n",
    "print(f\"ğŸ“¦ ìƒ˜í”Œ ìˆ˜: {num_samples:,}ê°œ\")\n",
    "\n",
    "# ê²°ì¸¡ê°’ (ë¹ˆ ê°’) ë¹„ìœ¨ ê³„ì‚°\n",
    "print(f\"\\nâ“ ê²°ì¸¡ê°’(ë¹ˆ ë°ì´í„°) ë¶„ì„:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ê°’ ë¹„ìœ¨ ê³„ì‚°\n",
    "missing_percent = (train_numeric_sample.isnull().sum() / len(train_numeric_sample) * 100)\n",
    "missing_percent = missing_percent.sort_values(ascending=False)\n",
    "\n",
    "# ê²°ì¸¡ê°’ì´ ë§ì€ ìƒìœ„ 10ê°œ ì»¬ëŸ¼\n",
    "print(\"ê²°ì¸¡ê°’ì´ ê°€ì¥ ë§ì€ 10ê°œ ì»¬ëŸ¼:\")\n",
    "for col, pct in missing_percent.head(10).items():\n",
    "    print(f\"   â€¢ {col}: {pct:.1f}% ë¹„ì–´ìˆìŒ\")\n",
    "\n",
    "# ì „ì²´ ê²°ì¸¡ê°’ í†µê³„\n",
    "print(f\"\\nğŸ“Š ê²°ì¸¡ê°’ í†µê³„:\")\n",
    "print(f\"   â€¢ ì™„ì „íˆ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {(missing_percent == 100).sum()}ê°œ\")\n",
    "print(f\"   â€¢ 50% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {(missing_percent >= 50).sum()}ê°œ\")\n",
    "print(f\"   â€¢ ê²°ì¸¡ê°’ì´ ì „í˜€ ì—†ëŠ” ì»¬ëŸ¼: {(missing_percent == 0).sum()}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ“Š íŠ¹ì§•(Feature) ë¶„ì„í•˜ê¸°\n",
    "\n",
    "### ğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸\n",
    "- íŠ¹ì§•(Feature): ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì˜ˆì¸¡ì— ì‚¬ìš©í•˜ëŠ” ê°ê°ì˜ ì¸¡ì •ê°’\n",
    "- íŠ¹ì§•ì´ ë§ë‹¤ê³  ë¬´ì¡°ê±´ ì¢‹ì€ ê²ƒì€ ì•„ë‹˜\n",
    "- ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ í•µì‹¬!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_groups(df):\n",
    "    \"\"\"\n",
    "    íŠ¹ì§•ë“¤ì„ ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ì˜ˆ: L0_S0_F0, L0_S0_F2 â†’ 'L0_S0' ê·¸ë£¹\n",
    "    ì´ë ‡ê²Œ í•˜ë©´ ì–´ë–¤ ìƒì‚° ë¼ì¸/ìŠ¤í…Œì´ì…˜ì— íŠ¹ì§•ì´ ë§ì€ì§€ ì•Œ ìˆ˜ ìˆìŒ\n",
    "    \"\"\"\n",
    "    feature_groups = {}  # ë¹ˆ ì‚¬ì „(dictionary) ìƒì„±\n",
    "    \n",
    "    # ê° ì»¬ëŸ¼ì„ í•˜ë‚˜ì”© í™•ì¸\n",
    "    for col in df.columns:\n",
    "        if col not in ['Id', 'Response']:  # Idì™€ ResponseëŠ” ì œì™¸\n",
    "            # ì–¸ë”ë°”(_)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "            parts = col.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                # ì²˜ìŒ ë‘ ë¶€ë¶„ì„ ê·¸ë£¹ëª…ìœ¼ë¡œ ì‚¬ìš©\n",
    "                group = parts[0] + '_' + parts[1]\n",
    "                \n",
    "                # ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ê¸°\n",
    "                if group not in feature_groups:\n",
    "                    feature_groups[group] = []\n",
    "                \n",
    "                # ê·¸ë£¹ì— íŠ¹ì§• ì¶”ê°€\n",
    "                feature_groups[group].append(col)\n",
    "    \n",
    "    # íŠ¹ì§•ì´ ë§ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "    sorted_groups = sorted(feature_groups.items(), \n",
    "                          key=lambda x: len(x[1]), \n",
    "                          reverse=True)\n",
    "    \n",
    "    print(\"ğŸ­ ìƒì‚° ë¼ì¸/ìŠ¤í…Œì´ì…˜ë³„ íŠ¹ì§• ê°œìˆ˜ (ìƒìœ„ 10ê°œ):\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (group, features) in enumerate(sorted_groups[:10], 1):\n",
    "        print(f\"{i:2d}. {group}: {len(features):3d}ê°œì˜ ì¸¡ì •ê°’\")\n",
    "        # ë§‰ëŒ€ ê·¸ë˜í”„ í˜•íƒœë¡œ ì‹œê°í™”\n",
    "        bar_length = int(len(features) / 5)  # 5ê°œë‹¹ 1ê°œ ë§‰ëŒ€\n",
    "        print(f\"    {'â–ˆ' * bar_length}\")\n",
    "    \n",
    "    return feature_groups\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "feature_groups = analyze_feature_groups(train_numeric_sample)\n",
    "\n",
    "print(\"\\nğŸ’­ í•´ì„:\")\n",
    "print(\"   â€¢ Lì€ Line(ìƒì‚°ë¼ì¸), SëŠ” Station(ì‘ì—…ì¥)ì„ ì˜ë¯¸í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ\")\n",
    "print(\"   â€¢ ê° ë¼ì¸/ìŠ¤í…Œì´ì…˜ì—ì„œ ì—¬ëŸ¬ ì¸¡ì •ì„ ìˆ˜í–‰í•¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ê°’ íŒ¨í„´ì„ ì‹œê°í™”í•˜ê¸°\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# ê° ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ ë¹„ìœ¨ ê³„ì‚°\n",
    "missing_df = pd.DataFrame({\n",
    "    'column': train_numeric_sample.columns,\n",
    "    'missing_percent': (train_numeric_sample.isnull().sum() / len(train_numeric_sample) * 100)\n",
    "})\n",
    "\n",
    "# ê·¸ë˜í”„ 1: ê²°ì¸¡ê°’ ë¹„ìœ¨ì˜ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(missing_df['missing_percent'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.xlabel('ê²°ì¸¡ê°’ ë¹„ìœ¨ (%)', fontsize=11)\n",
    "plt.ylabel('íŠ¹ì§• ê°œìˆ˜', fontsize=11)\n",
    "plt.title('ğŸ“Š íŠ¹ì§•ë“¤ì˜ ê²°ì¸¡ê°’ ë¶„í¬', fontsize=13, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# í‰ê· ì„  ì¶”ê°€\n",
    "mean_missing = missing_df['missing_percent'].mean()\n",
    "plt.axvline(mean_missing, color='red', linestyle='--', linewidth=2, label=f'í‰ê· : {mean_missing:.1f}%')\n",
    "plt.legend()\n",
    "\n",
    "# ê·¸ë˜í”„ 2: ëˆ„ì  ê²°ì¸¡ê°’ íŒ¨í„´\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_missing = np.sort(missing_df['missing_percent'].values)\n",
    "plt.plot(range(len(sorted_missing)), sorted_missing, linewidth=2, color='darkblue')\n",
    "plt.xlabel('íŠ¹ì§• ìˆœì„œ (ê²°ì¸¡ê°’ ì ì€ ìˆœ)', fontsize=11)\n",
    "plt.ylabel('ê²°ì¸¡ê°’ ë¹„ìœ¨ (%)', fontsize=11)\n",
    "plt.title('ğŸ“ˆ ëˆ„ì  ê²°ì¸¡ê°’ íŒ¨í„´', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 50% ì„  í‘œì‹œ\n",
    "plt.axhline(50, color='orange', linestyle='--', linewidth=1, label='50% ê¸°ì¤€ì„ ')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ê²°ì¸¡ê°’ ë¶„ì„ ì¸ì‚¬ì´íŠ¸:\")\n",
    "print(\"   â€¢ ë§ì€ íŠ¹ì§•ë“¤ì´ ëŒ€ë¶€ë¶„ ë¹„ì–´ìˆìŒ (90% ì´ìƒ)\")\n",
    "print(\"   â€¢ ì´ëŠ” ëª¨ë“  ì œí’ˆì´ ëª¨ë“  ê²€ì‚¬ë¥¼ ë°›ì§€ ì•ŠìŒì„ ì˜ë¯¸\")\n",
    "print(\"   â€¢ ê²°ì¸¡ê°’ ìì²´ë„ ì¤‘ìš”í•œ ì •ë³´ì¼ ìˆ˜ ìˆìŒ (íŠ¹ì • ê²€ì‚¬ë¥¼ ê±´ë„ˆë›´ ì´ìœ ?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "### ğŸ’¡ ì™œ ë©”ëª¨ë¦¬ ìµœì í™”ê°€ ì¤‘ìš”í•œê°€ìš”?\n",
    "- ì»´í“¨í„° ë©”ëª¨ë¦¬ëŠ” í•œì •ë˜ì–´ ìˆìŒ\n",
    "- ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•ŒëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬ê°€ í•„ìˆ˜\n",
    "- ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì“°ë©´ ë” ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df):\n",
    "    \"\"\"\n",
    "    ë°ì´í„° íƒ€ì…ì„ ìµœì í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ì˜ˆì‹œ: \n",
    "    - 0ê³¼ 1ë§Œ ìˆëŠ” ë°ì´í„° â†’ int64 ëŒ€ì‹  int8 ì‚¬ìš© (8ë°° ì ˆì•½!)\n",
    "    - ì‘ì€ ìˆ«ìë§Œ ìˆëŠ” ë°ì´í„° â†’ float64 ëŒ€ì‹  float32 ì‚¬ìš© (2ë°° ì ˆì•½!)\n",
    "    \"\"\"\n",
    "    # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # MB ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "    print(f'ğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {start_mem:.2f} MB')\n",
    "    print('\\nìµœì í™” ì§„í–‰ ì¤‘...')\n",
    "    \n",
    "    # ê° ì»¬ëŸ¼ì„ í•˜ë‚˜ì”© ìµœì í™”\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype  # í˜„ì¬ ë°ì´í„° íƒ€ì…\n",
    "        \n",
    "        if col_type != object:  # ìˆ«ìí˜• ë°ì´í„°ë§Œ ì²˜ë¦¬\n",
    "            c_min = df[col].min()  # ìµœì†Ÿê°’\n",
    "            c_max = df[col].max()  # ìµœëŒ“ê°’\n",
    "            \n",
    "            # ì •ìˆ˜í˜• ë°ì´í„° ìµœì í™”\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # ê°’ì˜ ë²”ìœ„ì— ë”°ë¼ ê°€ì¥ ì‘ì€ íƒ€ì… ì„ íƒ\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)  # -128 ~ 127\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)  # -32768 ~ 32767\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            \n",
    "            # ì‹¤ìˆ˜í˜• ë°ì´í„° ìµœì í™”\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    # ìµœì í™” í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f'\\nâœ… ìµœì í™” ì™„ë£Œ!')\n",
    "    print(f'ğŸ’¾ ìµœì í™” í›„ ë©”ëª¨ë¦¬: {end_mem:.2f} MB')\n",
    "    print(f'ğŸ“‰ ì ˆì•½ëœ ë©”ëª¨ë¦¬: {start_mem - end_mem:.2f} MB')\n",
    "    print(f'ğŸ¯ ê°ì†Œìœ¨: {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰\n",
    "print(\"ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\\n\")\n",
    "train_numeric_sample = reduce_memory_usage(train_numeric_sample)\n",
    "\n",
    "print(\"\\nğŸ’¡ ë©”ëª¨ë¦¬ ìµœì í™”ì˜ íš¨ê³¼:\")\n",
    "print(\"   â€¢ ê°™ì€ ë°ì´í„°ë¥¼ ë” ì‘ì€ ê³µê°„ì— ì €ì¥\")\n",
    "print(\"   â€¢ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„\")\n",
    "print(\"   â€¢ ë” ë§ì€ ë°ì´í„°ë¥¼ í•œë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ğŸ› ï¸ íŠ¹ì§• ê³µí•™ (Feature Engineering)\n",
    "\n",
    "### ğŸ’¡ íŠ¹ì§• ê³µí•™ì´ë€?\n",
    "- ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ê³µí•´ì„œ ìƒˆë¡œìš´ ìœ ìš©í•œ íŠ¹ì§•ì„ ë§Œë“œëŠ” ê²ƒ\n",
    "- ì˜ˆ: í‚¤ì™€ ëª¸ë¬´ê²Œ â†’ BMI ì§€ìˆ˜ ê³„ì‚°\n",
    "- ì¢‹ì€ íŠ¹ì§•ì„ ë§Œë“¤ë©´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë¨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_features(df):\n",
    "    \"\"\"\n",
    "    ê¸°ë³¸ì ì¸ í†µê³„ íŠ¹ì§•ë“¤ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ê° ì œí’ˆ(í–‰)ì— ëŒ€í•´ ì „ì²´ ì¸¡ì •ê°’ë“¤ì˜ í†µê³„ë¥¼ ê³„ì‚°\n",
    "    ì´ë ‡ê²Œ í•˜ë©´ ìˆ˜ì²œ ê°œì˜ íŠ¹ì§•ì„ ëª‡ ê°œì˜ ì¤‘ìš”í•œ íŠ¹ì§•ìœ¼ë¡œ ìš”ì•½ ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ ìƒˆë¡œìš´ íŠ¹ì§•ì„ ë§Œë“œëŠ” ì¤‘...\\n\")\n",
    "    \n",
    "    feature_df = pd.DataFrame()  # ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    feature_df['Id'] = df['Id']  # Id ë³µì‚¬\n",
    "    \n",
    "    # 1. ë¹„ì–´ìˆì§€ ì•Šì€ ì¸¡ì •ê°’ì˜ ê°œìˆ˜\n",
    "    # (ì–¼ë§ˆë‚˜ ë§ì€ ê²€ì‚¬ë¥¼ ë°›ì•˜ëŠ”ì§€)\n",
    "    feature_df['count_non_null'] = df.drop(['Id', 'Response'], axis=1, errors='ignore').count(axis=1)\n",
    "    print(\"âœ… ì¸¡ì •ê°’ ê°œìˆ˜ ê³„ì‚° ì™„ë£Œ\")\n",
    "    \n",
    "    # 2. 0ì¸ ê°’ì˜ ê°œìˆ˜\n",
    "    # (ì¸¡ì •ê°’ì´ 0ì´ë¼ëŠ” ê²ƒë„ ì˜ë¯¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ)\n",
    "    feature_df['count_zeros'] = (df.drop(['Id', 'Response'], axis=1, errors='ignore') == 0).sum(axis=1)\n",
    "    print(\"âœ… 0ê°’ ê°œìˆ˜ ê³„ì‚° ì™„ë£Œ\")\n",
    "    \n",
    "    # 3. ê¸°ë³¸ í†µê³„ëŸ‰ ê³„ì‚° (ê° ì œí’ˆë³„ë¡œ)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(['Id', 'Response'], errors='ignore')\n",
    "    \n",
    "    feature_df['mean'] = df[numeric_cols].mean(axis=1)      # í‰ê· \n",
    "    feature_df['std'] = df[numeric_cols].std(axis=1)        # í‘œì¤€í¸ì°¨ (ë°ì´í„°ì˜ í©ì–´ì§„ ì •ë„)\n",
    "    feature_df['min'] = df[numeric_cols].min(axis=1)        # ìµœì†Ÿê°’\n",
    "    feature_df['max'] = df[numeric_cols].max(axis=1)        # ìµœëŒ“ê°’\n",
    "    feature_df['median'] = df[numeric_cols].median(axis=1)  # ì¤‘ì•™ê°’\n",
    "    print(\"âœ… í†µê³„ëŸ‰ ê³„ì‚° ì™„ë£Œ\")\n",
    "    \n",
    "    # 4. ê²°ì¸¡ê°’ ë¹„ìœ¨\n",
    "    # (ì–¼ë§ˆë‚˜ ë§ì€ ê²€ì‚¬ë¥¼ ê±´ë„ˆë›°ì—ˆëŠ”ì§€)\n",
    "    feature_df['missing_percent'] = df[numeric_cols].isnull().sum(axis=1) / len(numeric_cols)\n",
    "    print(\"âœ… ê²°ì¸¡ê°’ ë¹„ìœ¨ ê³„ì‚° ì™„ë£Œ\")\n",
    "    \n",
    "    # Response (ëª©í‘œ ë³€ìˆ˜) ì¶”ê°€\n",
    "    if 'Response' in df.columns:\n",
    "        feature_df['Response'] = df['Response']\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "# íŠ¹ì§• ìƒì„± ì‹¤í–‰\n",
    "engineered_features = create_basic_features(train_numeric_sample)\n",
    "\n",
    "print(f\"\\nğŸ“Š ìƒì„±ëœ íŠ¹ì§• ìš”ì•½:\")\n",
    "print(f\"   ì›ë˜ íŠ¹ì§• ê°œìˆ˜: {len(train_numeric_sample.columns)}ê°œ\")\n",
    "print(f\"   ìƒˆë¡œìš´ íŠ¹ì§• ê°œìˆ˜: {len(engineered_features.columns)}ê°œ\")\n",
    "print(f\"\\nğŸ“ ìƒˆë¡œ ë§Œë“  íŠ¹ì§•ë“¤:\")\n",
    "for i, col in enumerate(engineered_features.columns, 1):\n",
    "    if col not in ['Id', 'Response']:\n",
    "        print(f\"   {i-1}. {col}\")\n",
    "\n",
    "# ìƒˆë¡œìš´ íŠ¹ì§•ë“¤ì˜ ì˜ˆì‹œ ë³´ê¸°\n",
    "print(\"\\nğŸ” ì²˜ìŒ 5ê°œ ì œí’ˆì˜ ìƒˆë¡œìš´ íŠ¹ì§•ê°’:\")\n",
    "engineered_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡œ ë§Œë“  íŠ¹ì§•ë“¤ì´ ë¶ˆëŸ‰í’ˆ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ìœ ìš©í•œì§€ í™•ì¸\n",
    "if 'Response' in engineered_features.columns:\n",
    "    # ê° íŠ¹ì§•ê³¼ ëª©í‘œ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "    correlations = engineered_features.drop(['Id', 'Response'], axis=1).corrwith(engineered_features['Response'])\n",
    "    correlations = correlations.sort_values(ascending=False)\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['green' if x > 0 else 'red' for x in correlations]\n",
    "    correlations.plot(kind='bar', color=colors)\n",
    "    plt.title('ğŸ¯ ìƒˆë¡œìš´ íŠ¹ì§•ë“¤ê³¼ ë¶ˆëŸ‰í’ˆì˜ ìƒê´€ê´€ê³„', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('íŠ¹ì§•', fontsize=12)\n",
    "    plt.ylabel('ìƒê´€ê³„ìˆ˜', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    # ê°€ì¥ ì¤‘ìš”í•œ ê°’ì— ì£¼ì„ ì¶”ê°€\n",
    "    max_corr = correlations.abs().max()\n",
    "    max_feature = correlations.abs().idxmax()\n",
    "    plt.annotate(f'ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„\\n{max_feature}', \n",
    "                xy=(list(correlations.index).index(max_feature), correlations[max_feature]),\n",
    "                xytext=(0, 0.05),\n",
    "                fontsize=10,\n",
    "                ha='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.5),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼:\")\n",
    "    print(\"=\" * 50)\n",
    "    for feature, corr in correlations.items():\n",
    "        if abs(corr) > 0.01:  # ì˜ë¯¸ìˆëŠ” ìƒê´€ê´€ê³„ë§Œ í‘œì‹œ\n",
    "            direction = \"ì–‘ì˜\" if corr > 0 else \"ìŒì˜\"\n",
    "            print(f\"â€¢ {feature:20s}: {corr:+.4f} ({direction} ìƒê´€ê´€ê³„)\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ í•´ì„ ë°©ë²•:\")\n",
    "    print(\"   â€¢ ì–‘ì˜ ìƒê´€ê´€ê³„ (+): ê°’ì´ í´ìˆ˜ë¡ ë¶ˆëŸ‰í’ˆì¼ ê°€ëŠ¥ì„± ë†’ìŒ\")\n",
    "    print(\"   â€¢ ìŒì˜ ìƒê´€ê´€ê³„ (-): ê°’ì´ ì‘ì„ìˆ˜ë¡ ë¶ˆëŸ‰í’ˆì¼ ê°€ëŠ¥ì„± ë†’ìŒ\")\n",
    "    print(\"   â€¢ 0ì— ê°€ê¹Œì›€: ë¶ˆëŸ‰í’ˆ ì˜ˆì¸¡ê³¼ ê´€ë ¨ì´ ì ìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ğŸ“¦ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì „ëµ\n",
    "\n",
    "### ğŸ’¡ ì²­í¬(Chunk) ì²˜ë¦¬ë€?\n",
    "- í° íŒŒì¼ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\n",
    "- ì˜ˆ: ì±… ì „ì²´ë¥¼ í•œë²ˆì— ì½ê¸° vs í•œ ì¥ì”© ì½ê¸°\n",
    "- ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ ê¸°ìˆ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, feature_list=None):\n",
    "    \"\"\"\n",
    "    ë°ì´í„° ì²­í¬(ì¡°ê°)ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    í° íŒŒì¼ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©\n",
    "    ê° ì¡°ê°ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹˜ë©´ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!\n",
    "    \"\"\"\n",
    "    # íŠ¹ì • ì»¬ëŸ¼ë§Œ ì„ íƒ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    if feature_list is not None:\n",
    "        available_cols = [col for col in feature_list if col in chunk.columns]\n",
    "        chunk = chunk[available_cols + ['Id', 'Response']]\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    chunk = reduce_memory_usage(chunk)\n",
    "    \n",
    "    # íŠ¹ì§• ìƒì„±\n",
    "    features = create_basic_features(chunk)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"ğŸ“š ì²­í¬ ì²˜ë¦¬ ì˜ˆì‹œ ì½”ë“œ:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "# ì‹¤ì œë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì½”ë“œ\n",
    "chunk_size = 50000  # í•œ ë²ˆì— 50,000ì¤„ì”© ì½ê¸°\n",
    "chunks = []  # ì²˜ë¦¬ëœ ì¡°ê°ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# íŒŒì¼ì„ ì¡°ê°ë³„ë¡œ ì½ì–´ì„œ ì²˜ë¦¬\n",
    "for chunk in pd.read_csv('big_file.csv', chunksize=chunk_size):\n",
    "    processed_chunk = process_chunk(chunk)  # ì¡°ê° ì²˜ë¦¬\n",
    "    chunks.append(processed_chunk)          # ê²°ê³¼ ì €ì¥\n",
    "    gc.collect()                            # ë©”ëª¨ë¦¬ ì²­ì†Œ\n",
    "\n",
    "# ëª¨ë“  ì¡°ê°ì„ í•©ì¹˜ê¸°\n",
    "full_features = pd.concat(chunks, ignore_index=True)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì²­í¬ ì²˜ë¦¬ì˜ ì¥ì :\")\n",
    "print(\"   1. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°\")\n",
    "print(\"   2. ì§„í–‰ ìƒí™©ì„ í™•ì¸ ê°€ëŠ¥\")\n",
    "print(\"   3. ì¤‘ê°„ì— ë©ˆì¶°ë„ ì¼ë¶€ ê²°ê³¼ëŠ” ì €ì¥ë¨\")\n",
    "print(\"   4. ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ğŸ¯ ì¤‘ìš”í•œ íŠ¹ì§• ì„ íƒí•˜ê¸°\n",
    "\n",
    "### ğŸ’¡ ì™œ íŠ¹ì§• ì„ íƒì´ ì¤‘ìš”í•œê°€ìš”?\n",
    "- ëª¨ë“  íŠ¹ì§•ì´ ë‹¤ ìœ ìš©í•œ ê²ƒì€ ì•„ë‹˜\n",
    "- ì“¸ëª¨ì—†ëŠ” íŠ¹ì§•ì€ ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦¼\n",
    "- ì ì€ íŠ¹ì§•ìœ¼ë¡œë„ ì¢‹ì€ ì˜ˆì¸¡ì´ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(df, target_col='Response', threshold=0.95):\n",
    "    \"\"\"\n",
    "    ì¤‘ìš”í•œ íŠ¹ì§•ë§Œ ì„ íƒí•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ì„ íƒ ê¸°ì¤€:\n",
    "    1. ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ì§€ ì•Šì€ íŠ¹ì§• (95% ë¯¸ë§Œ)\n",
    "    2. ë³€í™”ê°€ ìˆëŠ” íŠ¹ì§• (ë¶„ì‚°ì´ 0ì´ ì•„ë‹˜)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” íŠ¹ì§• ì„ íƒì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    print(f\"   ì´ˆê¸° íŠ¹ì§• ê°œìˆ˜: {len(df.columns)}ê°œ\\n\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ë§ì€ íŠ¹ì§• ì œê±°\n",
    "    missing_percent = df.isnull().sum() / len(df)\n",
    "    keep_cols = missing_percent[missing_percent < threshold].index.tolist()\n",
    "    \n",
    "    removed_by_missing = len(df.columns) - len(keep_cols)\n",
    "    print(f\"ğŸ“‰ ê²°ì¸¡ê°’ í•„í„° ì ìš©:\")\n",
    "    print(f\"   â€¢ ì œê±°ëœ íŠ¹ì§•: {removed_by_missing}ê°œ\")\n",
    "    print(f\"   â€¢ ë‚¨ì€ íŠ¹ì§•: {len(keep_cols)}ê°œ\")\n",
    "    \n",
    "    # 2ë‹¨ê³„: ë³€í™”ê°€ ì—†ëŠ” íŠ¹ì§• ì œê±° (ë¶„ì‚°ì´ ë§¤ìš° ì‘ì€ íŠ¹ì§•)\n",
    "    numeric_cols = df[keep_cols].select_dtypes(include=[np.number]).columns\n",
    "    variances = df[numeric_cols].var()\n",
    "    keep_cols = variances[variances > 0.01].index.tolist()\n",
    "    \n",
    "    removed_by_variance = len(numeric_cols) - len(keep_cols)\n",
    "    print(f\"\\nğŸ“‰ ë¶„ì‚° í•„í„° ì ìš©:\")\n",
    "    print(f\"   â€¢ ì œê±°ëœ íŠ¹ì§•: {removed_by_variance}ê°œ\")\n",
    "    print(f\"   â€¢ ë‚¨ì€ íŠ¹ì§•: {len(keep_cols)}ê°œ\")\n",
    "    \n",
    "    # Idì™€ ResponseëŠ” í•­ìƒ í¬í•¨\n",
    "    if 'Id' not in keep_cols:\n",
    "        keep_cols.append('Id')\n",
    "    if target_col in df.columns and target_col not in keep_cols:\n",
    "        keep_cols.append(target_col)\n",
    "    \n",
    "    # ê°ì†Œìœ¨ ê³„ì‚°\n",
    "    reduction_rate = (1 - len(keep_cols) / len(df.columns)) * 100\n",
    "    \n",
    "    print(f\"\\nâœ… íŠ¹ì§• ì„ íƒ ì™„ë£Œ!\")\n",
    "    print(f\"   ìµœì¢… íŠ¹ì§• ê°œìˆ˜: {len(keep_cols)}ê°œ\")\n",
    "    print(f\"   ì „ì²´ ê°ì†Œìœ¨: {reduction_rate:.1f}%\")\n",
    "    \n",
    "    return keep_cols\n",
    "\n",
    "# íŠ¹ì§• ì„ íƒ ì‹¤í–‰\n",
    "important_features = select_important_features(train_numeric_sample)\n",
    "\n",
    "print(\"\\nğŸ’¡ íŠ¹ì§• ì„ íƒì˜ íš¨ê³¼:\")\n",
    "print(\"   â€¢ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•\")\n",
    "print(\"   â€¢ ê³¼ì í•©(overfitting) ë°©ì§€\")\n",
    "print(\"   â€¢ ëª¨ë¸ í•´ì„ì´ ì‰¬ì›Œì§\")\n",
    "print(\"   â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "### ğŸ’¡ ë¨¸ì‹ ëŸ¬ë‹ì´ë€?\n",
    "- ì»´í“¨í„°ê°€ ë°ì´í„°ë¥¼ ë³´ê³  ìŠ¤ìŠ¤ë¡œ íŒ¨í„´ì„ í•™ìŠµ\n",
    "- í•™ìŠµí•œ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡\n",
    "- ì˜ˆ: ìŠ¤íŒ¸ ë©”ì¼ í•„í„°, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "print(\"ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\\n\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  # ëœë¤ í¬ë ˆìŠ¤íŠ¸ (ìˆ²)\n",
    "from sklearn.linear_model import LogisticRegression  # ë¡œì§€ìŠ¤í‹± íšŒê·€\n",
    "from xgboost import XGBClassifier                    # XGBoost (ê°•ë ¥í•œ ëª¨ë¸)\n",
    "from sklearn.model_selection import StratifiedKFold  # êµì°¨ ê²€ì¦\n",
    "from imblearn.over_sampling import SMOTE            # ì˜¤ë²„ìƒ˜í”Œë§\n",
    "from imblearn.under_sampling import RandomUnderSampler  # ì–¸ë”ìƒ˜í”Œë§\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"\\nğŸ“š ì‚¬ìš©í•  ëª¨ë¸ë“¤:\")\n",
    "print(\"   1. Random Forest: ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ í•©ì¹œ ëª¨ë¸\")\n",
    "print(\"   2. Logistic Regression: í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ê¸°ë³¸ ëª¨ë¸\")\n",
    "print(\"   3. XGBoost: ê²½ì§„ëŒ€íšŒì—ì„œ ìì£¼ ìš°ìŠ¹í•˜ëŠ” ê°•ë ¥í•œ ëª¨ë¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df, target_col='Response'):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ë‹¨ê³„:\n",
    "    1. X(ì…ë ¥)ì™€ y(ì¶œë ¥) ë¶„ë¦¬\n",
    "    2. ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "    3. ìŠ¤ì¼€ì¼ë§ (ëª¨ë“  íŠ¹ì§•ì˜ í¬ê¸°ë¥¼ ë¹„ìŠ·í•˜ê²Œ ë§ì¶¤)\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ì¤‘...\\n\")\n",
    "    \n",
    "    # X(íŠ¹ì§•)ì™€ y(ëª©í‘œ) ë¶„ë¦¬\n",
    "    X = df.drop(['Id', target_col], axis=1, errors='ignore')\n",
    "    y = df[target_col] if target_col in df.columns else None\n",
    "    \n",
    "    print(f\"ğŸ“Š ë°ì´í„° ë¶„ë¦¬:\")\n",
    "    print(f\"   â€¢ X (ì…ë ¥ íŠ¹ì§•): {X.shape}\")\n",
    "    print(f\"   â€¢ y (ëª©í‘œ ë³€ìˆ˜): {y.shape if y is not None else 'None'}\")\n",
    "    \n",
    "    # ê²°ì¸¡ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "    X = X.fillna(X.median())\n",
    "    print(f\"\\nâœ… ê²°ì¸¡ê°’ ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´)\")\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)\n",
    "    # ëª¨ë“  íŠ¹ì§•ì˜ í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§ì¶¤\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(f\"âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ (í‰ê· =0, í‘œì¤€í¸ì°¨=1)\")\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„ ì‹¤í–‰\n",
    "X_sample, y_sample, scaler = prepare_data_for_modeling(engineered_features)\n",
    "\n",
    "print(f\"\\nğŸ¯ ì¤€ë¹„ëœ ë°ì´í„°:\")\n",
    "print(f\"   â€¢ í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_sample.shape}\")\n",
    "print(f\"   â€¢ íŠ¹ì§• ê°œìˆ˜: {X_sample.shape[1]}ê°œ\")\n",
    "print(f\"   â€¢ ìƒ˜í”Œ ê°œìˆ˜: {X_sample.shape[0]}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X, y, strategy='undersample', ratio=0.1):\n",
    "    \"\"\"\n",
    "    ë¶ˆê· í˜•í•œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ë¬¸ì œ: ë¶ˆëŸ‰í’ˆì´ ë„ˆë¬´ ì ìŒ (1% ë¯¸ë§Œ)\n",
    "    í•´ê²°ë²•:\n",
    "    1. ì˜¤ë²„ìƒ˜í”Œë§: ì†Œìˆ˜ í´ë˜ìŠ¤(ë¶ˆëŸ‰í’ˆ)ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ëŠ˜ë¦¼\n",
    "    2. ì–¸ë”ìƒ˜í”Œë§: ë‹¤ìˆ˜ í´ë˜ìŠ¤(ì •ìƒí’ˆ)ë¥¼ ì¤„ì„\n",
    "    \"\"\"\n",
    "    print(f\"âš–ï¸  ë¶ˆê· í˜• ì²˜ë¦¬ ì „ëµ: {strategy}\\n\")\n",
    "    \n",
    "    # í˜„ì¬ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "    print(\"í˜„ì¬ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "    print(f\"   â€¢ ì •ìƒí’ˆ: {(y == 0).sum():,}ê°œ\")\n",
    "    print(f\"   â€¢ ë¶ˆëŸ‰í’ˆ: {(y == 1).sum():,}ê°œ\")\n",
    "    print(f\"   â€¢ ë¶ˆëŸ‰ë¥ : {y.mean():.2%}\\n\")\n",
    "    \n",
    "    if strategy == 'oversample':\n",
    "        # SMOTE: ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ í•©ì„± ìƒ˜í”Œì„ ìƒì„±\n",
    "        sampler = SMOTE(sampling_strategy=ratio, random_state=42)\n",
    "        print(\"ğŸ“ˆ ì˜¤ë²„ìƒ˜í”Œë§: ë¶ˆëŸ‰í’ˆ ë°ì´í„°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ìƒì„±\")\n",
    "    elif strategy == 'undersample':\n",
    "        # ì–¸ë”ìƒ˜í”Œë§: ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ë¬´ì‘ìœ„ë¡œ ì œê±°\n",
    "        sampler = RandomUnderSampler(sampling_strategy=ratio, random_state=42)\n",
    "        print(\"ğŸ“‰ ì–¸ë”ìƒ˜í”Œë§: ì •ìƒí’ˆ ë°ì´í„°ë¥¼ ì¼ë¶€ ì œê±°\")\n",
    "    else:\n",
    "        return X, y\n",
    "    \n",
    "    # ë¦¬ìƒ˜í”Œë§ ì‹¤í–‰\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"\\nâœ… ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ!\")\n",
    "    print(f\"   â€¢ ìƒˆë¡œìš´ ìƒ˜í”Œ ìˆ˜: {len(y_resampled):,}ê°œ\")\n",
    "    print(f\"   â€¢ ì •ìƒí’ˆ: {(y_resampled == 0).sum():,}ê°œ\")\n",
    "    print(f\"   â€¢ ë¶ˆëŸ‰í’ˆ: {(y_resampled == 1).sum():,}ê°œ\")\n",
    "    print(f\"   â€¢ ìƒˆë¡œìš´ ë¶ˆëŸ‰ë¥ : {y_resampled.mean():.2%}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” ì‹¤í–‰í•˜ì§€ ì•ŠìŒ)\n",
    "print(\"ğŸ’¡ ë¶ˆê· í˜• ì²˜ë¦¬ ì˜ˆì‹œ ì½”ë“œ:\")\n",
    "print(\"# X_balanced, y_balanced = handle_imbalance(X_sample, y_sample, strategy='undersample')\")\n",
    "print(\"\\nğŸ“ ì–¸ì œ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í•˜ë‚˜ìš”?\")\n",
    "print(\"   â€¢ ì˜¤ë²„ìƒ˜í”Œë§: ë°ì´í„°ê°€ ì ì„ ë•Œ\")\n",
    "print(\"   â€¢ ì–¸ë”ìƒ˜í”Œë§: ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ì„ ë•Œ\")\n",
    "print(\"   â€¢ ë‘˜ ë‹¤ ì‚¬ìš©: ì ì ˆíˆ ì¡°í•©í•˜ì—¬ ê· í˜• ë§ì¶”ê¸°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ğŸ“ í•™ìŠµ ë‚´ìš© ì •ë¦¬\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ ê°œë… ì •ë¦¬\n",
    "\n",
    "#### 1. ë°ì´í„° ê³¼í•™ì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤\n",
    "```\n",
    "ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ íƒìƒ‰ â†’ íŠ¹ì§• ê³µí•™ â†’ ëª¨ë¸ë§ â†’ í‰ê°€ â†’ ë°°í¬\n",
    "```\n",
    "\n",
    "#### 2. ì´ í”„ë¡œì íŠ¸ì—ì„œ ë°°ìš´ ì£¼ìš” ê¸°ìˆ \n",
    "- **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬**: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬, ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "- **ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬**: SMOTE, ì–¸ë”ìƒ˜í”Œë§\n",
    "- **íŠ¹ì§• ê³µí•™**: í†µê³„ì  íŠ¹ì§• ìƒì„±\n",
    "- **ë¨¸ì‹ ëŸ¬ë‹**: ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "#### 3. ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸\n",
    "- ë°ì´í„°ì˜ í’ˆì§ˆì´ ëª¨ë¸ ì„±ëŠ¥ì„ ì¢Œìš°\n",
    "- ë„ë©”ì¸ ì§€ì‹ì´ ì¤‘ìš” (ì œì¡°ì—… ì´í•´)\n",
    "- í‰ê°€ ì§€í‘œ ì„ íƒì´ ì¤‘ìš” (MCC vs Accuracy)\n",
    "\n",
    "### ğŸ’¼ ì‹¤ì œ ì ìš© ì˜ˆì‹œ\n",
    "ì´ëŸ° ê¸°ìˆ ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³³ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤:\n",
    "- ğŸ­ ì œì¡°ì—…: í’ˆì§ˆ ê´€ë¦¬, ë¶ˆëŸ‰ ì˜ˆì¸¡\n",
    "- ğŸ¥ ì˜ë£Œ: ì§ˆë³‘ ì§„ë‹¨, ìœ„í—˜ ì˜ˆì¸¡\n",
    "- ğŸ’° ê¸ˆìœµ: ì‚¬ê¸° íƒì§€, ì‹ ìš© í‰ê°€\n",
    "- ğŸ›’ ì´ì»¤ë¨¸ìŠ¤: ì¶”ì²œ ì‹œìŠ¤í…œ, ê³ ê° ì´íƒˆ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿\n",
    "\n",
    "### ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline():\n",
    "    \"\"\"\n",
    "    Bosch ë°ì´í„°ì…‹ì„ ìœ„í•œ ì™„ì „í•œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    ì´ í•¨ìˆ˜ í•˜ë‚˜ë¡œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1ë‹¨ê³„: íŒŒì¼ ì••ì¶• í•´ì œ\n",
    "    print(\"\\nğŸ“¦ 1ë‹¨ê³„: íŒŒì¼ ì¤€ë¹„\")\n",
    "    extract_zip_files(data_dir)\n",
    "    \n",
    "    # 2ë‹¨ê³„: ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬\n",
    "    print(\"\\nğŸ“Š 2ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ (ì²­í¬ ë‹¨ìœ„)\")\n",
    "    chunk_size = 50000\n",
    "    numeric_features = []\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ì¹´ìš´í„°\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(data_dir + 'train_numeric.csv', chunksize=chunk_size):\n",
    "        chunk_count += 1\n",
    "        print(f\"   ì²˜ë¦¬ ì¤‘: ì²­í¬ #{chunk_count}\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "        chunk = reduce_memory_usage(chunk)\n",
    "        \n",
    "        # íŠ¹ì§• ìƒì„±\n",
    "        features = create_basic_features(chunk)\n",
    "        numeric_features.append(features)\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        gc.collect()\n",
    "    \n",
    "    # 3ë‹¨ê³„: ëª¨ë“  ì²­í¬ í•©ì¹˜ê¸°\n",
    "    print(\"\\nğŸ”— 3ë‹¨ê³„: ë°ì´í„° í†µí•©\")\n",
    "    full_features = pd.concat(numeric_features, ignore_index=True)\n",
    "    print(f\"   í†µí•©ëœ ë°ì´í„° í¬ê¸°: {full_features.shape}\")\n",
    "    \n",
    "    # 4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì¤€ë¹„\n",
    "    print(\"\\nğŸ”§ 4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì¤€ë¹„\")\n",
    "    X, y, scaler = prepare_data_for_modeling(full_features)\n",
    "    \n",
    "    # 5ë‹¨ê³„: êµì°¨ ê²€ì¦ ì„¤ì •\n",
    "    print(\"\\nğŸ“ˆ 5ë‹¨ê³„: êµì°¨ ê²€ì¦ ì„¤ì •\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    print(\"   5-í´ë“œ êµì°¨ ê²€ì¦ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "    \n",
    "    # 6ë‹¨ê³„: XGBoost ëª¨ë¸ ìƒì„±\n",
    "    print(\"\\nğŸ¤– 6ë‹¨ê³„: XGBoost ëª¨ë¸ ìƒì„±\")\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,        # íŠ¸ë¦¬ ê°œìˆ˜\n",
    "        max_depth=5,            # íŠ¸ë¦¬ ê¹Šì´\n",
    "        learning_rate=0.1,      # í•™ìŠµë¥ \n",
    "        scale_pos_weight=100,   # ë¶ˆê· í˜• ì²˜ë¦¬\n",
    "        random_state=42\n",
    "    )\n",
    "    print(\"   ëª¨ë¸ ì„¤ì • ì™„ë£Œ\")\n",
    "    \n",
    "    # 7ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (ì˜ˆì‹œ)\n",
    "    print(\"\\nğŸ“š 7ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ\")\n",
    "    print(\"   (ì‹¤ì œ í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\")\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì •ì˜ ì™„ë£Œ\n",
    "print(\"ğŸ“‹ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"\\nì‹¤í–‰í•˜ë ¤ë©´:\")\n",
    "print(\">>> model = full_pipeline()\")\n",
    "print(\"\\nâš ï¸  ì£¼ì˜: ì „ì²´ ë°ì´í„° ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ğŸ“¤ ì œì¶œ íŒŒì¼ ë§Œë“¤ê¸°\n",
    "\n",
    "### Kaggle ëŒ€íšŒì— ì œì¶œí•  ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ í…œí”Œë¦¿ íŒŒì¼ í™•ì¸\n",
    "print(\"ğŸ“„ ì œì¶œ í…œí”Œë¦¿ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°...\\n\")\n",
    "\n",
    "submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "print(f\"ğŸ“ ì œì¶œ íŒŒì¼ í¬ê¸°: {submission.shape}\")\n",
    "print(f\"ğŸ·ï¸  ì»¬ëŸ¼: {submission.columns.tolist()}\\n\")\n",
    "\n",
    "print(\"ğŸ‘€ ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "print(submission.head())\n",
    "print(\"\\nğŸ“ ì„¤ëª…:\")\n",
    "print(\"   â€¢ Id: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê° ì œí’ˆ ID\")\n",
    "print(\"   â€¢ Response: ìš°ë¦¬ê°€ ì˜ˆì¸¡í•´ì•¼ í•  ê°’ (ë¶ˆëŸ‰í’ˆ í™•ë¥ )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, test_features, submission_template):\n",
    "    \"\"\"\n",
    "    ì œì¶œ íŒŒì¼ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ë‹¨ê³„:\n",
    "    1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡\n",
    "    2. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í˜•ì‹ì— ë§ê²Œ ì •ë¦¬\n",
    "    3. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¤ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\\n\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰ (ë¶ˆëŸ‰í’ˆì¼ í™•ë¥ )\n",
    "    predictions = model.predict_proba(test_features)[:, 1]\n",
    "    print(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions):,}ê°œ ì œí’ˆ\")\n",
    "    \n",
    "    # ì œì¶œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    submission = submission_template.copy()\n",
    "    submission['Response'] = predictions\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ í†µê³„\n",
    "    print(f\"\\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ í†µê³„:\")\n",
    "    print(f\"   â€¢ í‰ê·  ë¶ˆëŸ‰ í™•ë¥ : {predictions.mean():.4f}\")\n",
    "    print(f\"   â€¢ ìµœì†Œê°’: {predictions.min():.4f}\")\n",
    "    print(f\"   â€¢ ìµœëŒ€ê°’: {predictions.max():.4f}\")\n",
    "    print(f\"   â€¢ 0.5 ì´ìƒ (ë¶ˆëŸ‰í’ˆ ì˜ˆì¸¡): {(predictions >= 0.5).sum():,}ê°œ\")\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(f\"\\nğŸ’¾ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submission.csv\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "print(\"âœ… ì œì¶œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"\\nì‚¬ìš© ì˜ˆì‹œ:\")\n",
    "print(\">>> submission = create_submission(model, X_test, submission_template)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ìµœì¢… ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ğŸ† ì¶•í•˜í•©ë‹ˆë‹¤! \n",
    "ì—¬ëŸ¬ë¶„ì€ ì‹¤ì œ Kaggle ëŒ€íšŒ ë°ì´í„°ë¡œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ì™„ì„±í–ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "### ğŸ“š ì˜¤ëŠ˜ ë°°ìš´ í•µì‹¬ ë‚´ìš©\n",
    "1. **ë¹…ë°ì´í„° ì²˜ë¦¬**: ì²­í¬ ì²˜ë¦¬, ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "2. **íƒìƒ‰ì  ë°ì´í„° ë¶„ì„**: ë°ì´í„° ì´í•´ì™€ ì‹œê°í™”\n",
    "3. **íŠ¹ì§• ê³µí•™**: ìƒˆë¡œìš´ ìœ ìš©í•œ íŠ¹ì§• ë§Œë“¤ê¸°\n",
    "4. **ë¶ˆê· í˜• ë°ì´í„°**: í¬ê·€ ì´ë²¤íŠ¸ ì˜ˆì¸¡ ë°©ë²•\n",
    "5. **ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸**: ì „ì²´ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ\n",
    "1. **ëª¨ë¸ ê°œì„ í•˜ê¸°**\n",
    "   - ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì‹œë„ (LightGBM, CatBoost)\n",
    "   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "   - ì•™ìƒë¸” ê¸°ë²• ì ìš©\n",
    "\n",
    "2. **ë” ê¹Šì´ ë°°ìš°ê¸°**\n",
    "   - ë‚ ì§œ ë°ì´í„° í™œìš©\n",
    "   - ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬\n",
    "   - ê³ ê¸‰ íŠ¹ì§• ê³µí•™\n",
    "\n",
    "3. **ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ë„ì „**\n",
    "   - Kaggleì˜ ë‹¤ë¥¸ ëŒ€íšŒ ì°¸ê°€\n",
    "   - ìì‹ ë§Œì˜ ë°ì´í„° í”„ë¡œì íŠ¸ ì‹œì‘\n",
    "\n",
    "### ğŸ’ª ê²©ë ¤ì˜ ë§\n",
    "ë°ì´í„° ê³¼í•™ì€ ì‹¤ìŠµì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. \n",
    "ì‹¤ìˆ˜ë¥¼ ë‘ë ¤ì›Œí•˜ì§€ ë§ê³  ê³„ì† ë„ì „í•˜ì„¸ìš”!\n",
    "\n",
    "**\"The expert in anything was once a beginner.\"**\n",
    "\n",
    "### ğŸ“§ ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±\n",
    "ì´ ë…¸íŠ¸ë¶ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ê°œì„  ì‚¬í•­ì´ ìˆë‹¤ë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”!\n",
    "\n",
    "---\n",
    "**Happy Learning! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}