#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bosch Production Line Performance - ê¸°ë³¸ ë°ì´í„° ë¶„ì„ (ì‹œê°í™” ì—†ìŒ)
"""
import pandas as pd
import numpy as np
import warnings
import gc
import os

warnings.filterwarnings('ignore')

def analyze_data_overview():
    """ë°ì´í„° ì „ì²´ ê°œìš” ë¶„ì„"""
    print("=" * 60)
    print("ğŸ­ BOSCH PRODUCTION LINE PERFORMANCE ì‹¤ì œ ë°ì´í„° ë¶„ì„")
    print("=" * 60)
    
    data_dir = "C:/Users/ASUS/bosch/data/"
    
    # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
    files = {
        'train_numeric': f'{data_dir}train_numeric.csv',
        'train_categorical': f'{data_dir}train_categorical.csv', 
        'train_date': f'{data_dir}train_date.csv',
        'test_numeric': f'{data_dir}test_numeric.csv',
        'test_categorical': f'{data_dir}test_categorical.csv',
        'test_date': f'{data_dir}test_date.csv'
    }
    
    print("\nğŸ“Š ë°ì´í„° íŒŒì¼ ì •ë³´:")
    print("-" * 50)
    
    for name, path in files.items():
        try:
            size_mb = os.path.getsize(path) / (1024*1024)
            
            # ì²« ì¤„ë§Œ ì½ì–´ì„œ ì»¬ëŸ¼ ìˆ˜ í™•ì¸
            header = pd.read_csv(path, nrows=0)
            num_cols = len(header.columns)
            
            # ë¹ ë¥¸ í–‰ ìˆ˜ ê³„ì‚°
            chunk_size = 10000
            total_rows = 0
            for chunk in pd.read_csv(path, chunksize=chunk_size):
                total_rows += len(chunk)
                break  # ì²« ë²ˆì§¸ ì²­í¬ë§Œìœ¼ë¡œ ì¶”ì •
            
            # ì‹¤ì œë¡œëŠ” ì „ì²´ í–‰ ìˆ˜ë¥¼ ì •í™•íˆ ê³„ì‚°í•˜ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
            # ì²« ì²­í¬ë¡œ ì¶”ì •
            with open(path, 'r') as f:
                estimated_rows = sum(1 for _ in f) - 1  # í—¤ë” ì œì™¸
            
            print(f"{name:20s}: {size_mb:7.1f} MB | ~{estimated_rows:7,} rows | {num_cols:4,} cols")
            
        except Exception as e:
            print(f"{name:20s}: Error - {str(e)}")
    
    return files

def analyze_train_numeric_detailed(data_dir, sample_size=50000):
    """ìˆ«ì ë°ì´í„° ìƒì„¸ ë¶„ì„"""
    print(f"\nğŸ”¢ TRAIN NUMERIC ë°ì´í„° ìƒì„¸ ë¶„ì„")
    print("=" * 50)
    
    file_path = f'{data_dir}train_numeric.csv'
    print(f"ìƒ˜í”Œ í¬ê¸°: {sample_size:,}ê°œ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´)")
    
    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    print("\nâ³ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv(file_path, nrows=sample_size)
    
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} x {df.shape[1]:,}")
    
    # 1. ê¸°ë³¸ ì •ë³´
    print(f"\nğŸ“‹ ê¸°ë³¸ ì •ë³´:")
    print(f"   â€¢ ì œí’ˆ ìˆ˜: {df.shape[0]:,}ê°œ")
    print(f"   â€¢ ì „ì²´ íŠ¹ì§• ìˆ˜: {df.shape[1]:,}ê°œ")
    print(f"   â€¢ ì‹¤ì œ ì¸¡ì • íŠ¹ì§• ìˆ˜: {df.shape[1] - 2:,}ê°œ (Id, Response ì œì™¸)")
    
    # 2. ëª©í‘œ ë³€ìˆ˜ ë¶„ì„
    print(f"\nğŸ¯ ëª©í‘œ ë³€ìˆ˜ (Response) ë¶„ì„:")
    response_counts = df['Response'].value_counts().sort_index()
    failure_rate = df['Response'].mean()
    
    print(f"   â€¢ ì •ìƒí’ˆ (0): {response_counts[0]:,}ê°œ ({(1-failure_rate)*100:.2f}%)")
    print(f"   â€¢ ë¶ˆëŸ‰í’ˆ (1): {response_counts[1]:,}ê°œ ({failure_rate*100:.2f}%)")
    print(f"   â€¢ ë¶ˆëŸ‰ë¥ : {failure_rate:.4%}")
    print(f"   â€¢ ë¶ˆê· í˜• ë¹„ìœ¨: 1:{int((1-failure_rate)/failure_rate)}")
    
    # 3. ê²°ì¸¡ê°’ ìƒì„¸ ë¶„ì„
    print(f"\nâ“ ê²°ì¸¡ê°’ ìƒì„¸ ë¶„ì„:")
    missing_stats = df.isnull().sum()
    total_features = len(df.columns) - 2  # Id, Response ì œì™¸
    
    # ê²°ì¸¡ê°’ ë¹„ìœ¨ë³„ ë¶„ë¥˜
    completely_empty = (missing_stats == len(df)).sum()
    over_95_empty = (missing_stats > len(df) * 0.95).sum()
    over_90_empty = (missing_stats > len(df) * 0.90).sum() 
    over_50_empty = (missing_stats > len(df) * 0.50).sum()
    over_10_empty = (missing_stats > len(df) * 0.10).sum()
    no_missing = (missing_stats == 0).sum()
    
    print(f"   ì „ì²´ íŠ¹ì§• ìˆ˜: {total_features:,}ê°œ")
    print(f"   ì™„ì „íˆ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {completely_empty:,}ê°œ ({completely_empty/total_features*100:.1f}%)")
    print(f"   95% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {over_95_empty:,}ê°œ ({over_95_empty/total_features*100:.1f}%)")
    print(f"   90% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {over_90_empty:,}ê°œ ({over_90_empty/total_features*100:.1f}%)")
    print(f"   50% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {over_50_empty:,}ê°œ ({over_50_empty/total_features*100:.1f}%)")
    print(f"   10% ì´ìƒ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {over_10_empty:,}ê°œ ({over_10_empty/total_features*100:.1f}%)")
    print(f"   ê²°ì¸¡ê°’ì´ ì „í˜€ ì—†ëŠ” ì»¬ëŸ¼: {no_missing:,}ê°œ ({no_missing/total_features*100:.1f}%)")
    
    # 4. íŠ¹ì§• ê·¸ë£¹ ë¶„ì„ (ìŠ¤í…Œì´ì…˜ë³„)
    print(f"\nğŸ­ ìŠ¤í…Œì´ì…˜/ë¼ì¸ë³„ íŠ¹ì§• ë¶„ì„:")
    feature_groups = {}
    
    for col in df.columns:
        if col not in ['Id', 'Response']:
            parts = col.split('_')
            if len(parts) >= 2:
                group = parts[0] + '_' + parts[1]
                if group not in feature_groups:
                    feature_groups[group] = []
                feature_groups[group].append(col)
    
    # ìƒìœ„ 15ê°œ ê·¸ë£¹
    sorted_groups = sorted(feature_groups.items(), key=lambda x: len(x[1]), reverse=True)
    print(f"   ì´ {len(feature_groups)}ê°œì˜ ì¸¡ì • ìŠ¤í…Œì´ì…˜ ë°œê²¬")
    print("   ìƒìœ„ 15ê°œ ìŠ¤í…Œì´ì…˜:")
    
    for i, (group, features) in enumerate(sorted_groups[:15], 1):
        # ê° ê·¸ë£¹ì˜ ê²°ì¸¡ë¥  ê³„ì‚°
        group_missing_rate = df[features].isnull().sum().sum() / (len(features) * len(df)) * 100
        print(f"   {i:2d}. {group}: {len(features):3d}ê°œ íŠ¹ì§• (ê²°ì¸¡ë¥ : {group_missing_rate:.1f}%)")
    
    return df, feature_groups

def create_engineered_features(df):
    """íŠ¹ì§• ê³µí•™"""
    print(f"\nğŸ› ï¸ íŠ¹ì§• ê³µí•™:")
    print("-" * 30)
    
    feature_df = pd.DataFrame()
    feature_df['Id'] = df['Id']
    
    # ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(['Id', 'Response'], errors='ignore')
    print(f"   ì²˜ë¦¬í•  ìˆ«ì íŠ¹ì§•: {len(numeric_cols):,}ê°œ")
    
    # ê¸°ë³¸ ì§‘ê³„ íŠ¹ì§•ë“¤
    print("   â³ ê¸°ë³¸ í†µê³„ íŠ¹ì§• ìƒì„± ì¤‘...")
    feature_df['count_non_null'] = df[numeric_cols].count(axis=1)
    feature_df['count_zeros'] = (df[numeric_cols] == 0).sum(axis=1) 
    feature_df['missing_count'] = df[numeric_cols].isnull().sum(axis=1)
    feature_df['missing_ratio'] = feature_df['missing_count'] / len(numeric_cols)
    
    # ê°’ì´ ìˆëŠ” í–‰ì— ëŒ€í•œ í†µê³„
    print("   â³ í†µê³„ëŸ‰ ê³„ì‚° ì¤‘...")
    feature_df['mean'] = df[numeric_cols].mean(axis=1, skipna=True)
    feature_df['std'] = df[numeric_cols].std(axis=1, skipna=True)
    feature_df['min'] = df[numeric_cols].min(axis=1, skipna=True)
    feature_df['max'] = df[numeric_cols].max(axis=1, skipna=True)
    feature_df['range'] = feature_df['max'] - feature_df['min']
    feature_df['median'] = df[numeric_cols].median(axis=1, skipna=True)
    feature_df['q25'] = df[numeric_cols].quantile(0.25, axis=1, numeric_only=True)
    feature_df['q75'] = df[numeric_cols].quantile(0.75, axis=1, numeric_only=True)
    feature_df['iqr'] = feature_df['q75'] - feature_df['q25']
    
    # ìŠ¤í…Œì´ì…˜ë³„ íŠ¹ì§• (ì£¼ìš” ìŠ¤í…Œì´ì…˜ë§Œ)
    print("   â³ ìŠ¤í…Œì´ì…˜ë³„ íŠ¹ì§• ìƒì„± ì¤‘...")
    station_groups = {}
    for col in numeric_cols:
        parts = col.split('_')
        if len(parts) >= 2:
            station = parts[0] + '_' + parts[1]
            if station not in station_groups:
                station_groups[station] = []
            station_groups[station].append(col)
    
    # ìƒìœ„ 10ê°œ ìŠ¤í…Œì´ì…˜ì˜ í†µê³„
    top_stations = sorted(station_groups.items(), key=lambda x: len(x[1]), reverse=True)[:10]
    
    for station, cols in top_stations:
        if len(cols) > 1:  # íŠ¹ì§•ì´ 2ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ
            station_data = df[cols]
            feature_df[f'{station}_mean'] = station_data.mean(axis=1, skipna=True)
            feature_df[f'{station}_count'] = station_data.count(axis=1)
            feature_df[f'{station}_std'] = station_data.std(axis=1, skipna=True)
    
    # ëª©í‘œ ë³€ìˆ˜ ì¶”ê°€
    if 'Response' in df.columns:
        feature_df['Response'] = df['Response']
    
    print(f"   âœ… ì™„ë£Œ! ì´ {len(feature_df.columns)}ê°œ íŠ¹ì§• ìƒì„±")
    
    return feature_df

def analyze_feature_importance(feature_df):
    """íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ (ìƒê´€ê´€ê³„ ê¸°ë°˜)"""
    print(f"\nğŸ“Š íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„:")
    print("-" * 40)
    
    if 'Response' not in feature_df.columns:
        print("ëª©í‘œ ë³€ìˆ˜ê°€ ì—†ì–´ì„œ ì¤‘ìš”ë„ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    feature_cols = [col for col in feature_df.columns if col not in ['Id', 'Response']]
    
    print(f"   ë¶„ì„í•  íŠ¹ì§• ìˆ˜: {len(feature_cols)}ê°œ")
    
    correlations = {}
    for col in feature_cols:
        try:
            corr = feature_df[col].corr(feature_df['Response'])
            if not np.isnan(corr):
                correlations[col] = corr
        except:
            continue
    
    # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
    
    print(f"\n   ìƒìœ„ 20ê°œ ì¤‘ìš” íŠ¹ì§• (ìƒê´€ê³„ìˆ˜ ê¸°ì¤€):")
    print("   " + "-" * 50)
    
    for i, (feature, corr) in enumerate(sorted_corr[:20], 1):
        direction = "â†—ï¸ ì–‘ì˜" if corr > 0 else "â†˜ï¸ ìŒì˜"
        print(f"   {i:2d}. {feature:25s}: {corr:+.5f} ({direction})")
    
    # ê·¸ë£¹ë³„ ì¤‘ìš”ë„ (í‰ê· )
    print(f"\n   ìŠ¤í…Œì´ì…˜ë³„ í‰ê·  ì¤‘ìš”ë„:")
    print("   " + "-" * 40)
    
    station_importance = {}
    for feature, corr in correlations.items():
        if '_' in feature and feature.count('_') >= 1:
            parts = feature.split('_')
            if len(parts) >= 2:
                station = parts[0] + '_' + parts[1]
                if station not in station_importance:
                    station_importance[station] = []
                station_importance[station].append(abs(corr))
    
    # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
    station_avg_importance = {
        station: np.mean(corrs) 
        for station, corrs in station_importance.items() 
        if len(corrs) > 0
    }
    
    sorted_stations = sorted(station_avg_importance.items(), key=lambda x: x[1], reverse=True)
    
    for i, (station, avg_importance) in enumerate(sorted_stations[:10], 1):
        feature_count = len(station_importance.get(station, []))
        print(f"   {i:2d}. {station:15s}: {avg_importance:.5f} ({feature_count}ê°œ íŠ¹ì§•)")
    
    return sorted_corr

def compare_failure_vs_normal(feature_df):
    """ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ ë¹„êµ ë¶„ì„"""
    print(f"\nâš–ï¸ ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ ë¹„êµ ë¶„ì„:")
    print("-" * 50)
    
    if 'Response' not in feature_df.columns:
        print("ëª©í‘œ ë³€ìˆ˜ê°€ ì—†ì–´ì„œ ë¹„êµ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    normal = feature_df[feature_df['Response'] == 0]
    failure = feature_df[feature_df['Response'] == 1]
    
    print(f"   ì •ìƒí’ˆ: {len(normal):,}ê°œ")
    print(f"   ë¶ˆëŸ‰í’ˆ: {len(failure):,}ê°œ")
    
    # ì£¼ìš” íŠ¹ì§•ë“¤ ë¹„êµ
    key_features = ['count_non_null', 'missing_ratio', 'mean', 'std', 'range']
    
    print(f"\n   ì£¼ìš” íŠ¹ì§• ë¹„êµ:")
    print(f"   {'íŠ¹ì§•':15s} {'ì •ìƒí’ˆ í‰ê· ':>12s} {'ë¶ˆëŸ‰í’ˆ í‰ê· ':>12s} {'ì°¨ì´':>12s} {'ë¹„ìœ¨':>10s}")
    print("   " + "-" * 70)
    
    for feature in key_features:
        if feature in feature_df.columns:
            normal_mean = normal[feature].mean()
            failure_mean = failure[feature].mean()
            diff = failure_mean - normal_mean
            ratio = failure_mean / normal_mean if normal_mean != 0 else float('inf')
            
            print(f"   {feature:15s} {normal_mean:12.3f} {failure_mean:12.3f} {diff:+12.3f} {ratio:10.3f}")
    
    # í†µê³„ì  ìœ ì˜ì„± (ê°„ë‹¨í•œ t-test ëŒ€ì‹  í‰ê·  ì°¨ì´ë¡œ íŒë‹¨)
    print(f"\n   ğŸ’¡ ì£¼ìš” ì°¨ì´ì :")
    
    # ì¸¡ì • ê°œìˆ˜ ì°¨ì´
    normal_count = normal['count_non_null'].mean()
    failure_count = failure['count_non_null'].mean()
    if abs(failure_count - normal_count) > normal_count * 0.01:  # 1% ì´ìƒ ì°¨ì´
        direction = "ë” ë§ì´" if failure_count > normal_count else "ë” ì ê²Œ"
        print(f"   â€¢ ë¶ˆëŸ‰í’ˆì€ ì •ìƒí’ˆë³´ë‹¤ ì¸¡ì •ì„ {direction} ë°›ìŒ ({failure_count:.1f} vs {normal_count:.1f})")
    
    # ê²°ì¸¡ë¥  ì°¨ì´
    normal_missing = normal['missing_ratio'].mean()
    failure_missing = failure['missing_ratio'].mean()
    if abs(failure_missing - normal_missing) > 0.01:  # 1%p ì´ìƒ ì°¨ì´
        direction = "ë†’ìŒ" if failure_missing > normal_missing else "ë‚®ìŒ"
        print(f"   â€¢ ë¶ˆëŸ‰í’ˆì˜ ê²°ì¸¡ë¥ ì´ {direction} ({failure_missing:.1%} vs {normal_missing:.1%})")
    
    # ì¸¡ì •ê°’ ì°¨ì´
    normal_mean_val = normal['mean'].mean()
    failure_mean_val = failure['mean'].mean()
    if not np.isnan(normal_mean_val) and not np.isnan(failure_mean_val):
        if abs(failure_mean_val - normal_mean_val) > abs(normal_mean_val * 0.05):  # 5% ì´ìƒ ì°¨ì´
            direction = "ë†’ìŒ" if failure_mean_val > normal_mean_val else "ë‚®ìŒ"
            print(f"   â€¢ ë¶ˆëŸ‰í’ˆì˜ ì¸¡ì •ê°’ í‰ê· ì´ {direction} ({failure_mean_val:.3f} vs {normal_mean_val:.3f})")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    try:
        start_time = pd.Timestamp.now()
        
        # 1. ì „ì²´ ê°œìš”
        print("ğŸš€ ë¶„ì„ ì‹œì‘...")
        files = analyze_data_overview()
        
        # 2. ìˆ«ì ë°ì´í„° ìƒì„¸ ë¶„ì„
        data_dir = "C:/Users/ASUS/bosch/data/"
        df, feature_groups = analyze_train_numeric_detailed(data_dir, sample_size=100000)
        
        # 3. íŠ¹ì§• ê³µí•™
        feature_df = create_engineered_features(df)
        
        # 4. íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
        correlations = analyze_feature_importance(feature_df)
        
        # 5. ì •ìƒí’ˆ vs ë¶ˆëŸ‰í’ˆ ë¹„êµ
        compare_failure_vs_normal(feature_df)
        
        # 6. ìµœì¢… ìš”ì•½
        end_time = pd.Timestamp.now()
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\n" + "="*70)
        print("ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*70)
        
        failure_rate = df['Response'].mean()
        total_features = len(df.columns) - 2
        useful_features = len([col for col in df.columns if df[col].isnull().sum() < len(df) * 0.9])
        
        print(f"ğŸ” ë°ì´í„°ì…‹ íŠ¹ì„±:")
        print(f"   â€¢ ë¶„ì„ ìƒ˜í”Œ: {df.shape[0]:,} x {df.shape[1]:,}")
        print(f"   â€¢ ë¶ˆëŸ‰ë¥ : {failure_rate:.4%} (ê·¹ë„ë¡œ ë¶ˆê· í˜•)")
        print(f"   â€¢ ì „ì²´ íŠ¹ì§•: {total_features:,}ê°œ")
        print(f"   â€¢ ìœ ìš©í•œ íŠ¹ì§•: {useful_features:,}ê°œ (90% ë¯¸ë§Œ ê²°ì¸¡)")
        print(f"   â€¢ ì¸¡ì • ìŠ¤í…Œì´ì…˜: {len(feature_groups)}ê°œ")
        
        print(f"\nğŸ› ï¸ íŠ¹ì§• ê³µí•™ ê²°ê³¼:")
        print(f"   â€¢ ìƒì„±ëœ ì§‘ê³„ íŠ¹ì§•: {len(feature_df.columns)}ê°œ")
        
        if correlations and len(correlations) > 0:
            max_corr = max(abs(corr) for _, corr in correlations[:10])
            print(f"   â€¢ ìµœê³  ìƒê´€ê³„ìˆ˜: {max_corr:.4f}")
        
        print(f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
        print(f"   1. ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë°ì´í„° (ë¶ˆëŸ‰ë¥  {failure_rate:.2%})")
        print(f"   2. ëŒ€ë¶€ë¶„ì˜ íŠ¹ì§•ì´ í¬ì†Œí•¨ (90% ì´ìƒ ê²°ì¸¡)")
        print(f"   3. ê²°ì¸¡ê°’ íŒ¨í„´ ìì²´ê°€ ì¤‘ìš”í•œ ì •ë³´ì›")
        print(f"   4. ì§‘ê³„ íŠ¹ì§•(count, mean ë“±)ì´ ì˜ˆì¸¡ì— ìœ ìš©")
        print(f"   5. ìŠ¤í…Œì´ì…˜ë³„ ë¶„ì„ ì ‘ê·¼ë²•ì´ í•„ìš”")
        
        print(f"\nğŸ¯ ì¶”ì²œ ëª¨ë¸ë§ ì „ëµ:")
        print(f"   â€¢ ë¶ˆê· í˜• ì²˜ë¦¬: SMOTE, ê°€ì¤‘ì¹˜ ì¡°ì •, ì„ê³„ê°’ ìµœì í™”")
        print(f"   â€¢ íŠ¹ì§• ì„ íƒ: ìƒê´€ê´€ê³„, ë¶„ì‚° ê¸°ë°˜ í•„í„°ë§")
        print(f"   â€¢ ëª¨ë¸: XGBoost, LightGBM (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸)")
        print(f"   â€¢ í‰ê°€: Matthews Correlation Coefficient (MCC)")
        
        print(f"\nâ±ï¸ ë¶„ì„ ì™„ë£Œ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del df, feature_df
        gc.collect()
        
        print(f"\nâœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()